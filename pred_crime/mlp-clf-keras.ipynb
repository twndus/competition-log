{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84406, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>월</th>\n",
       "      <th>요일</th>\n",
       "      <th>시간</th>\n",
       "      <th>소관경찰서</th>\n",
       "      <th>소관지역</th>\n",
       "      <th>사건발생거리</th>\n",
       "      <th>강수량(mm)</th>\n",
       "      <th>강설량(mm)</th>\n",
       "      <th>적설량(cm)</th>\n",
       "      <th>풍향</th>\n",
       "      <th>안개</th>\n",
       "      <th>짙은안개</th>\n",
       "      <th>번개</th>\n",
       "      <th>진눈깨비</th>\n",
       "      <th>서리</th>\n",
       "      <th>연기/연무</th>\n",
       "      <th>눈날림</th>\n",
       "      <th>범죄발생지</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>9</td>\n",
       "      <td>화요일</td>\n",
       "      <td>10</td>\n",
       "      <td>137</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.611124</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>차도</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>11</td>\n",
       "      <td>화요일</td>\n",
       "      <td>6</td>\n",
       "      <td>438</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.209093</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>차도</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>8</td>\n",
       "      <td>일요일</td>\n",
       "      <td>6</td>\n",
       "      <td>1729</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.619597</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>인도</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>5</td>\n",
       "      <td>월요일</td>\n",
       "      <td>6</td>\n",
       "      <td>2337</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.921615</td>\n",
       "      <td>11.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>주거지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>9</td>\n",
       "      <td>일요일</td>\n",
       "      <td>11</td>\n",
       "      <td>1439</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.789721</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>주유소</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID   월   요일  시간  소관경찰서  소관지역    사건발생거리  강수량(mm)  강설량(mm)  적설량(cm)   \n",
       "0  TRAIN_00000   9  화요일  10    137   8.0  2.611124    0.000      0.0      0.0  \\\n",
       "1  TRAIN_00001  11  화요일   6    438  13.0  3.209093    0.000      0.0      0.0   \n",
       "2  TRAIN_00002   8  일요일   6   1729  47.0  1.619597    0.000      0.0      0.0   \n",
       "3  TRAIN_00003   5  월요일   6   2337  53.0  1.921615   11.375      0.0      0.0   \n",
       "4  TRAIN_00004   9  일요일  11   1439  41.0  1.789721    0.000      0.0      0.0   \n",
       "\n",
       "      풍향   안개  짙은안개   번개  진눈깨비   서리  연기/연무  눈날림 범죄발생지  TARGET  \n",
       "0  245.0  0.0   0.0  0.0   0.0  0.0    0.0  0.0    차도       2  \n",
       "1  200.0  0.0   0.0  0.0   0.0  0.0    0.0  0.0    차도       0  \n",
       "2   40.0  1.0   0.0  0.0   0.0  0.0    1.0  0.0    인도       1  \n",
       "3  225.0  1.0   1.0  0.0   0.0  0.0    0.0  0.0   주거지       1  \n",
       "4  255.0  0.0   0.0  0.0   0.0  0.0    0.0  0.0   주유소       2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17289, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>월</th>\n",
       "      <th>요일</th>\n",
       "      <th>시간</th>\n",
       "      <th>소관경찰서</th>\n",
       "      <th>소관지역</th>\n",
       "      <th>사건발생거리</th>\n",
       "      <th>강수량(mm)</th>\n",
       "      <th>강설량(mm)</th>\n",
       "      <th>적설량(cm)</th>\n",
       "      <th>풍향</th>\n",
       "      <th>안개</th>\n",
       "      <th>짙은안개</th>\n",
       "      <th>번개</th>\n",
       "      <th>진눈깨비</th>\n",
       "      <th>서리</th>\n",
       "      <th>연기/연무</th>\n",
       "      <th>눈날림</th>\n",
       "      <th>범죄발생지</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>9</td>\n",
       "      <td>금요일</td>\n",
       "      <td>5</td>\n",
       "      <td>927</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.570654</td>\n",
       "      <td>19.625000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>차도</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>5</td>\n",
       "      <td>수요일</td>\n",
       "      <td>3</td>\n",
       "      <td>926</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.712457</td>\n",
       "      <td>21.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>식당</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>5</td>\n",
       "      <td>월요일</td>\n",
       "      <td>6</td>\n",
       "      <td>1437</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>25.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>주거지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>11</td>\n",
       "      <td>화요일</td>\n",
       "      <td>1</td>\n",
       "      <td>1739</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.878585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>주거지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>10</td>\n",
       "      <td>목요일</td>\n",
       "      <td>10</td>\n",
       "      <td>830</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.496423</td>\n",
       "      <td>26.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>주거지</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID   월   요일  시간  소관경찰서  소관지역    사건발생거리    강수량(mm)  강설량(mm)   \n",
       "0  TEST_00000   9  금요일   5    927  28.0  1.570654  19.625000      0.0  \\\n",
       "1  TEST_00001   5  수요일   3    926  28.0  1.712457  21.444444      0.0   \n",
       "2  TEST_00002   5  월요일   6   1437  33.0  0.447496  25.200000      0.0   \n",
       "3  TEST_00003  11  화요일   1   1739  31.0  0.878585   0.000000      0.0   \n",
       "4  TEST_00004  10  목요일  10    830  15.0  0.496423  26.142857      0.0   \n",
       "\n",
       "   적설량(cm)     풍향   안개  짙은안개   번개  진눈깨비   서리  연기/연무  눈날림 범죄발생지  \n",
       "0      0.0  165.0  1.0   0.0  1.0   0.0  0.0    0.0  0.0    차도  \n",
       "1      0.0  175.0  1.0   0.0  0.0   0.0  0.0    1.0  0.0    식당  \n",
       "2      0.0  290.0  1.0   0.0  0.0   0.0  0.0    0.0  0.0   주거지  \n",
       "3      0.0  285.0  0.0   0.0  0.0   0.0  0.0    0.0  0.0   주거지  \n",
       "4      0.0   95.0  1.0   0.0  0.0   0.0  0.0    0.0  0.0   주거지  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical columns:  ['요일', '범죄발생지']\n",
      "numeric columns:  ['월', '시간', '소관경찰서', '소관지역', '사건발생거리', '강수량(mm)', '강설량(mm)', '적설량(cm)', '풍향', '안개', '짙은안개', '번개', '진눈깨비', '서리', '연기/연무', '눈날림']\n"
     ]
    }
   ],
   "source": [
    "cat_cols = []\n",
    "num_cols = []\n",
    "\n",
    "for col in test_df.columns[1:]:\n",
    "    if train_df[col].dtype == 'object':\n",
    "        cat_cols.append(col)\n",
    "    else:\n",
    "        num_cols.append(col)\n",
    "\n",
    "print('categorical columns: ', cat_cols)\n",
    "print('numeric columns: ', num_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess cat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['금요일', '목요일', '수요일', '월요일', '일요일', '토요일', '화요일', '공원', '백화점', '병원',\n",
       "       '식당', '약국', '은행', '인도', '주거지', '주유소', '주차장', '차도', '편의점', '학교',\n",
       "       '호텔/모텔'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat_array = encoder.fit_transform(train_df[cat_cols])\n",
    "test_cat_array = encoder.transform(test_df[cat_cols])\n",
    "\n",
    "encoded_cols = np.concatenate(encoder.categories_)\n",
    "encoded_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84406, 39), (17289, 38))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([\n",
    "    train_df.drop(columns=cat_cols),\n",
    "    pd.DataFrame(train_cat_array, columns=encoded_cols)], axis=1)\n",
    "    \n",
    "test_df = pd.concat([\n",
    "    test_df.drop(columns=cat_cols),\n",
    "    pd.DataFrame(test_cat_array, columns=encoded_cols)], axis=1)\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>월</th>\n",
       "      <th>시간</th>\n",
       "      <th>소관경찰서</th>\n",
       "      <th>소관지역</th>\n",
       "      <th>사건발생거리</th>\n",
       "      <th>강수량(mm)</th>\n",
       "      <th>강설량(mm)</th>\n",
       "      <th>적설량(cm)</th>\n",
       "      <th>풍향</th>\n",
       "      <th>안개</th>\n",
       "      <th>짙은안개</th>\n",
       "      <th>번개</th>\n",
       "      <th>진눈깨비</th>\n",
       "      <th>서리</th>\n",
       "      <th>연기/연무</th>\n",
       "      <th>눈날림</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.430195</td>\n",
       "      <td>6.769507</td>\n",
       "      <td>1060.027581</td>\n",
       "      <td>26.881726</td>\n",
       "      <td>1.912424</td>\n",
       "      <td>24.608776</td>\n",
       "      <td>2.284407</td>\n",
       "      <td>23.430503</td>\n",
       "      <td>186.926107</td>\n",
       "      <td>0.385423</td>\n",
       "      <td>0.017842</td>\n",
       "      <td>0.144042</td>\n",
       "      <td>0.020330</td>\n",
       "      <td>0.010260</td>\n",
       "      <td>0.210755</td>\n",
       "      <td>0.008921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.108302</td>\n",
       "      <td>3.566390</td>\n",
       "      <td>698.380485</td>\n",
       "      <td>13.870968</td>\n",
       "      <td>0.958556</td>\n",
       "      <td>62.711211</td>\n",
       "      <td>15.852881</td>\n",
       "      <td>85.199896</td>\n",
       "      <td>98.299485</td>\n",
       "      <td>0.486698</td>\n",
       "      <td>0.132379</td>\n",
       "      <td>0.351134</td>\n",
       "      <td>0.141128</td>\n",
       "      <td>0.100771</td>\n",
       "      <td>0.407847</td>\n",
       "      <td>0.094030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.012269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>526.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.209985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>937.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>1.822279</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1638.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>2.476528</td>\n",
       "      <td>18.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2450.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>4.998936</td>\n",
       "      <td>614.875000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>649.800000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  월            시간         소관경찰서          소관지역        사건발생거리   \n",
       "count  84406.000000  84406.000000  84406.000000  84406.000000  84406.000000  \\\n",
       "mean       6.430195      6.769507   1060.027581     26.881726      1.912424   \n",
       "std        3.108302      3.566390    698.380485     13.870968      0.958556   \n",
       "min        1.000000      1.000000     26.000000      5.000000      0.012269   \n",
       "25%        4.000000      4.000000    526.000000     13.000000      1.209985   \n",
       "50%        7.000000      7.000000    937.000000     27.000000      1.822279   \n",
       "75%        9.000000     10.000000   1638.000000     38.000000      2.476528   \n",
       "max       12.000000     12.000000   2450.000000     54.000000      4.998936   \n",
       "\n",
       "            강수량(mm)       강설량(mm)       적설량(cm)            풍향            안개   \n",
       "count  84406.000000  84406.000000  84406.000000  84406.000000  84406.000000  \\\n",
       "mean      24.608776      2.284407     23.430503    186.926107      0.385423   \n",
       "std       62.711211     15.852881     85.199896     98.299485      0.486698   \n",
       "min        0.000000      0.000000      0.000000     10.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000     95.000000      0.000000   \n",
       "50%        0.625000      0.000000      0.000000    205.000000      0.000000   \n",
       "75%       18.571429      0.000000      0.000000    260.000000      1.000000   \n",
       "max      614.875000    295.000000    649.800000    360.000000      1.000000   \n",
       "\n",
       "               짙은안개            번개          진눈깨비            서리         연기/연무   \n",
       "count  84406.000000  84406.000000  84406.000000  84406.000000  84406.000000  \\\n",
       "mean       0.017842      0.144042      0.020330      0.010260      0.210755   \n",
       "std        0.132379      0.351134      0.141128      0.100771      0.407847   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                눈날림  \n",
       "count  84406.000000  \n",
       "mean       0.008921  \n",
       "std        0.094030  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[num_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>월</th>\n",
       "      <th>시간</th>\n",
       "      <th>소관경찰서</th>\n",
       "      <th>소관지역</th>\n",
       "      <th>사건발생거리</th>\n",
       "      <th>강수량(mm)</th>\n",
       "      <th>강설량(mm)</th>\n",
       "      <th>적설량(cm)</th>\n",
       "      <th>풍향</th>\n",
       "      <th>안개</th>\n",
       "      <th>짙은안개</th>\n",
       "      <th>번개</th>\n",
       "      <th>진눈깨비</th>\n",
       "      <th>서리</th>\n",
       "      <th>연기/연무</th>\n",
       "      <th>눈날림</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.522002e-16</td>\n",
       "      <td>1.146132e-16</td>\n",
       "      <td>3.703988e-17</td>\n",
       "      <td>7.176477e-17</td>\n",
       "      <td>2.200506e-16</td>\n",
       "      <td>7.660520e-18</td>\n",
       "      <td>-1.043851e-17</td>\n",
       "      <td>4.322722e-17</td>\n",
       "      <td>5.219256e-17</td>\n",
       "      <td>-3.493534e-18</td>\n",
       "      <td>-4.444785e-17</td>\n",
       "      <td>1.056794e-16</td>\n",
       "      <td>-4.284841e-17</td>\n",
       "      <td>7.500576e-17</td>\n",
       "      <td>-1.052269e-17</td>\n",
       "      <td>-6.759778e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.747008e+00</td>\n",
       "      <td>-1.617754e+00</td>\n",
       "      <td>-1.480617e+00</td>\n",
       "      <td>-1.577529e+00</td>\n",
       "      <td>-1.982320e+00</td>\n",
       "      <td>-3.924166e-01</td>\n",
       "      <td>-1.441013e-01</td>\n",
       "      <td>-2.750079e-01</td>\n",
       "      <td>-1.799879e+00</td>\n",
       "      <td>-7.919185e-01</td>\n",
       "      <td>-1.347830e-01</td>\n",
       "      <td>-4.102213e-01</td>\n",
       "      <td>-1.440563e-01</td>\n",
       "      <td>-1.018150e-01</td>\n",
       "      <td>-5.167533e-01</td>\n",
       "      <td>-9.487608e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.818446e-01</td>\n",
       "      <td>-7.765621e-01</td>\n",
       "      <td>-7.646702e-01</td>\n",
       "      <td>-1.000782e+00</td>\n",
       "      <td>-7.328137e-01</td>\n",
       "      <td>-3.924166e-01</td>\n",
       "      <td>-1.441013e-01</td>\n",
       "      <td>-2.750079e-01</td>\n",
       "      <td>-9.351692e-01</td>\n",
       "      <td>-7.919185e-01</td>\n",
       "      <td>-1.347830e-01</td>\n",
       "      <td>-4.102213e-01</td>\n",
       "      <td>-1.440563e-01</td>\n",
       "      <td>-1.018150e-01</td>\n",
       "      <td>-5.167533e-01</td>\n",
       "      <td>-9.487608e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.833184e-01</td>\n",
       "      <td>6.462963e-02</td>\n",
       "      <td>-1.761623e-01</td>\n",
       "      <td>8.526751e-03</td>\n",
       "      <td>-9.404287e-02</td>\n",
       "      <td>-3.824502e-01</td>\n",
       "      <td>-1.441013e-01</td>\n",
       "      <td>-2.750079e-01</td>\n",
       "      <td>1.838667e-01</td>\n",
       "      <td>-7.919185e-01</td>\n",
       "      <td>-1.347830e-01</td>\n",
       "      <td>-4.102213e-01</td>\n",
       "      <td>-1.440563e-01</td>\n",
       "      <td>-1.018150e-01</td>\n",
       "      <td>-5.167533e-01</td>\n",
       "      <td>-9.487608e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.267604e-01</td>\n",
       "      <td>9.058214e-01</td>\n",
       "      <td>8.275945e-01</td>\n",
       "      <td>8.015547e-01</td>\n",
       "      <td>5.884964e-01</td>\n",
       "      <td>-9.627278e-02</td>\n",
       "      <td>-1.441013e-01</td>\n",
       "      <td>-2.750079e-01</td>\n",
       "      <td>7.433846e-01</td>\n",
       "      <td>1.262756e+00</td>\n",
       "      <td>-1.347830e-01</td>\n",
       "      <td>-4.102213e-01</td>\n",
       "      <td>-1.440563e-01</td>\n",
       "      <td>-1.018150e-01</td>\n",
       "      <td>-5.167533e-01</td>\n",
       "      <td>-9.487608e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.791923e+00</td>\n",
       "      <td>1.466616e+00</td>\n",
       "      <td>1.990291e+00</td>\n",
       "      <td>1.955050e+00</td>\n",
       "      <td>3.219977e+00</td>\n",
       "      <td>9.412507e+00</td>\n",
       "      <td>1.846461e+01</td>\n",
       "      <td>7.351807e+00</td>\n",
       "      <td>1.760690e+00</td>\n",
       "      <td>1.262756e+00</td>\n",
       "      <td>7.419332e+00</td>\n",
       "      <td>2.437709e+00</td>\n",
       "      <td>6.941732e+00</td>\n",
       "      <td>9.821737e+00</td>\n",
       "      <td>1.935160e+00</td>\n",
       "      <td>1.054006e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  월            시간         소관경찰서          소관지역        사건발생거리   \n",
       "count  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  \\\n",
       "mean   1.522002e-16  1.146132e-16  3.703988e-17  7.176477e-17  2.200506e-16   \n",
       "std    1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00   \n",
       "min   -1.747008e+00 -1.617754e+00 -1.480617e+00 -1.577529e+00 -1.982320e+00   \n",
       "25%   -7.818446e-01 -7.765621e-01 -7.646702e-01 -1.000782e+00 -7.328137e-01   \n",
       "50%    1.833184e-01  6.462963e-02 -1.761623e-01  8.526751e-03 -9.404287e-02   \n",
       "75%    8.267604e-01  9.058214e-01  8.275945e-01  8.015547e-01  5.884964e-01   \n",
       "max    1.791923e+00  1.466616e+00  1.990291e+00  1.955050e+00  3.219977e+00   \n",
       "\n",
       "            강수량(mm)       강설량(mm)       적설량(cm)            풍향            안개   \n",
       "count  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  \\\n",
       "mean   7.660520e-18 -1.043851e-17  4.322722e-17  5.219256e-17 -3.493534e-18   \n",
       "std    1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00   \n",
       "min   -3.924166e-01 -1.441013e-01 -2.750079e-01 -1.799879e+00 -7.919185e-01   \n",
       "25%   -3.924166e-01 -1.441013e-01 -2.750079e-01 -9.351692e-01 -7.919185e-01   \n",
       "50%   -3.824502e-01 -1.441013e-01 -2.750079e-01  1.838667e-01 -7.919185e-01   \n",
       "75%   -9.627278e-02 -1.441013e-01 -2.750079e-01  7.433846e-01  1.262756e+00   \n",
       "max    9.412507e+00  1.846461e+01  7.351807e+00  1.760690e+00  1.262756e+00   \n",
       "\n",
       "               짙은안개            번개          진눈깨비            서리         연기/연무   \n",
       "count  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  \\\n",
       "mean  -4.444785e-17  1.056794e-16 -4.284841e-17  7.500576e-17 -1.052269e-17   \n",
       "std    1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00   \n",
       "min   -1.347830e-01 -4.102213e-01 -1.440563e-01 -1.018150e-01 -5.167533e-01   \n",
       "25%   -1.347830e-01 -4.102213e-01 -1.440563e-01 -1.018150e-01 -5.167533e-01   \n",
       "50%   -1.347830e-01 -4.102213e-01 -1.440563e-01 -1.018150e-01 -5.167533e-01   \n",
       "75%   -1.347830e-01 -4.102213e-01 -1.440563e-01 -1.018150e-01 -5.167533e-01   \n",
       "max    7.419332e+00  2.437709e+00  6.941732e+00  9.821737e+00  1.935160e+00   \n",
       "\n",
       "                눈날림  \n",
       "count  8.440600e+04  \n",
       "mean  -6.759778e-17  \n",
       "std    1.000006e+00  \n",
       "min   -9.487608e-02  \n",
       "25%   -9.487608e-02  \n",
       "50%   -9.487608e-02  \n",
       "75%   -9.487608e-02  \n",
       "max    1.054006e+01  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n",
    "test_df[num_cols] = scaler.transform(test_df[num_cols])\n",
    "\n",
    "train_df[num_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84406, 37), (17289, 37))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = train_df[np.concatenate([encoded_cols, num_cols])]\n",
    "test_X = test_df[np.concatenate([encoded_cols, num_cols])]\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84406,), (84406, 3))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = train_df['TARGET']\n",
    "train_y_multi = pd.get_dummies(train_df['TARGET'])\n",
    "train_y.shape, train_y_multi.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train data into train/val with StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[    0     1     2 ... 84403 84404 84405]\n",
      "  Test:  index=[    6    10    20 ... 84387 84393 84394]\n",
      "Fold 1:\n",
      "  Train: index=[    0     1     2 ... 84403 84404 84405]\n",
      "  Test:  index=[    7    11    12 ... 84390 84392 84402]\n",
      "Fold 2:\n",
      "  Train: index=[    0     1     2 ... 84402 84403 84405]\n",
      "  Test:  index=[   13    17    18 ... 84399 84400 84404]\n",
      "Fold 3:\n",
      "  Train: index=[    4     5     6 ... 84401 84402 84404]\n",
      "  Test:  index=[    0     1     2 ... 84397 84403 84405]\n",
      "Fold 4:\n",
      "  Train: index=[    0     1     2 ... 84403 84404 84405]\n",
      "  Test:  index=[    4     5     9 ... 84396 84398 84401]\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, val_index) in enumerate(skf.split(train_X, train_y)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={val_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 나중에 꼭 잘라서 교차검증하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 23:27:14.097564: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-16 23:27:14.291991: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-16 23:27:15.239304: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/:/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/\n",
      "2023-05-16 23:27:15.239474: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/:/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/\n",
      "2023-05-16 23:27:15.239492: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.regularizers.L1L2 at 0x7fe5693f7010>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_dim):\n",
    "    inputs = tf.keras.Input(shape=input_dim, dtype='float32')\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=100, activation='relu', kernel_initializer='he_normal', \n",
    "        kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
    "        )(inputs)\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=3, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 23:27:16.964750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:16.965101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:17.010854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:17.011193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:17.011403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:17.011584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:17.012904: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-16 23:27:17.281940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:17.282623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:17.282863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:17.283039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:17.283206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:17.283398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:18.858838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:18.859160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:18.859414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:18.859604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:18.859780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:18.859986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30961 MB memory:  -> device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:00:06.0, compute capability: 7.0\n",
      "2023-05-16 23:27:18.860756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-16 23:27:18.860942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30961 MB memory:  -> device: 1, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:00:07.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 37)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               3800      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,103\n",
      "Trainable params: 4,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlpclassifier = build_mlp(train_X.shape[1])\n",
    "mlpclassifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "epochs = 200\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "mlpclassifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate), \n",
    "    loss='categorical_crossentropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 23:27:21.692276: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fdad82497d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-16 23:27:21.692343: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla V100S-PCIE-32GB, Compute Capability 7.0\n",
      "2023-05-16 23:27:21.692353: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): Tesla V100S-PCIE-32GB, Compute Capability 7.0\n",
      "2023-05-16 23:27:21.698435: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-16 23:27:21.851604: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676/676 [==============================] - 4s 3ms/step - loss: 8.8609 - val_loss: 7.4326\n",
      "Epoch 2/200\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 6.2979 - val_loss: 5.2594\n",
      "Epoch 3/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 4.4336 - val_loss: 3.6910\n",
      "Epoch 4/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 3.1170 - val_loss: 2.6072\n",
      "Epoch 5/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 2.2275 - val_loss: 1.9003\n",
      "Epoch 6/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.6644 - val_loss: 1.4625\n",
      "Epoch 7/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.3248 - val_loss: 1.2147\n",
      "Epoch 8/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.1486 - val_loss: 1.1020\n",
      "Epoch 9/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0795 - val_loss: 1.0694\n",
      "Epoch 10/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0638 - val_loss: 1.0628\n",
      "Epoch 11/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0595 - val_loss: 1.0599\n",
      "Epoch 12/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0570 - val_loss: 1.0579\n",
      "Epoch 13/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0552 - val_loss: 1.0565\n",
      "Epoch 14/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0538 - val_loss: 1.0552\n",
      "Epoch 15/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0526 - val_loss: 1.0543\n",
      "Epoch 16/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0515 - val_loss: 1.0533\n",
      "Epoch 17/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0505 - val_loss: 1.0523\n",
      "Epoch 18/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0497 - val_loss: 1.0515\n",
      "Epoch 19/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0489 - val_loss: 1.0509\n",
      "Epoch 20/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0480 - val_loss: 1.0499\n",
      "Epoch 21/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0472 - val_loss: 1.0493\n",
      "Epoch 22/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0464 - val_loss: 1.0484\n",
      "Epoch 23/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0456 - val_loss: 1.0475\n",
      "Epoch 24/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0448 - val_loss: 1.0467\n",
      "Epoch 25/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0440 - val_loss: 1.0459\n",
      "Epoch 26/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0430 - val_loss: 1.0449\n",
      "Epoch 27/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0421 - val_loss: 1.0438\n",
      "Epoch 28/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0411 - val_loss: 1.0430\n",
      "Epoch 29/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0400 - val_loss: 1.0417\n",
      "Epoch 30/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0389 - val_loss: 1.0406\n",
      "Epoch 31/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0378 - val_loss: 1.0395\n",
      "Epoch 32/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0368 - val_loss: 1.0383\n",
      "Epoch 33/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0359 - val_loss: 1.0372\n",
      "Epoch 34/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0349 - val_loss: 1.0364\n",
      "Epoch 35/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0341 - val_loss: 1.0356\n",
      "Epoch 36/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0332 - val_loss: 1.0345\n",
      "Epoch 37/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0323 - val_loss: 1.0336\n",
      "Epoch 38/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0315 - val_loss: 1.0328\n",
      "Epoch 39/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0307 - val_loss: 1.0319\n",
      "Epoch 40/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0299 - val_loss: 1.0311\n",
      "Epoch 41/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0291 - val_loss: 1.0303\n",
      "Epoch 42/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0284 - val_loss: 1.0297\n",
      "Epoch 43/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0277 - val_loss: 1.0288\n",
      "Epoch 44/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0270 - val_loss: 1.0281\n",
      "Epoch 45/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0263 - val_loss: 1.0274\n",
      "Epoch 46/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0257 - val_loss: 1.0268\n",
      "Epoch 47/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0251 - val_loss: 1.0262\n",
      "Epoch 48/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0245 - val_loss: 1.0255\n",
      "Epoch 49/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0239 - val_loss: 1.0247\n",
      "Epoch 50/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0232 - val_loss: 1.0242\n",
      "Epoch 51/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0227 - val_loss: 1.0236\n",
      "Epoch 52/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0221 - val_loss: 1.0231\n",
      "Epoch 53/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0216 - val_loss: 1.0225\n",
      "Epoch 54/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0210 - val_loss: 1.0221\n",
      "Epoch 55/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0205 - val_loss: 1.0215\n",
      "Epoch 56/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0201 - val_loss: 1.0210\n",
      "Epoch 57/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0198 - val_loss: 1.0207\n",
      "Epoch 58/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0194 - val_loss: 1.0202\n",
      "Epoch 59/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0190 - val_loss: 1.0198\n",
      "Epoch 60/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0187 - val_loss: 1.0194\n",
      "Epoch 61/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0183 - val_loss: 1.0191\n",
      "Epoch 62/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0180 - val_loss: 1.0186\n",
      "Epoch 63/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0177 - val_loss: 1.0183\n",
      "Epoch 64/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0174 - val_loss: 1.0182\n",
      "Epoch 65/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0171 - val_loss: 1.0177\n",
      "Epoch 66/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0168 - val_loss: 1.0175\n",
      "Epoch 67/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0165 - val_loss: 1.0172\n",
      "Epoch 68/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0162 - val_loss: 1.0170\n",
      "Epoch 69/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0160 - val_loss: 1.0167\n",
      "Epoch 70/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0157 - val_loss: 1.0164\n",
      "Epoch 71/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0155 - val_loss: 1.0162\n",
      "Epoch 72/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0153 - val_loss: 1.0159\n",
      "Epoch 73/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0150 - val_loss: 1.0159\n",
      "Epoch 74/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0148 - val_loss: 1.0155\n",
      "Epoch 75/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0145 - val_loss: 1.0150\n",
      "Epoch 76/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0143 - val_loss: 1.0150\n",
      "Epoch 77/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0141 - val_loss: 1.0147\n",
      "Epoch 78/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0139 - val_loss: 1.0143\n",
      "Epoch 79/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0136 - val_loss: 1.0141\n",
      "Epoch 80/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0135 - val_loss: 1.0139\n",
      "Epoch 81/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0132 - val_loss: 1.0137\n",
      "Epoch 82/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0130 - val_loss: 1.0135\n",
      "Epoch 83/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0129 - val_loss: 1.0134\n",
      "Epoch 84/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0127 - val_loss: 1.0132\n",
      "Epoch 85/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0126 - val_loss: 1.0128\n",
      "Epoch 86/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0123 - val_loss: 1.0127\n",
      "Epoch 87/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0122 - val_loss: 1.0127\n",
      "Epoch 88/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0120 - val_loss: 1.0125\n",
      "Epoch 89/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0119 - val_loss: 1.0122\n",
      "Epoch 90/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0116 - val_loss: 1.0122\n",
      "Epoch 91/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0115 - val_loss: 1.0119\n",
      "Epoch 92/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0113 - val_loss: 1.0117\n",
      "Epoch 93/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0112 - val_loss: 1.0115\n",
      "Epoch 94/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0111 - val_loss: 1.0115\n",
      "Epoch 95/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0109 - val_loss: 1.0115\n",
      "Epoch 96/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0107 - val_loss: 1.0117\n",
      "Epoch 97/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0106 - val_loss: 1.0109\n",
      "Epoch 98/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0105 - val_loss: 1.0108\n",
      "Epoch 99/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0103 - val_loss: 1.0106\n",
      "Epoch 100/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0102 - val_loss: 1.0106\n",
      "Epoch 101/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0100 - val_loss: 1.0107\n",
      "Epoch 102/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0099 - val_loss: 1.0103\n",
      "Epoch 103/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0098 - val_loss: 1.0103\n",
      "Epoch 104/200\n",
      "676/676 [==============================] - 2s 2ms/step - loss: 1.0097 - val_loss: 1.0099\n",
      "Epoch 105/200\n",
      "676/676 [==============================] - 2s 2ms/step - loss: 1.0095 - val_loss: 1.0099\n",
      "Epoch 106/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0094 - val_loss: 1.0101\n",
      "Epoch 107/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0093 - val_loss: 1.0098\n",
      "Epoch 108/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0092 - val_loss: 1.0095\n",
      "Epoch 109/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0091 - val_loss: 1.0093\n",
      "Epoch 110/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0090 - val_loss: 1.0096\n",
      "Epoch 111/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0089 - val_loss: 1.0091\n",
      "Epoch 112/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0087 - val_loss: 1.0091\n",
      "Epoch 113/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0086 - val_loss: 1.0089\n",
      "Epoch 114/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0085 - val_loss: 1.0091\n",
      "Epoch 115/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0084 - val_loss: 1.0086\n",
      "Epoch 116/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0084 - val_loss: 1.0084\n",
      "Epoch 117/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0082 - val_loss: 1.0086\n",
      "Epoch 118/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0081 - val_loss: 1.0085\n",
      "Epoch 119/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0080 - val_loss: 1.0082\n",
      "Epoch 120/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0080 - val_loss: 1.0080\n",
      "Epoch 121/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0078 - val_loss: 1.0080\n",
      "Epoch 122/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0078 - val_loss: 1.0080\n",
      "Epoch 123/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0076 - val_loss: 1.0077\n",
      "Epoch 124/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0075 - val_loss: 1.0078\n",
      "Epoch 125/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0075 - val_loss: 1.0077\n",
      "Epoch 126/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0073 - val_loss: 1.0076\n",
      "Epoch 127/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0074 - val_loss: 1.0076\n",
      "Epoch 128/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0072 - val_loss: 1.0073\n",
      "Epoch 129/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0071 - val_loss: 1.0076\n",
      "Epoch 130/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0071 - val_loss: 1.0074\n",
      "Epoch 131/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0070 - val_loss: 1.0070\n",
      "Epoch 132/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0069 - val_loss: 1.0069\n",
      "Epoch 133/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0068 - val_loss: 1.0069\n",
      "Epoch 134/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0067 - val_loss: 1.0068\n",
      "Epoch 135/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0066 - val_loss: 1.0068\n",
      "Epoch 136/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0066 - val_loss: 1.0070\n",
      "Epoch 137/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0065 - val_loss: 1.0067\n",
      "Epoch 138/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0064 - val_loss: 1.0069\n",
      "Epoch 139/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0064 - val_loss: 1.0066\n",
      "Epoch 140/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0063 - val_loss: 1.0061\n",
      "Epoch 141/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0062 - val_loss: 1.0063\n",
      "Epoch 142/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0062 - val_loss: 1.0063\n",
      "Epoch 143/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0061 - val_loss: 1.0064\n",
      "Epoch 144/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0060 - val_loss: 1.0060\n",
      "Epoch 145/200\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0059 - val_loss: 1.0060\n",
      "Epoch 146/200\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0058 - val_loss: 1.0062\n",
      "Epoch 147/200\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0058 - val_loss: 1.0057\n",
      "Epoch 148/200\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0057 - val_loss: 1.0060\n",
      "Epoch 149/200\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0056 - val_loss: 1.0055\n",
      "Epoch 150/200\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0056 - val_loss: 1.0054\n",
      "Epoch 151/200\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0055 - val_loss: 1.0055\n",
      "Epoch 152/200\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0055 - val_loss: 1.0055\n",
      "Epoch 153/200\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0054 - val_loss: 1.0051\n",
      "Epoch 154/200\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0053 - val_loss: 1.0053\n",
      "Epoch 155/200\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0053 - val_loss: 1.0053\n",
      "Epoch 156/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0052 - val_loss: 1.0051\n",
      "Epoch 157/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0052 - val_loss: 1.0052\n",
      "Epoch 158/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0051 - val_loss: 1.0051\n",
      "Epoch 159/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0050 - val_loss: 1.0052\n",
      "Epoch 160/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0049 - val_loss: 1.0050\n",
      "Epoch 161/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0049 - val_loss: 1.0046\n",
      "Epoch 162/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0047 - val_loss: 1.0048\n",
      "Epoch 163/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0047 - val_loss: 1.0052\n",
      "Epoch 164/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0047 - val_loss: 1.0045\n",
      "Epoch 165/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0047 - val_loss: 1.0047\n",
      "Epoch 166/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0045 - val_loss: 1.0048\n",
      "Epoch 167/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0046 - val_loss: 1.0042\n",
      "Epoch 168/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0046 - val_loss: 1.0042\n",
      "Epoch 169/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0044 - val_loss: 1.0041\n",
      "Epoch 170/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0044 - val_loss: 1.0041\n",
      "Epoch 171/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0042 - val_loss: 1.0042\n",
      "Epoch 172/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0043 - val_loss: 1.0042\n",
      "Epoch 173/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0043 - val_loss: 1.0038\n",
      "Epoch 174/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0041 - val_loss: 1.0041\n",
      "Epoch 175/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0041 - val_loss: 1.0041\n",
      "Epoch 176/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0041 - val_loss: 1.0041\n",
      "Epoch 177/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0040 - val_loss: 1.0039\n",
      "Epoch 178/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0040 - val_loss: 1.0035\n",
      "Epoch 179/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0038 - val_loss: 1.0040\n",
      "Epoch 180/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0039 - val_loss: 1.0037\n",
      "Epoch 181/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0038 - val_loss: 1.0036\n",
      "Epoch 182/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0037 - val_loss: 1.0036\n",
      "Epoch 183/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0037 - val_loss: 1.0033\n",
      "Epoch 184/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0036 - val_loss: 1.0036\n",
      "Epoch 185/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0035 - val_loss: 1.0034\n",
      "Epoch 186/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0036 - val_loss: 1.0033\n",
      "Epoch 187/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0035 - val_loss: 1.0034\n",
      "Epoch 188/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0035 - val_loss: 1.0033\n",
      "Epoch 189/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0034 - val_loss: 1.0033\n",
      "Epoch 190/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0034 - val_loss: 1.0030\n",
      "Epoch 191/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0033 - val_loss: 1.0033\n",
      "Epoch 192/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0032 - val_loss: 1.0034\n",
      "Epoch 193/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0032 - val_loss: 1.0029\n",
      "Epoch 194/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0031 - val_loss: 1.0030\n",
      "Epoch 195/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0031 - val_loss: 1.0030\n",
      "Epoch 196/200\n",
      "676/676 [==============================] - 2s 2ms/step - loss: 1.0031 - val_loss: 1.0031\n",
      "Epoch 197/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0031 - val_loss: 1.0028\n",
      "Epoch 198/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0030 - val_loss: 1.0028\n",
      "Epoch 199/200\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0030 - val_loss: 1.0027\n",
      "Epoch 200/200\n",
      "676/676 [==============================] - 2s 2ms/step - loss: 1.0029 - val_loss: 1.0026\n"
     ]
    }
   ],
   "source": [
    "history = mlpclassifier.fit(\n",
    "    train_X, train_y_multi,\n",
    "    validation_split=0.2, epochs=epochs, batch_size=batch_size,\n",
    "    callbacks=[callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAu80lEQVR4nO3de3RU5b3/8c+emWQIkIT7JXIRqYXK7afUsqin1laK5ketrS61HPoTL9Vq41Fr68/m/JZa6tFQPcva26KeLgusequu5aXHHssBFajlIhc59dKiUIQoIN7IJJBM5vL8/piZnRmSAJPM7E2eeb/W2mZmz94z350NzMdnP8+zHWOMEQAAQAEE/C4AAADYg2ABAAAKhmABAAAKhmABAAAKhmABAAAKhmABAAAKhmABAAAKhmABAAAKJuT1ByaTSe3du1eVlZVyHMfrjwcAAD1gjFFzc7NqamoUCHTfLuF5sNi7d6/Gjh3r9ccCAIACaGxs1JgxY7p93fNgUVlZKSlVWFVVldcfDwAAeiASiWjs2LHu93h3PA8WmcsfVVVVBAsAAPqYY3VjoPMmAAAoGIIFAAAoGIIFAAAomLyDRXNzs26++WaNHz9eFRUV+vznP69NmzYVozYAANDH5B0svv3tb2vlypX63e9+p9dee01z587VnDlz9N577xWjPgAA0Ic4xhhzvBu3traqsrJSzz77rObNm+eunzlzpmpra/Vv//Zvx3yPSCSi6upqNTU1MSoEAIA+4ni/v/NqsYjH40okEurXr1/O+oqKCr388ss9qxQAAFgjr2BRWVmp2bNn66677tLevXuVSCT08MMPa/369dq3b1+X+0SjUUUikZwFAADYKe8+Fr/73e9kjNFJJ52kcDisn//855o/f36384Y3NDSourraXZjOGwAAe+XVxyLboUOHFIlENHr0aF122WVqaWnRH//4x07bRaNRRaNR93lmSlD6WAAA0Hccbx+LHk/pPWDAAA0YMECffPKJVqxYoXvvvbfL7cLhsMLhcE8/BgAA9CF5B4sVK1bIGKNJkyZpx44duvXWWzV58mRdeeWVxagPAAD0IXkHi6amJtXX1+vdd9/VkCFDdPHFF+vuu+9WWVlZMeo7bvf/93Y1tcb03S99SiOr+h17BwAAUHB5B4tLL71Ul156aTFq6ZXHNjXqg+aoLj1zLMECAACfWHOvkLJA6jau8USP+qICAIACsCZYhIKpQ4knkz5XAgBA6bImWJQFUy0WMVosAADwjUXBInUosQQtFgAA+MWaYBEK0scCAAC/2RMsArRYAADgN2uCRbl7KYQWCwAA/GJNsHAvhTAqBAAA31gULGixAADAb9YEi3K38yYtFgAA+MWaYEHnTQAA/GdPsGCCLAAAfGdNsChnSm8AAHxnTbCgxQIAAP9ZFCzoYwEAgN+sCRbcNh0AAP/ZEywyLRb0sQAAwDfWBAv3UkicFgsAAPxiTbAoY0pvAAB8Z1GwYEpvAAD8Zk2w6BhuSosFAAB+sSZYlKWn9OZeIQAA+MeaYOG2WCS5FAIAgF+sCRZuH4s4LRYAAPjFomCRGRVCiwUAAH6xJlhw23QAAPxnTbAoC2U6b9JiAQCAX+wJFgGGmwIA4DdrgoU7pTd9LAAA8I01wcLtvEmLBQAAvrEoWNB5EwAAv1kTLEKBTIsFl0IAAPBLXsEikUjo9ttv14QJE1RRUaGJEyfqrrvukjH+f5l39LGgxQIAAL+E8tn4Jz/5iZYsWaLly5drypQp2rx5s6688kpVV1frxhtvLFaNx6U8yHBTAAD8llewWLdunS688ELNmzdPknTyySfrscce0yuvvFKU4vLB3U0BAPBfXpdCPv/5z+uFF17QW2+9JUn6n//5H7388suqra3tdp9oNKpIJJKzFEOZGyxosQAAwC95tVj88Ic/VCQS0eTJkxUMBpVIJHT33XdrwYIF3e7T0NCgRYsW9brQYykLctt0AAD8lleLxRNPPKFHHnlEjz76qLZu3arly5fr3//937V8+fJu96mvr1dTU5O7NDY29rrorridN2mxAADAN3m1WNx666364Q9/qG9+85uSpGnTpmn37t1qaGjQwoULu9wnHA4rHA73vtJjcKf0ZlQIAAC+yavF4vDhwwoEcncJBoNKngBf5plLIcZICab1BgDAF3m1WFxwwQW6++67NW7cOE2ZMkWvvvqq7r//fl111VXFqu+4ZUaFSKmRIcFA0MdqAAAoTXkFi1/84he6/fbb9d3vflcHDhxQTU2NvvOd7+iOO+4oVn3HLdNiIaWCRb8yggUAAF7LK1hUVlbqgQce0AMPPFCkcnouM6W3xCRZAAD4xZp7hQQDjpx0tqADJwAA/rAmWDiOo7IAQ04BAPCTNcFC6ujAySRZAAD4w6pgUcYkWQAA+MqyYJFusaCPBQAAvrAqWIQyfSzitFgAAOAHq4JFWYhpvQEA8JNdwSKQucMpLRYAAPjBqmCRGRUSY1QIAAC+sCtYuPNYECwAAPCDVcGiLMSlEAAA/GRXsAgw3BQAAD9ZFSwyfSzaabEAAMAXVgWLzMybTOkNAIA/LA0WtFgAAOAHq4JFKJC5FEKLBQAAfrAnWKz7pb5+cJmGqYlLIQAA+MSeYPGXn+l/f/w7DXcOKp7kUggAAH6wJ1iEwpKkMsW5bToAAD6xJ1gEyyRlggWXQgAA8INFwaJcklTuxOljAQCATywKFqkWi3LFFaOPBQAAvrAoWGT1sYjTYgEAgB8sChapSyFlijMqBAAAn1gULDKXQmJ03gQAwCcWBYvszpu0WAAA4Ad7goU7j0WCFgsAAHxiT7DInseCPhYAAPjComCRvhSiGPNYAADgE4uCBTNvAgDgN4uCRaqPRbnDvUIAAPCLRcEiex4LWiwAAPBDXsHi5JNPluM4nZa6urpi1Xf8sqf0psUCAABfhPLZeNOmTUokEu7z119/XV/5yld0ySWXFLywvGW1WNDHAgAAf+QVLIYPH57zfPHixZo4caK++MUvFrSoHknPY1EuJsgCAMAveQWLbO3t7Xr44Yd1yy23yHGcbreLRqOKRqPu80gk0tOPPDpGhQAA4Lsed9585plndPDgQV1xxRVH3a6hoUHV1dXuMnbs2J5+5NFlLoU4BAsAAPzS42Dx0EMPqba2VjU1NUfdrr6+Xk1NTe7S2NjY0488OneCLO5uCgCAX3p0KWT37t1atWqVnnrqqWNuGw6HFQ6He/Ix+ckebkofCwAAfNGjFoulS5dqxIgRmjdvXqHr6bmsFot2LoUAAOCLvINFMpnU0qVLtXDhQoVCPe77WXhZnTe5VwgAAP7IO1isWrVKe/bs0VVXXVWMenou02LhcCkEAAC/5N3kMHfuXBlzAn5xp+exSN02nRYLAAD8YNG9QpjSGwAAv1kULDpGhSSS5sRsVQEAwHLWBYtyxSSJVgsAAHxgXbAoc1I3SWP2TQAAvGdfsFBckhgZAgCAD6wLFuXpYMHIEAAAvGdRsMiMCkn1saDFAgAA79kTLLLmsZDoYwEAgB/sCRbpSyFBxyigJMECAAAfWBQsytyHZdw6HQAAX1gULMrdh2HF1B6nxQIAAK9ZGSxosQAAwB/2BAvHkQLcOh0AAD/ZEyykrNk3uREZAAB+sCxYZN/hlBYLAAC8ZlewSM9lUa644sy8CQCA5+wKFln3C+FSCAAA3rMsWHR03uRSCAAA3rMsWKRvRObEuVcIAAA+sDNY0GIBAIAvrAwWTJAFAIA/rA0WtFgAAOA9y4JFdudNWiwAAPCaXcEiPY9F2InRYgEAgA/sChbupZAEdzcFAMAHlgWLjksh0XjC52IAACg9lgWLjs6btFgAAOA9K4NFuWIECwAAfGBnsHDiihIsAADwnJXBgkshAAD4w7Jgkem8mVCU4aYAAHjOrmCRnseCFgsAAPyRd7B477339K1vfUtDhw5VRUWFpk2bps2bNxejtvylL4WEFaOPBQAAPgjls/Enn3yis846S1/60pf0/PPPa/jw4Xr77bc1ePDgYtWXn6x5LNqZxwIAAM/lFSx+8pOfaOzYsVq6dKm7bsKECQUvqscynTcdLoUAAOCHvC6F/OEPf9BnP/tZXXLJJRoxYoROP/10/eY3vznqPtFoVJFIJGcpmqxRIVwKAQDAe3kFi3/84x9asmSJTj31VK1YsULXX3+9brzxRi1fvrzbfRoaGlRdXe0uY8eO7XXR3XInyKLFAgAAP+QVLJLJpM444wzdc889Ov3003Xttdfqmmuu0a9//etu96mvr1dTU5O7NDY29rrobmUHC4abAgDgubyCxejRo3XaaaflrPvMZz6jPXv2dLtPOBxWVVVVzlI0OZ03CRYAAHgtr2Bx1llnafv27Tnr3nrrLY0fP76gRfVYVudN+lgAAOC9vILF9773PW3YsEH33HOPduzYoUcffVT/8R//obq6umLVl5/0BFn0sQAAwB95BYszzzxTTz/9tB577DFNnTpVd911lx544AEtWLCgWPXlJ30phGABAIA/8prHQpK++tWv6qtf/Woxaum97JuQJZIyxshxHJ+LAgCgdNh1r5CsYCGJfhYAAHjMymBR7qSCBUNOAQDwlpXBItNiQT8LAAC8ZWWwKOdSCAAAvrAsWHRMkCXRYgEAgNfsChaZeSwcggUAAH6wK1ikL4UElVRASYIFAAAesyxYlLkPU7dOT/hYDAAApceyYFHuPmT2TQAAvGdtsChTXFHmsQAAwFN2BQvHkQKZ+4XEFI0RLAAA8JJdwULKuXU6M28CAOAtC4MFdzgFAMAv9gWL9FwWZUoQLAAA8Jh9wcKd1jvGcFMAADxmYbDomNabFgsAALxlYbDouHU6wQIAAG9ZGyzKxKgQAAC8Zm2wKFec26YDAOAxa4MFfSwAAPCehcGio/MmLRYAAHjLvmCRnsei3OHupgAAeM2+YJHVx4JLIQAAeMvCYME8FgAA+MXCYJG+FKIYw00BAPCYfcEi3ccizG3TAQDwnH3BoqxCktTPaafFAgAAj9kXLLJaLOhjAQCAtywMFukWC7Uz3BQAAI9ZGCxSLRb91E6LBQAAHrMvWKT7WIQdLoUAAOC1vILFj370IzmOk7NMnjy5WLX1TKifpHSLBZ03AQDwVCjfHaZMmaJVq1Z1vEEo77corqxgwXBTAAC8lXcqCIVCGjVqVDFqKYyyVLAIOzFFabEAAMBTefexePvtt1VTU6NTTjlFCxYs0J49e4pRV8+lR4WE0503jTE+FwQAQOnIq8Vi1qxZWrZsmSZNmqR9+/Zp0aJF+sIXvqDXX39dlZWVXe4TjUYVjUbd55FIpHcVH4s7KiQmSYoljMpDTnE/EwAASMozWNTW1rqPp0+frlmzZmn8+PF64okndPXVV3e5T0NDgxYtWtS7KvNR1tFiIUnReELlIfsGvwAAcCLq1TfuoEGD9OlPf1o7duzodpv6+no1NTW5S2NjY28+8tgynTedVIsFQ04BAPBOr4JFS0uLdu7cqdGjR3e7TTgcVlVVVc5SVOlgkWmxYMgpAADeyStY/OAHP9CaNWv0zjvvaN26dfrGN76hYDCo+fPnF6u+/JVlhpumWiwYcgoAgHfy6mPx7rvvav78+froo480fPhw/dM//ZM2bNig4cOHF6u+/IU67m4qGVosAADwUF7B4vHHHy9WHYWTHhUSkFGZEvSxAADAQ/YNl0iPCpEydzglWAAA4BX7gkWwXFJq3gpunQ4AgLfsCxaO0zEyhDucAgDgKfuChdRxv5D0tN4AAMAbdgaL7DucEiwAAPCM1cEiLC6FAADgJTuDRVnHXBbMYwEAgHfsDBbuHU7pYwEAgJcsDRaZO5zGGG4KAICH7AwWZR2dN2mxAADAO3YGC/fW6QQLAAC8ZHWwCCumKJ03AQDwjJ3BIjMqRO3cNh0AAA/ZGSwyo0IYbgoAgKcsDRYdo0LoYwEAgHfsDBZlTOkNAIAf7AwWOVN6M48FAABesTtYOLRYAADgJauDRT+1qy1GiwUAAF6xM1iUdVwKaWW4KQAAnrEzWIQ65rFoa6fFAgAAr1gaLDLzWMTUyqUQAAA8Y2ewKMvMY9Guw7RYAADgGTuDhdt5M0bnTQAAPGR5sGhXaywhY4zPBQEAUBrsDBaZUSFOTImk4X4hAAB4xM5gkTUqRJLa2gkWAAB4wdJgkR4Vkg4WjAwBAMAbdgaL9KiQcicuR0kdbo/7XBAAAKXBzmCR7rwpZWbfpMUCAAAvWB8suF8IAADesTNYBENSICQp3WJB500AADzRq2CxePFiOY6jm2++uUDlFFBmZIjTTh8LAAA80uNgsWnTJj344IOaPn16IespnKyRIfSxAADAGz0KFi0tLVqwYIF+85vfaPDgwYWuqTDc+4UwrTcAAF7pUbCoq6vTvHnzNGfOnGNuG41GFYlEchZPZLVYcCMyAAC8Ecp3h8cff1xbt27Vpk2bjmv7hoYGLVq0KO/Cei3dxyLMrdMBAPBMXi0WjY2Nuummm/TII4+oX79+x95BUn19vZqamtylsbGxR4XmrazjRmRttFgAAOCJvFostmzZogMHDuiMM85w1yUSCa1du1a//OUvFY1GFQwGc/YJh8MKh8OFqTYfWXc45VIIAADeyCtYnHvuuXrttddy1l155ZWaPHmybrvttk6hwlehjjuccikEAABv5BUsKisrNXXq1Jx1AwYM0NChQzut913WpZDDBAsAADxh58ybUkeLhdrVyqUQAAA8kfeokCOtXr26AGUUgRssuBQCAIBX7G2xKOuY0psWCwAAvGFvsHAnyGLmTQAAvGJxsMhM6c1wUwAAvGJvsHBHhdDHAgAAr9gbLDITZDntXAoBAMAj1geLsGJcCgEAwCP2Bov0qJAKRdUaS8gY43NBAADYz95gUT5AklThRGWMFI0nfS4IAAD7WR8sBqpNkpjLAgAAD1gcLColSQOcdLCgAycAAEVncbBIt1gQLAAA8Iy9wSI8UJI0QK2SuBQCAIAX7A0W5alg0U/tCihJiwUAAB6wPlhI0gC10WIBAIAH7A0WobAUSN0Vvr/aaLEAAMAD9gYLx8nqwNlKiwUAAB6wN1hI7pDT/unZNwEAQHFZHixosQAAwEt2B4v0kFP6WAAA4A27g0W6xYJRIQAAeMPyYNExrTctFgAAFJ/lwaKjxeIwLRYAABSd3cHCnda7TW20WAAAUHR2B4tMi4VDHwsAALxgebBI97FgVAgAAJ6wPFhkWiyYxwIAAC/YHSzcPhbMvAkAgBfsDhblmWDRSrAAAMADJREs+tN5EwAAT9gdLNKXQgbSeRMAAE/YHSzSnTdpsQAAwBuWB4vUcNNMi0UiaXwuCAAAu+UVLJYsWaLp06erqqpKVVVVmj17tp5//vli1dZ7mRYLtUkyammL+1sPAACWyytYjBkzRosXL9aWLVu0efNmffnLX9aFF16oN954o1j19U66j0XISSqsmCJtMZ8LAgDAbqF8Nr7gggtynt99991asmSJNmzYoClTphS0sIIoG+A+HKhWggUAAEWWV7DIlkgk9OSTT+rQoUOaPXt2t9tFo1FFo1H3eSQS6elH5i8QSIWL2CH1d9rUzKUQAACKKu/Om6+99poGDhyocDis6667Tk8//bROO+20brdvaGhQdXW1u4wdO7ZXBect3c9ioNoUaaXFAgCAYso7WEyaNEnbtm3Txo0bdf3112vhwoV68803u92+vr5eTU1N7tLY2NirgvOW7mfRX7RYAABQbHlfCikvL9enPvUpSdLMmTO1adMm/exnP9ODDz7Y5fbhcFjhcLh3VfZGpsXCaVMzfSwAACiqXs9jkUwmc/pQnHDSc1nQYgEAQPHl1WJRX1+v2tpajRs3Ts3NzXr00Ue1evVqrVixolj19Z7bYsGoEAAAii2vYHHgwAFdfvnl2rdvn6qrqzV9+nStWLFCX/nKV4pVX++5fSyitFgAAFBkeQWLhx56qFh1FE+6xWKAWvUhwQIAgKKy+14hktvHYoDTxqUQAACKzP5gkb4UMkBtitBiAQBAUdkfLNxLIQw3BQCg2EogWKRbLJjSGwCAoiudYEGLBQAARWd/sAh3tFi0xZJqjyd9LggAAHvZHyyy+lhIotUCAIAiKoFgkRpuOtDJBAv6WQAAUCwlECzSLRYECwAAis7+YJE1j4UkJskCAKCI7A8W/aolSWG1K6x2+lgAAFBE9geLcJUUSN0SZZBamH0TAIAisj9YOI5UMViSNMRppo8FAABFZH+wkKSKIZKkQU6LIq1cCgEAoFhKI1j0HypJGixaLAAAKKYSCRapFovBTgudNwEAKKLSCBbpPhapzpsECwAAiqU0gkW6xYLOmwAAFFdpBAu38ybBAgCAYiqNYJHpYyH6WAAAUEwlEizSo0IcJsgCAKCYSiNYZC6FqFnNbTEZY3wuCAAAO5VGsMgabhpLGEXjSZ8LAgDATqURLNzOm4cUVIIhpwAAFEmJBIvB7sNqHVKklX4WAAAUQ2kEi2BICqdunz7YaWZkCAAARVIawUJy+1kMUos+OdzuczEAANip5ILFYKdFHzYTLAAAKIbSCRYVmWDRrA9aoj4XAwCAnUonWLizbzbrg2aCBQAAxVA6waKi41IIwQIAgOIonWCR1XnzQHObz8UAAGCnvIJFQ0ODzjzzTFVWVmrEiBH6+te/ru3btxertsLqT4sFAADFllewWLNmjerq6rRhwwatXLlSsVhMc+fO1aFDh4pVX+Fkd94kWAAAUBShfDb+05/+lPN82bJlGjFihLZs2aKzzz67oIUVXFbnzUPtCR2KxjUgnNfhAwCAY+jVN2tTU5MkaciQId1uE41GFY12tBBEIpHefGTPZXXelKQPW6IECwAACqzHnTeTyaRuvvlmnXXWWZo6dWq32zU0NKi6utpdxo4d29OP7J3+2cHCcDkEAIAi6HGwqKur0+uvv67HH3/8qNvV19erqanJXRobG3v6kb2TbrEIKaGBaiVYAABQBD26FnDDDTfoueee09q1azVmzJijbhsOhxUOh3tUXEGV95dCFVK8VYOcFmbfBACgCPJqsTDG6IYbbtDTTz+tF198URMmTChWXcWRvhwyhNk3AQAoirxaLOrq6vToo4/q2WefVWVlpfbv3y9Jqq6uVkVFRVEKLKgBw6XIexrhHCRYAABQBHm1WCxZskRNTU0655xzNHr0aHf5/e9/X6z6Cqs6ddnmJOdDggUAAEWQV4uFMaZYdXhj0DhJqWCxlWABAEDBlc69QiSpOjXU9STnA1osAAAogtIKFoMyweJDfdgSVTLZx1tgAAA4wZRWsKjuCBbxpNHB1pjPBQEAYJfSChbpPhbDnYjCaudyCAAABVZawaJisFQ2QBIjQwAAKIbSChaOk9PP4oOWNp8LAgDALqUVLKScfha0WAAAUFilFyyyWize/aTV52IAALBL6QWLrBaLf3xwyOdiAACwS+kFi6zZN3d+0OJzMQAA2KX0gkVWi8W+pja1ROM+FwQAgD1KL1ik+1iMcj5WUAnt4nIIAAAFU3rBYuAoKVCmkJIapY+5HAIAQAGVXrAIBKTqkyTRzwIAgEIrvWAh5fSzIFgAAFA4pRksBo2XJI11PtDOA/SxAACgUEozWIyYLEmaEnhHuz48pAS3TwcAoCBKM1ic9FlJ0v8K7FR7IqF3Pznsc0EAANihNIPF6BmSE9RI5xNGhgAAUEClGSzK+0sjT5OUarWgnwUAAIVRmsFCyrocsoMWCwAACqSEg8VMSdLpgR362/5mn4sBAMAOpRssxqRaLKY5u/Tmux/p40PtPhcEAEDfV7rBYtinpfJK9Xeimqj39NLfD/hdEQAAfV7pBotAUDrpdEmpfhYv/P19nwsCAKDvK91gIbn9LGY4O7X2rQ/VHk/6XBAAAH1baQeLsbMkSV8JvapY9LA27vrI54IAAOjbSjtYTDxXqhqjYTqoi4J/1gt/o58FAAC9UdrBIlQuff5fJEnfCT6nF9/Yq3iCyyEAAPRUaQcLSTrj/8hUDNHJgfc1vXmNfrrqLb8rAgCgzyJYlA+QM+s6SdJNoaf0zEsb9CIjRAAA6JG8g8XatWt1wQUXqKamRo7j6JlnnilCWR773DVSv0E6NfCeVob/r/72+P/T448t1batG7V3z05FDn6oZCwqGW6vDgDA0YTy3eHQoUOaMWOGrrrqKl100UXFqMl7/YdIV/+3kn+4Sf0b16tOT0rbn5S2524WV0BRhdXm9FO7ypR0Qko4ISWdoIwTVNIJKemEZAJBmUBIJv1cgaCME0qtC4RknKBMoCw1l0Z6ndKLk/kZzP4ZVCAYkgJlCgRT2zjB1BIIhhQIlikQLJMTDCkYCskJlikYLFOgrEyBQEiBUOZ5SKFgmQKhkEKhcve9lLPQiAUA6Lm8g0Vtba1qa2uLUYu/hk9S4Krn1b71UX286QklP9mtyuj7qjBtCjmpDp0hJRVSqwaY1tQ+FjZgJOUooaASCqR/BpVwgkqmHyedgJKZ5+lQlXR/pgJWJmSlAlTmZypcKR26lH6cCVZOZn06XHVsH5KcoBRw5CggJ+BITkByAnIcR04gIMcJSI4jxwm661LPA3ICQTmOOr3m7u8E048lpbcx6edOIJi1Xcd7Kv04EAhICuR8nvszEFAgva+OqDGzr6NM/UGZQLpex5HjSAFHqeN1lN6v47njOAo4TnpbR5LTcQKdzGPnGM+7WJfzGgD0TN7BwmqOo/KZCzRq5oKc1W1trWpujqilpVnRwy1Kth9SItqqRDyWXtqVSMSUjMeViMeUTMRkEnEl4u1yknGZZEJOIiaTjMtJxqRkQjJxOcm4nERMMkk5ybhk4gqYhJRMKGDicpIJOSYhJ73eMQkFshfF3cdBJRQ0WZEgsy4dCULpx5mfZU6iy19BQEYBxVWWvdJ08xhWS5qOoJF5aNJhxCg3hHS1/ljb5P5R6vq1jvfovE/n147YxulcS8f+R+7TOVR1en+n6/Vd7e++75HB7ijbqtM+R9nmyPfoIjDmbuN0+r1kXneO3Nbp+vMya4ycrMNxuvydOk7nz8/+cdTjcY587Yj37yocZ9fWxTHm/v7VaRunu207nYsj3rfL89vdcXR+r27f3zmypg6d/0x1fcxTv3WvKquHdNrfC0UPFtFoVNFo1H0eiUSK/ZEF169fhfr1q9Dw4SP9LqVHjDFKGimeTCqZlNqSSR1KJBVPJFLBKBFTIhZXItGeCkeZkJSIK5lel0zElYzHZBIxJRPx1M9kXCYRTz9PLTJZj5NxmWTHz46QlVonk5DS653040AmRCVTASlg4nJMUpKRY4ykZPqnSQWyrPUyJvU8axvHpJ5LkpPZXib9WO5rAXW8l6OO1wJKvU8gs1/WElBSjlHO9k72e3WxXyCzOCd+Quu6xhO/blehS+1Dhw580HqHvcGioaFBixYtKvbH4Cgcx1HQkYKBYHpN8Kjbo/iMMUomjYxJppZkUsYk3P7BJmmUNEZGRsZkFknuOimZ3tgkU+tSr2U9T2+n9DNJUjLpfn76QfYPGWXN45JMv6bMNrn7uu8hI8ekv3eNu7VbW/rNsj4nu670vqZjn473yOzQxbEY9wM7Wjc6fnk5x9axT9bvP/N70BG/h0wIzbxF9n/NEceSzHpD9/WOz5TJrSH7PYxJ/b+++xtOh+JMnZnXs98vZ5/07yj1ezcdn9fFMenI58ZkbaOc1zrWJ3NelltP1p8Rk/seqXrMEfWk63S37/iNOu45zX2tU71HFtLlNlntGt0dt6vjz3hODer4/ea8R3effcT757QtdFOD02VNxv19ZK1y9+34c3K0Y+uceqcOqOq0zitFDxb19fW65ZZb3OeRSERjx44t9scCJzTHcRQMpto3AMAmRQ8W4XBY4XC42B8DAABOAHkHi5aWFu3YscN9vmvXLm3btk1DhgzRuHHjClocAADoW/IOFps3b9aXvvQl93nmMsfChQu1bNmyghUGAAD6nryDxTnnnJPViQkAAKADPccAAEDBECwAAEDBECwAAEDBECwAAEDBECwAAEDBECwAAEDBECwAAEDBECwAAEDBECwAAEDBFP0mZEfKzNoZiUS8/mgAANBDme/tY82+7XmwaG5uliRunQ4AQB/U3Nys6urqbl93jMc3/kgmk9q7d68qKyvlOE7B3jcSiWjs2LFqbGxUVVVVwd73RGL7Mdp+fBLHaAPbj0/iGG1QjOMzxqi5uVk1NTUKBLrvSeF5i0UgENCYMWOK9v5VVVVW/iHJZvsx2n58EsdoA9uPT+IYbVDo4ztaS0UGnTcBAEDBECwAAEDBWBMswuGw7rzzToXDYb9LKRrbj9H245M4RhvYfnwSx2gDP4/P886bAADAXta0WAAAAP8RLAAAQMEQLAAAQMEQLAAAQMFYEyx+9atf6eSTT1a/fv00a9YsvfLKK36X1CMNDQ0688wzVVlZqREjRujrX/+6tm/fnrPNOeecI8dxcpbrrrvOp4rz96Mf/ahT/ZMnT3Zfb2trU11dnYYOHaqBAwfq4osv1vvvv+9jxfk5+eSTOx2f4ziqq6uT1DfP39q1a3XBBReopqZGjuPomWeeyXndGKM77rhDo0ePVkVFhebMmaO33347Z5uPP/5YCxYsUFVVlQYNGqSrr75aLS0tHh7F0R3tGGOxmG677TZNmzZNAwYMUE1NjS6//HLt3bs35z26OveLFy/2+Ei6dqxzeMUVV3Sq/fzzz8/Zpi+fQ0ld/r10HEf33Xefu82JfA6P5/vheP793LNnj+bNm6f+/ftrxIgRuvXWWxWPxwtWpxXB4ve//71uueUW3Xnnndq6datmzJih8847TwcOHPC7tLytWbNGdXV12rBhg1auXKlYLKa5c+fq0KFDOdtdc8012rdvn7vce++9PlXcM1OmTMmp/+WXX3Zf+973vqf//M//1JNPPqk1a9Zo7969uuiii3ysNj+bNm3KObaVK1dKki655BJ3m752/g4dOqQZM2boV7/6VZev33vvvfr5z3+uX//619q4caMGDBig8847T21tbe42CxYs0BtvvKGVK1fqueee09q1a3Xttdd6dQjHdLRjPHz4sLZu3arbb79dW7du1VNPPaXt27fra1/7Wqdtf/zjH+ec23/5l3/xovxjOtY5lKTzzz8/p/bHHnss5/W+fA4l5Rzbvn379Nvf/laO4+jiiy/O2e5EPYfH8/1wrH8/E4mE5s2bp/b2dq1bt07Lly/XsmXLdMcddxSuUGOBz33uc6aurs59nkgkTE1NjWloaPCxqsI4cOCAkWTWrFnjrvviF79obrrpJv+K6qU777zTzJgxo8vXDh48aMrKysyTTz7prvvb3/5mJJn169d7VGFh3XTTTWbixIkmmUwaY/r++ZNknn76afd5Mpk0o0aNMvfdd5+77uDBgyYcDpvHHnvMGGPMm2++aSSZTZs2uds8//zzxnEc895773lW+/E68hi78sorrxhJZvfu3e668ePHm5/+9KfFLa4Aujq+hQsXmgsvvLDbfWw8hxdeeKH58pe/nLOur5xDYzp/PxzPv5//9V//ZQKBgNm/f7+7zZIlS0xVVZWJRqMFqavPt1i0t7dry5YtmjNnjrsuEAhozpw5Wr9+vY+VFUZTU5MkaciQITnrH3nkEQ0bNkxTp05VfX29Dh8+7Ed5Pfb222+rpqZGp5xyihYsWKA9e/ZIkrZs2aJYLJZzPidPnqxx48b1yfPZ3t6uhx9+WFdddVXOTff6+vnLtmvXLu3fvz/nnFVXV2vWrFnuOVu/fr0GDRqkz372s+42c+bMUSAQ0MaNGz2vuRCamprkOI4GDRqUs37x4sUaOnSoTj/9dN13330FbWIuttWrV2vEiBGaNGmSrr/+en300Ufua7adw/fff19//OMfdfXVV3d6ra+cwyO/H47n38/169dr2rRpGjlypLvNeeedp0gkojfeeKMgdXl+E7JC+/DDD5VIJHJ+SZI0cuRI/f3vf/epqsJIJpO6+eabddZZZ2nq1Knu+n/+53/W+PHjVVNTo7/+9a+67bbbtH37dj311FM+Vnv8Zs2apWXLlmnSpEnat2+fFi1apC984Qt6/fXXtX//fpWXl3f6x3rkyJHav3+/PwX3wjPPPKODBw/qiiuucNf19fN3pMx56ervYOa1/fv3a8SIETmvh0IhDRkypE+e17a2Nt12222aP39+zg2ebrzxRp1xxhkaMmSI1q1bp/r6eu3bt0/333+/j9Uen/PPP18XXXSRJkyYoJ07d+pf//VfVVtbq/Xr1ysYDFp3DpcvX67KyspOl1n7yjns6vvheP793L9/f5d/VzOvFUKfDxY2q6ur0+uvv57T/0BSzjXNadOmafTo0Tr33HO1c+dOTZw40esy81ZbW+s+nj59umbNmqXx48friSeeUEVFhY+VFd5DDz2k2tpa1dTUuOv6+vkrdbFYTJdeeqmMMVqyZEnOa7fccov7ePr06SovL9d3vvMdNTQ0nPBTR3/zm990H0+bNk3Tp0/XxIkTtXr1ap177rk+VlYcv/3tb7VgwQL169cvZ31fOYfdfT+cCPr8pZBhw4YpGAx26vX6/vvva9SoUT5V1Xs33HCDnnvuOb300kvHvM38rFmzJEk7duzworSCGzRokD796U9rx44dGjVqlNrb23Xw4MGcbfri+dy9e7dWrVqlb3/720fdrq+fv8x5OdrfwVGjRnXqTB2Px/Xxxx/3qfOaCRW7d+/WypUrj3k76lmzZikej+udd97xpsACOuWUUzRs2DD3z6Ut51CS/vznP2v79u3H/LspnZjnsLvvh+P593PUqFFd/l3NvFYIfT5YlJeXa+bMmXrhhRfcdclkUi+88IJmz57tY2U9Y4zRDTfcoKefflovvviiJkyYcMx9tm3bJkkaPXp0kasrjpaWFu3cuVOjR4/WzJkzVVZWlnM+t2/frj179vS587l06VKNGDFC8+bNO+p2ff38TZgwQaNGjco5Z5FIRBs3bnTP2ezZs3Xw4EFt2bLF3ebFF19UMpl0g9WJLhMq3n77ba1atUpDhw495j7btm1TIBDodAmhL3j33Xf10UcfuX8ubTiHGQ899JBmzpypGTNmHHPbE+kcHuv74Xj+/Zw9e7Zee+21nJCYCcmnnXZawQrt8x5//HETDofNsmXLzJtvvmmuvfZaM2jQoJxer33F9ddfb6qrq83q1avNvn373OXw4cPGGGN27NhhfvzjH5vNmzebXbt2mWeffdaccsop5uyzz/a58uP3/e9/36xevdrs2rXL/OUvfzFz5swxw4YNMwcOHDDGGHPdddeZcePGmRdffNFs3rzZzJ4928yePdvnqvOTSCTMuHHjzG233Zazvq+ev+bmZvPqq6+aV1991Ugy999/v3n11VfdERGLFy82gwYNMs8++6z561//ai688EIzYcIE09ra6r7H+eefb04//XSzceNG8/LLL5tTTz3VzJ8/369D6uRox9je3m6+9rWvmTFjxpht27bl/N3M9KRft26d+elPf2q2bdtmdu7caR5++GEzfPhwc/nll/t8ZClHO77m5mbzgx/8wKxfv97s2rXLrFq1ypxxxhnm1FNPNW1tbe579OVzmNHU1GT69+9vlixZ0mn/E/0cHuv7wZhj//sZj8fN1KlTzdy5c822bdvMn/70JzN8+HBTX19fsDqtCBbGGPOLX/zCjBs3zpSXl5vPfe5zZsOGDX6X1COSulyWLl1qjDFmz5495uyzzzZDhgwx4XDYfOpTnzK33nqraWpq8rfwPFx22WVm9OjRpry83Jx00knmsssuMzt27HBfb21tNd/97nfN4MGDTf/+/c03vvENs2/fPh8rzt+KFSuMJLN9+/ac9X31/L300ktd/rlcuHChMSY15PT22283I0eONOFw2Jx77rmdjv2jjz4y8+fPNwMHDjRVVVXmyiuvNM3NzT4cTdeOdoy7du3q9u/mSy+9ZIwxZsuWLWbWrFmmurra9OvXz3zmM58x99xzT84Xs5+OdnyHDx82c+fONcOHDzdlZWVm/Pjx5pprrun0P2d9+RxmPPjgg6aiosIcPHiw0/4n+jk81veDMcf37+c777xjamtrTUVFhRk2bJj5/ve/b2KxWMHq5LbpAACgYPp8HwsAAHDiIFgAAICCIVgAAICCIVgAAICCIVgAAICCIVgAAICCIVgAAICCIVgAAICCIVgAAICCIVgAAICCIVgAAICCIVgAAICC+f+iOLECaQn7FwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541/541 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17289, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = mlpclassifier.predict(test_X)\n",
    "test_results = np.argmax(test_pred, axis=-1)\n",
    "test_pred.shape, test_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2638/2638 [==============================] - 3s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((84406, 3), (84406,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred = mlpclassifier.predict(train_X)\n",
    "train_results = np.argmax(train_pred, axis=-1)\n",
    "train_pred.shape, train_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49679350690694246"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# evaluate\n",
    "train_f1 = f1_score(train_y, train_results, average='macro')\n",
    "train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3개를 하나로!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84406, 39), (17289, 38), (17289, 2))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission_df = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "train_df.shape, test_df.shape, sample_submission_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mname = 'mlpclassifier_keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def make_report(template, test_pred, mname):\n",
    "    template['TARGET'] = test_pred\n",
    "    now = dt.strftime(dt.now(), '%y-%m-%d')\n",
    "    template.to_csv(f'results/{mname}-{now}-1.csv', index=False)\n",
    "    \n",
    "make_report(sample_submission_df, test_results, mname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimizer\n",
    "## early stopping\n",
    "## compile\n",
    "## train\n",
    "## evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
