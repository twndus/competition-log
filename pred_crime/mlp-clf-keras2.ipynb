{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84406, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>월</th>\n",
       "      <th>요일</th>\n",
       "      <th>시간</th>\n",
       "      <th>소관경찰서</th>\n",
       "      <th>소관지역</th>\n",
       "      <th>사건발생거리</th>\n",
       "      <th>강수량(mm)</th>\n",
       "      <th>강설량(mm)</th>\n",
       "      <th>적설량(cm)</th>\n",
       "      <th>풍향</th>\n",
       "      <th>안개</th>\n",
       "      <th>짙은안개</th>\n",
       "      <th>번개</th>\n",
       "      <th>진눈깨비</th>\n",
       "      <th>서리</th>\n",
       "      <th>연기/연무</th>\n",
       "      <th>눈날림</th>\n",
       "      <th>범죄발생지</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>9</td>\n",
       "      <td>화요일</td>\n",
       "      <td>10</td>\n",
       "      <td>137</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.611124</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>차도</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>11</td>\n",
       "      <td>화요일</td>\n",
       "      <td>6</td>\n",
       "      <td>438</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.209093</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>차도</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>8</td>\n",
       "      <td>일요일</td>\n",
       "      <td>6</td>\n",
       "      <td>1729</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.619597</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>인도</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>5</td>\n",
       "      <td>월요일</td>\n",
       "      <td>6</td>\n",
       "      <td>2337</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.921615</td>\n",
       "      <td>11.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>주거지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>9</td>\n",
       "      <td>일요일</td>\n",
       "      <td>11</td>\n",
       "      <td>1439</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.789721</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>주유소</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID   월   요일  시간  소관경찰서  소관지역    사건발생거리  강수량(mm)  강설량(mm)  적설량(cm)   \n",
       "0  TRAIN_00000   9  화요일  10    137   8.0  2.611124    0.000      0.0      0.0  \\\n",
       "1  TRAIN_00001  11  화요일   6    438  13.0  3.209093    0.000      0.0      0.0   \n",
       "2  TRAIN_00002   8  일요일   6   1729  47.0  1.619597    0.000      0.0      0.0   \n",
       "3  TRAIN_00003   5  월요일   6   2337  53.0  1.921615   11.375      0.0      0.0   \n",
       "4  TRAIN_00004   9  일요일  11   1439  41.0  1.789721    0.000      0.0      0.0   \n",
       "\n",
       "      풍향   안개  짙은안개   번개  진눈깨비   서리  연기/연무  눈날림 범죄발생지  TARGET  \n",
       "0  245.0  0.0   0.0  0.0   0.0  0.0    0.0  0.0    차도       2  \n",
       "1  200.0  0.0   0.0  0.0   0.0  0.0    0.0  0.0    차도       0  \n",
       "2   40.0  1.0   0.0  0.0   0.0  0.0    1.0  0.0    인도       1  \n",
       "3  225.0  1.0   1.0  0.0   0.0  0.0    0.0  0.0   주거지       1  \n",
       "4  255.0  0.0   0.0  0.0   0.0  0.0    0.0  0.0   주유소       2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17289, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>월</th>\n",
       "      <th>요일</th>\n",
       "      <th>시간</th>\n",
       "      <th>소관경찰서</th>\n",
       "      <th>소관지역</th>\n",
       "      <th>사건발생거리</th>\n",
       "      <th>강수량(mm)</th>\n",
       "      <th>강설량(mm)</th>\n",
       "      <th>적설량(cm)</th>\n",
       "      <th>풍향</th>\n",
       "      <th>안개</th>\n",
       "      <th>짙은안개</th>\n",
       "      <th>번개</th>\n",
       "      <th>진눈깨비</th>\n",
       "      <th>서리</th>\n",
       "      <th>연기/연무</th>\n",
       "      <th>눈날림</th>\n",
       "      <th>범죄발생지</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>9</td>\n",
       "      <td>금요일</td>\n",
       "      <td>5</td>\n",
       "      <td>927</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.570654</td>\n",
       "      <td>19.625000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>차도</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>5</td>\n",
       "      <td>수요일</td>\n",
       "      <td>3</td>\n",
       "      <td>926</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.712457</td>\n",
       "      <td>21.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>식당</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>5</td>\n",
       "      <td>월요일</td>\n",
       "      <td>6</td>\n",
       "      <td>1437</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>25.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>주거지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>11</td>\n",
       "      <td>화요일</td>\n",
       "      <td>1</td>\n",
       "      <td>1739</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.878585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>주거지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>10</td>\n",
       "      <td>목요일</td>\n",
       "      <td>10</td>\n",
       "      <td>830</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.496423</td>\n",
       "      <td>26.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>주거지</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID   월   요일  시간  소관경찰서  소관지역    사건발생거리    강수량(mm)  강설량(mm)   \n",
       "0  TEST_00000   9  금요일   5    927  28.0  1.570654  19.625000      0.0  \\\n",
       "1  TEST_00001   5  수요일   3    926  28.0  1.712457  21.444444      0.0   \n",
       "2  TEST_00002   5  월요일   6   1437  33.0  0.447496  25.200000      0.0   \n",
       "3  TEST_00003  11  화요일   1   1739  31.0  0.878585   0.000000      0.0   \n",
       "4  TEST_00004  10  목요일  10    830  15.0  0.496423  26.142857      0.0   \n",
       "\n",
       "   적설량(cm)     풍향   안개  짙은안개   번개  진눈깨비   서리  연기/연무  눈날림 범죄발생지  \n",
       "0      0.0  165.0  1.0   0.0  1.0   0.0  0.0    0.0  0.0    차도  \n",
       "1      0.0  175.0  1.0   0.0  0.0   0.0  0.0    1.0  0.0    식당  \n",
       "2      0.0  290.0  1.0   0.0  0.0   0.0  0.0    0.0  0.0   주거지  \n",
       "3      0.0  285.0  0.0   0.0  0.0   0.0  0.0    0.0  0.0   주거지  \n",
       "4      0.0   95.0  1.0   0.0  0.0   0.0  0.0    0.0  0.0   주거지  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical columns:  ['요일', '범죄발생지']\n",
      "numeric columns:  ['월', '시간', '소관경찰서', '소관지역', '사건발생거리', '강수량(mm)', '강설량(mm)', '적설량(cm)', '풍향', '안개', '짙은안개', '번개', '진눈깨비', '서리', '연기/연무', '눈날림']\n"
     ]
    }
   ],
   "source": [
    "cat_cols = []\n",
    "num_cols = []\n",
    "\n",
    "for col in test_df.columns[1:]:\n",
    "    if train_df[col].dtype == 'object':\n",
    "        cat_cols.append(col)\n",
    "    else:\n",
    "        num_cols.append(col)\n",
    "\n",
    "print('categorical columns: ', cat_cols)\n",
    "print('numeric columns: ', num_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess cat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['금요일', '목요일', '수요일', '월요일', '일요일', '토요일', '화요일', '공원', '백화점', '병원',\n",
       "       '식당', '약국', '은행', '인도', '주거지', '주유소', '주차장', '차도', '편의점', '학교',\n",
       "       '호텔/모텔'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat_array = encoder.fit_transform(train_df[cat_cols])\n",
    "test_cat_array = encoder.transform(test_df[cat_cols])\n",
    "\n",
    "encoded_cols = np.concatenate(encoder.categories_)\n",
    "encoded_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84406, 39), (17289, 38))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([\n",
    "    train_df.drop(columns=cat_cols),\n",
    "    pd.DataFrame(train_cat_array, columns=encoded_cols)], axis=1)\n",
    "    \n",
    "test_df = pd.concat([\n",
    "    test_df.drop(columns=cat_cols),\n",
    "    pd.DataFrame(test_cat_array, columns=encoded_cols)], axis=1)\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>월</th>\n",
       "      <th>시간</th>\n",
       "      <th>소관경찰서</th>\n",
       "      <th>소관지역</th>\n",
       "      <th>사건발생거리</th>\n",
       "      <th>강수량(mm)</th>\n",
       "      <th>강설량(mm)</th>\n",
       "      <th>적설량(cm)</th>\n",
       "      <th>풍향</th>\n",
       "      <th>안개</th>\n",
       "      <th>짙은안개</th>\n",
       "      <th>번개</th>\n",
       "      <th>진눈깨비</th>\n",
       "      <th>서리</th>\n",
       "      <th>연기/연무</th>\n",
       "      <th>눈날림</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.430195</td>\n",
       "      <td>6.769507</td>\n",
       "      <td>1060.027581</td>\n",
       "      <td>26.881726</td>\n",
       "      <td>1.912424</td>\n",
       "      <td>24.608776</td>\n",
       "      <td>2.284407</td>\n",
       "      <td>23.430503</td>\n",
       "      <td>186.926107</td>\n",
       "      <td>0.385423</td>\n",
       "      <td>0.017842</td>\n",
       "      <td>0.144042</td>\n",
       "      <td>0.020330</td>\n",
       "      <td>0.010260</td>\n",
       "      <td>0.210755</td>\n",
       "      <td>0.008921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.108302</td>\n",
       "      <td>3.566390</td>\n",
       "      <td>698.380485</td>\n",
       "      <td>13.870968</td>\n",
       "      <td>0.958556</td>\n",
       "      <td>62.711211</td>\n",
       "      <td>15.852881</td>\n",
       "      <td>85.199896</td>\n",
       "      <td>98.299485</td>\n",
       "      <td>0.486698</td>\n",
       "      <td>0.132379</td>\n",
       "      <td>0.351134</td>\n",
       "      <td>0.141128</td>\n",
       "      <td>0.100771</td>\n",
       "      <td>0.407847</td>\n",
       "      <td>0.094030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.012269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>526.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.209985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>937.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>1.822279</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1638.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>2.476528</td>\n",
       "      <td>18.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2450.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>4.998936</td>\n",
       "      <td>614.875000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>649.800000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  월            시간         소관경찰서          소관지역        사건발생거리   \n",
       "count  84406.000000  84406.000000  84406.000000  84406.000000  84406.000000  \\\n",
       "mean       6.430195      6.769507   1060.027581     26.881726      1.912424   \n",
       "std        3.108302      3.566390    698.380485     13.870968      0.958556   \n",
       "min        1.000000      1.000000     26.000000      5.000000      0.012269   \n",
       "25%        4.000000      4.000000    526.000000     13.000000      1.209985   \n",
       "50%        7.000000      7.000000    937.000000     27.000000      1.822279   \n",
       "75%        9.000000     10.000000   1638.000000     38.000000      2.476528   \n",
       "max       12.000000     12.000000   2450.000000     54.000000      4.998936   \n",
       "\n",
       "            강수량(mm)       강설량(mm)       적설량(cm)            풍향            안개   \n",
       "count  84406.000000  84406.000000  84406.000000  84406.000000  84406.000000  \\\n",
       "mean      24.608776      2.284407     23.430503    186.926107      0.385423   \n",
       "std       62.711211     15.852881     85.199896     98.299485      0.486698   \n",
       "min        0.000000      0.000000      0.000000     10.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000     95.000000      0.000000   \n",
       "50%        0.625000      0.000000      0.000000    205.000000      0.000000   \n",
       "75%       18.571429      0.000000      0.000000    260.000000      1.000000   \n",
       "max      614.875000    295.000000    649.800000    360.000000      1.000000   \n",
       "\n",
       "               짙은안개            번개          진눈깨비            서리         연기/연무   \n",
       "count  84406.000000  84406.000000  84406.000000  84406.000000  84406.000000  \\\n",
       "mean       0.017842      0.144042      0.020330      0.010260      0.210755   \n",
       "std        0.132379      0.351134      0.141128      0.100771      0.407847   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                눈날림  \n",
       "count  84406.000000  \n",
       "mean       0.008921  \n",
       "std        0.094030  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[num_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>월</th>\n",
       "      <th>시간</th>\n",
       "      <th>소관경찰서</th>\n",
       "      <th>소관지역</th>\n",
       "      <th>사건발생거리</th>\n",
       "      <th>강수량(mm)</th>\n",
       "      <th>강설량(mm)</th>\n",
       "      <th>적설량(cm)</th>\n",
       "      <th>풍향</th>\n",
       "      <th>안개</th>\n",
       "      <th>짙은안개</th>\n",
       "      <th>번개</th>\n",
       "      <th>진눈깨비</th>\n",
       "      <th>서리</th>\n",
       "      <th>연기/연무</th>\n",
       "      <th>눈날림</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.522002e-16</td>\n",
       "      <td>1.146132e-16</td>\n",
       "      <td>3.703988e-17</td>\n",
       "      <td>7.176477e-17</td>\n",
       "      <td>2.200506e-16</td>\n",
       "      <td>7.660520e-18</td>\n",
       "      <td>-1.043851e-17</td>\n",
       "      <td>4.322722e-17</td>\n",
       "      <td>5.219256e-17</td>\n",
       "      <td>-3.493534e-18</td>\n",
       "      <td>-4.444785e-17</td>\n",
       "      <td>1.056794e-16</td>\n",
       "      <td>-4.284841e-17</td>\n",
       "      <td>7.500576e-17</td>\n",
       "      <td>-1.052269e-17</td>\n",
       "      <td>-6.759778e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.747008e+00</td>\n",
       "      <td>-1.617754e+00</td>\n",
       "      <td>-1.480617e+00</td>\n",
       "      <td>-1.577529e+00</td>\n",
       "      <td>-1.982320e+00</td>\n",
       "      <td>-3.924166e-01</td>\n",
       "      <td>-1.441013e-01</td>\n",
       "      <td>-2.750079e-01</td>\n",
       "      <td>-1.799879e+00</td>\n",
       "      <td>-7.919185e-01</td>\n",
       "      <td>-1.347830e-01</td>\n",
       "      <td>-4.102213e-01</td>\n",
       "      <td>-1.440563e-01</td>\n",
       "      <td>-1.018150e-01</td>\n",
       "      <td>-5.167533e-01</td>\n",
       "      <td>-9.487608e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.818446e-01</td>\n",
       "      <td>-7.765621e-01</td>\n",
       "      <td>-7.646702e-01</td>\n",
       "      <td>-1.000782e+00</td>\n",
       "      <td>-7.328137e-01</td>\n",
       "      <td>-3.924166e-01</td>\n",
       "      <td>-1.441013e-01</td>\n",
       "      <td>-2.750079e-01</td>\n",
       "      <td>-9.351692e-01</td>\n",
       "      <td>-7.919185e-01</td>\n",
       "      <td>-1.347830e-01</td>\n",
       "      <td>-4.102213e-01</td>\n",
       "      <td>-1.440563e-01</td>\n",
       "      <td>-1.018150e-01</td>\n",
       "      <td>-5.167533e-01</td>\n",
       "      <td>-9.487608e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.833184e-01</td>\n",
       "      <td>6.462963e-02</td>\n",
       "      <td>-1.761623e-01</td>\n",
       "      <td>8.526751e-03</td>\n",
       "      <td>-9.404287e-02</td>\n",
       "      <td>-3.824502e-01</td>\n",
       "      <td>-1.441013e-01</td>\n",
       "      <td>-2.750079e-01</td>\n",
       "      <td>1.838667e-01</td>\n",
       "      <td>-7.919185e-01</td>\n",
       "      <td>-1.347830e-01</td>\n",
       "      <td>-4.102213e-01</td>\n",
       "      <td>-1.440563e-01</td>\n",
       "      <td>-1.018150e-01</td>\n",
       "      <td>-5.167533e-01</td>\n",
       "      <td>-9.487608e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.267604e-01</td>\n",
       "      <td>9.058214e-01</td>\n",
       "      <td>8.275945e-01</td>\n",
       "      <td>8.015547e-01</td>\n",
       "      <td>5.884964e-01</td>\n",
       "      <td>-9.627278e-02</td>\n",
       "      <td>-1.441013e-01</td>\n",
       "      <td>-2.750079e-01</td>\n",
       "      <td>7.433846e-01</td>\n",
       "      <td>1.262756e+00</td>\n",
       "      <td>-1.347830e-01</td>\n",
       "      <td>-4.102213e-01</td>\n",
       "      <td>-1.440563e-01</td>\n",
       "      <td>-1.018150e-01</td>\n",
       "      <td>-5.167533e-01</td>\n",
       "      <td>-9.487608e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.791923e+00</td>\n",
       "      <td>1.466616e+00</td>\n",
       "      <td>1.990291e+00</td>\n",
       "      <td>1.955050e+00</td>\n",
       "      <td>3.219977e+00</td>\n",
       "      <td>9.412507e+00</td>\n",
       "      <td>1.846461e+01</td>\n",
       "      <td>7.351807e+00</td>\n",
       "      <td>1.760690e+00</td>\n",
       "      <td>1.262756e+00</td>\n",
       "      <td>7.419332e+00</td>\n",
       "      <td>2.437709e+00</td>\n",
       "      <td>6.941732e+00</td>\n",
       "      <td>9.821737e+00</td>\n",
       "      <td>1.935160e+00</td>\n",
       "      <td>1.054006e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  월            시간         소관경찰서          소관지역        사건발생거리   \n",
       "count  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  \\\n",
       "mean   1.522002e-16  1.146132e-16  3.703988e-17  7.176477e-17  2.200506e-16   \n",
       "std    1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00   \n",
       "min   -1.747008e+00 -1.617754e+00 -1.480617e+00 -1.577529e+00 -1.982320e+00   \n",
       "25%   -7.818446e-01 -7.765621e-01 -7.646702e-01 -1.000782e+00 -7.328137e-01   \n",
       "50%    1.833184e-01  6.462963e-02 -1.761623e-01  8.526751e-03 -9.404287e-02   \n",
       "75%    8.267604e-01  9.058214e-01  8.275945e-01  8.015547e-01  5.884964e-01   \n",
       "max    1.791923e+00  1.466616e+00  1.990291e+00  1.955050e+00  3.219977e+00   \n",
       "\n",
       "            강수량(mm)       강설량(mm)       적설량(cm)            풍향            안개   \n",
       "count  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  \\\n",
       "mean   7.660520e-18 -1.043851e-17  4.322722e-17  5.219256e-17 -3.493534e-18   \n",
       "std    1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00   \n",
       "min   -3.924166e-01 -1.441013e-01 -2.750079e-01 -1.799879e+00 -7.919185e-01   \n",
       "25%   -3.924166e-01 -1.441013e-01 -2.750079e-01 -9.351692e-01 -7.919185e-01   \n",
       "50%   -3.824502e-01 -1.441013e-01 -2.750079e-01  1.838667e-01 -7.919185e-01   \n",
       "75%   -9.627278e-02 -1.441013e-01 -2.750079e-01  7.433846e-01  1.262756e+00   \n",
       "max    9.412507e+00  1.846461e+01  7.351807e+00  1.760690e+00  1.262756e+00   \n",
       "\n",
       "               짙은안개            번개          진눈깨비            서리         연기/연무   \n",
       "count  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  \\\n",
       "mean  -4.444785e-17  1.056794e-16 -4.284841e-17  7.500576e-17 -1.052269e-17   \n",
       "std    1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00   \n",
       "min   -1.347830e-01 -4.102213e-01 -1.440563e-01 -1.018150e-01 -5.167533e-01   \n",
       "25%   -1.347830e-01 -4.102213e-01 -1.440563e-01 -1.018150e-01 -5.167533e-01   \n",
       "50%   -1.347830e-01 -4.102213e-01 -1.440563e-01 -1.018150e-01 -5.167533e-01   \n",
       "75%   -1.347830e-01 -4.102213e-01 -1.440563e-01 -1.018150e-01 -5.167533e-01   \n",
       "max    7.419332e+00  2.437709e+00  6.941732e+00  9.821737e+00  1.935160e+00   \n",
       "\n",
       "                눈날림  \n",
       "count  8.440600e+04  \n",
       "mean  -6.759778e-17  \n",
       "std    1.000006e+00  \n",
       "min   -9.487608e-02  \n",
       "25%   -9.487608e-02  \n",
       "50%   -9.487608e-02  \n",
       "75%   -9.487608e-02  \n",
       "max    1.054006e+01  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n",
    "test_df[num_cols] = scaler.transform(test_df[num_cols])\n",
    "\n",
    "train_df[num_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84406, 37), (17289, 37))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = train_df[np.concatenate([encoded_cols, num_cols])]\n",
    "test_X = test_df[np.concatenate([encoded_cols, num_cols])]\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84406,), (84406, 3))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = train_df['TARGET']\n",
    "train_y_multi = pd.get_dummies(train_df['TARGET'])\n",
    "train_y.shape, train_y_multi.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train data into train/val with StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[    0     1     2 ... 84403 84404 84405]\n",
      "  Test:  index=[    6    10    20 ... 84387 84393 84394]\n",
      "Fold 1:\n",
      "  Train: index=[    0     1     2 ... 84403 84404 84405]\n",
      "  Test:  index=[    7    11    12 ... 84390 84392 84402]\n",
      "Fold 2:\n",
      "  Train: index=[    0     1     2 ... 84402 84403 84405]\n",
      "  Test:  index=[   13    17    18 ... 84399 84400 84404]\n",
      "Fold 3:\n",
      "  Train: index=[    4     5     6 ... 84401 84402 84404]\n",
      "  Test:  index=[    0     1     2 ... 84397 84403 84405]\n",
      "Fold 4:\n",
      "  Train: index=[    0     1     2 ... 84403 84404 84405]\n",
      "  Test:  index=[    4     5     9 ... 84396 84398 84401]\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, val_index) in enumerate(skf.split(train_X, train_y)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={val_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 나중에 꼭 잘라서 교차검증하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 01:03:35.260421: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-17 01:03:35.409820: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-17 01:03:36.150626: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/:/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/\n",
      "2023-05-17 01:03:36.150782: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/:/usr/local/cuda-11.7/lib64:/usr/local/cuda/extras/CUPTI/\n",
      "2023-05-17 01:03:36.150794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_dim):\n",
    "    inputs = tf.keras.Input(shape=input_dim, dtype='float32')\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=1000, activation='relu', kernel_initializer='he_normal', \n",
    "        kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
    "        )(inputs)\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=3, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 37)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              38000     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 3003      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,003\n",
      "Trainable params: 41,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 01:03:36.858420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:36.858778: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:36.894262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:36.894518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:36.894730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:36.894908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:36.895589: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-17 01:03:37.101225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:37.101523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:37.101749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:37.101966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:37.102164: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:37.102346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:38.285402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:38.285742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:38.286006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:38.286226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:38.286416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:38.286604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 168 MB memory:  -> device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:00:06.0, compute capability: 7.0\n",
      "2023-05-17 01:03:38.287249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 01:03:38.287428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30475 MB memory:  -> device: 1, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:00:07.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "mlpclassifier = build_mlp(train_X.shape[1])\n",
    "mlpclassifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "epochs = 500\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "precision = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "# f1_score = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "mlpclassifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(#RMSprop( #\n",
    "    learning_rate=learning_rate), \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 01:03:39.863763: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fba00052ea0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-17 01:03:39.863804: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla V100S-PCIE-32GB, Compute Capability 7.0\n",
      "2023-05-17 01:03:39.863810: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): Tesla V100S-PCIE-32GB, Compute Capability 7.0\n",
      "2023-05-17 01:03:39.869369: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-17 01:03:39.989453: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676/676 [==============================] - 4s 4ms/step - loss: 76.7234 - accuracy: 0.4722 - auc: 0.6380 - val_loss: 62.5759 - val_accuracy: 0.5113 - val_auc: 0.6724\n",
      "Epoch 2/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 51.2029 - accuracy: 0.5177 - auc: 0.6791 - val_loss: 40.9532 - val_accuracy: 0.5218 - val_auc: 0.6853\n",
      "Epoch 3/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 32.8366 - accuracy: 0.5246 - auc: 0.6868 - val_loss: 25.5975 - val_accuracy: 0.5216 - val_auc: 0.6874\n",
      "Epoch 4/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 20.0027 - accuracy: 0.5251 - auc: 0.6887 - val_loss: 15.0895 - val_accuracy: 0.5254 - val_auc: 0.6899\n",
      "Epoch 5/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 11.4440 - accuracy: 0.5249 - auc: 0.6884 - val_loss: 8.3120 - val_accuracy: 0.5216 - val_auc: 0.6873\n",
      "Epoch 6/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 6.1009 - accuracy: 0.5177 - auc: 0.6845 - val_loss: 4.2551 - val_accuracy: 0.5009 - val_auc: 0.6778\n",
      "Epoch 7/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 3.0393 - accuracy: 0.4897 - auc: 0.6725 - val_loss: 2.0821 - val_accuracy: 0.4764 - val_auc: 0.6663\n",
      "Epoch 8/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.5592 - accuracy: 0.4473 - auc: 0.6533 - val_loss: 1.2086 - val_accuracy: 0.4306 - val_auc: 0.6440\n",
      "Epoch 9/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.1092 - accuracy: 0.4322 - auc: 0.6383 - val_loss: 1.0805 - val_accuracy: 0.4306 - val_auc: 0.6364\n",
      "Epoch 10/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0777 - accuracy: 0.4322 - auc: 0.6350 - val_loss: 1.0769 - val_accuracy: 0.4306 - val_auc: 0.6284\n",
      "Epoch 11/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0742 - accuracy: 0.4327 - auc: 0.6385 - val_loss: 1.0738 - val_accuracy: 0.4391 - val_auc: 0.6373\n",
      "Epoch 12/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0704 - accuracy: 0.4377 - auc: 0.6422 - val_loss: 1.0697 - val_accuracy: 0.4410 - val_auc: 0.6420\n",
      "Epoch 13/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0668 - accuracy: 0.4474 - auc: 0.6474 - val_loss: 1.0665 - val_accuracy: 0.4469 - val_auc: 0.6510\n",
      "Epoch 14/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0638 - accuracy: 0.4602 - auc: 0.6536 - val_loss: 1.0645 - val_accuracy: 0.4858 - val_auc: 0.6572\n",
      "Epoch 15/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0611 - accuracy: 0.4841 - auc: 0.6586 - val_loss: 1.0610 - val_accuracy: 0.4834 - val_auc: 0.6580\n",
      "Epoch 16/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0586 - accuracy: 0.4915 - auc: 0.6611 - val_loss: 1.0590 - val_accuracy: 0.4931 - val_auc: 0.6604\n",
      "Epoch 17/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0564 - accuracy: 0.4948 - auc: 0.6635 - val_loss: 1.0572 - val_accuracy: 0.4931 - val_auc: 0.6631\n",
      "Epoch 18/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0544 - accuracy: 0.4975 - auc: 0.6658 - val_loss: 1.0552 - val_accuracy: 0.4921 - val_auc: 0.6635\n",
      "Epoch 19/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0525 - accuracy: 0.5009 - auc: 0.6676 - val_loss: 1.0530 - val_accuracy: 0.4968 - val_auc: 0.6660\n",
      "Epoch 20/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0508 - accuracy: 0.5052 - auc: 0.6698 - val_loss: 1.0531 - val_accuracy: 0.4956 - val_auc: 0.6657\n",
      "Epoch 21/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0492 - accuracy: 0.5081 - auc: 0.6713 - val_loss: 1.0503 - val_accuracy: 0.5110 - val_auc: 0.6703\n",
      "Epoch 22/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0477 - accuracy: 0.5109 - auc: 0.6731 - val_loss: 1.0486 - val_accuracy: 0.5104 - val_auc: 0.6717\n",
      "Epoch 23/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0462 - accuracy: 0.5118 - auc: 0.6747 - val_loss: 1.0479 - val_accuracy: 0.5175 - val_auc: 0.6745\n",
      "Epoch 24/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0451 - accuracy: 0.5169 - auc: 0.6759 - val_loss: 1.0459 - val_accuracy: 0.5158 - val_auc: 0.6748\n",
      "Epoch 25/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0438 - accuracy: 0.5192 - auc: 0.6774 - val_loss: 1.0451 - val_accuracy: 0.5185 - val_auc: 0.6768\n",
      "Epoch 26/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0427 - accuracy: 0.5191 - auc: 0.6786 - val_loss: 1.0435 - val_accuracy: 0.5174 - val_auc: 0.6781\n",
      "Epoch 27/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0415 - accuracy: 0.5199 - auc: 0.6799 - val_loss: 1.0423 - val_accuracy: 0.5172 - val_auc: 0.6791\n",
      "Epoch 28/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0405 - accuracy: 0.5203 - auc: 0.6806 - val_loss: 1.0417 - val_accuracy: 0.5179 - val_auc: 0.6789\n",
      "Epoch 29/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0396 - accuracy: 0.5210 - auc: 0.6814 - val_loss: 1.0406 - val_accuracy: 0.5201 - val_auc: 0.6808\n",
      "Epoch 30/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0386 - accuracy: 0.5217 - auc: 0.6823 - val_loss: 1.0399 - val_accuracy: 0.5203 - val_auc: 0.6813\n",
      "Epoch 31/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0379 - accuracy: 0.5219 - auc: 0.6828 - val_loss: 1.0387 - val_accuracy: 0.5189 - val_auc: 0.6818\n",
      "Epoch 32/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0370 - accuracy: 0.5225 - auc: 0.6835 - val_loss: 1.0380 - val_accuracy: 0.5195 - val_auc: 0.6824\n",
      "Epoch 33/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0363 - accuracy: 0.5234 - auc: 0.6840 - val_loss: 1.0373 - val_accuracy: 0.5212 - val_auc: 0.6824\n",
      "Epoch 34/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0356 - accuracy: 0.5245 - auc: 0.6845 - val_loss: 1.0366 - val_accuracy: 0.5221 - val_auc: 0.6833\n",
      "Epoch 35/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0350 - accuracy: 0.5253 - auc: 0.6850 - val_loss: 1.0366 - val_accuracy: 0.5220 - val_auc: 0.6844\n",
      "Epoch 36/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0343 - accuracy: 0.5256 - auc: 0.6856 - val_loss: 1.0352 - val_accuracy: 0.5224 - val_auc: 0.6847\n",
      "Epoch 37/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0338 - accuracy: 0.5258 - auc: 0.6859 - val_loss: 1.0347 - val_accuracy: 0.5227 - val_auc: 0.6853\n",
      "Epoch 38/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0332 - accuracy: 0.5263 - auc: 0.6864 - val_loss: 1.0344 - val_accuracy: 0.5223 - val_auc: 0.6849\n",
      "Epoch 39/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0327 - accuracy: 0.5258 - auc: 0.6867 - val_loss: 1.0340 - val_accuracy: 0.5219 - val_auc: 0.6857\n",
      "Epoch 40/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0321 - accuracy: 0.5259 - auc: 0.6872 - val_loss: 1.0330 - val_accuracy: 0.5242 - val_auc: 0.6868\n",
      "Epoch 41/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0316 - accuracy: 0.5267 - auc: 0.6876 - val_loss: 1.0327 - val_accuracy: 0.5228 - val_auc: 0.6870\n",
      "Epoch 42/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0311 - accuracy: 0.5263 - auc: 0.6879 - val_loss: 1.0322 - val_accuracy: 0.5238 - val_auc: 0.6875\n",
      "Epoch 43/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0307 - accuracy: 0.5266 - auc: 0.6882 - val_loss: 1.0323 - val_accuracy: 0.5221 - val_auc: 0.6868\n",
      "Epoch 44/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0303 - accuracy: 0.5263 - auc: 0.6883 - val_loss: 1.0314 - val_accuracy: 0.5225 - val_auc: 0.6873\n",
      "Epoch 45/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0297 - accuracy: 0.5265 - auc: 0.6887 - val_loss: 1.0303 - val_accuracy: 0.5233 - val_auc: 0.6882\n",
      "Epoch 46/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0294 - accuracy: 0.5265 - auc: 0.6890 - val_loss: 1.0303 - val_accuracy: 0.5241 - val_auc: 0.6881\n",
      "Epoch 47/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0290 - accuracy: 0.5267 - auc: 0.6890 - val_loss: 1.0300 - val_accuracy: 0.5235 - val_auc: 0.6883\n",
      "Epoch 48/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0286 - accuracy: 0.5271 - auc: 0.6894 - val_loss: 1.0299 - val_accuracy: 0.5230 - val_auc: 0.6879\n",
      "Epoch 49/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0283 - accuracy: 0.5271 - auc: 0.6895 - val_loss: 1.0290 - val_accuracy: 0.5236 - val_auc: 0.6892\n",
      "Epoch 50/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0278 - accuracy: 0.5271 - auc: 0.6897 - val_loss: 1.0290 - val_accuracy: 0.5234 - val_auc: 0.6889\n",
      "Epoch 51/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0276 - accuracy: 0.5275 - auc: 0.6899 - val_loss: 1.0286 - val_accuracy: 0.5241 - val_auc: 0.6893\n",
      "Epoch 52/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0273 - accuracy: 0.5276 - auc: 0.6899 - val_loss: 1.0280 - val_accuracy: 0.5238 - val_auc: 0.6901\n",
      "Epoch 53/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0269 - accuracy: 0.5274 - auc: 0.6903 - val_loss: 1.0279 - val_accuracy: 0.5236 - val_auc: 0.6893\n",
      "Epoch 54/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0267 - accuracy: 0.5270 - auc: 0.6903 - val_loss: 1.0275 - val_accuracy: 0.5238 - val_auc: 0.6900\n",
      "Epoch 55/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0263 - accuracy: 0.5275 - auc: 0.6905 - val_loss: 1.0272 - val_accuracy: 0.5240 - val_auc: 0.6900\n",
      "Epoch 56/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0260 - accuracy: 0.5276 - auc: 0.6906 - val_loss: 1.0267 - val_accuracy: 0.5239 - val_auc: 0.6902\n",
      "Epoch 57/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0258 - accuracy: 0.5275 - auc: 0.6908 - val_loss: 1.0266 - val_accuracy: 0.5240 - val_auc: 0.6902\n",
      "Epoch 58/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0255 - accuracy: 0.5278 - auc: 0.6908 - val_loss: 1.0266 - val_accuracy: 0.5238 - val_auc: 0.6901\n",
      "Epoch 59/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0252 - accuracy: 0.5276 - auc: 0.6910 - val_loss: 1.0262 - val_accuracy: 0.5250 - val_auc: 0.6907\n",
      "Epoch 60/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0250 - accuracy: 0.5278 - auc: 0.6912 - val_loss: 1.0259 - val_accuracy: 0.5246 - val_auc: 0.6903\n",
      "Epoch 61/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0247 - accuracy: 0.5278 - auc: 0.6914 - val_loss: 1.0255 - val_accuracy: 0.5251 - val_auc: 0.6911\n",
      "Epoch 62/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0245 - accuracy: 0.5276 - auc: 0.6914 - val_loss: 1.0259 - val_accuracy: 0.5256 - val_auc: 0.6908\n",
      "Epoch 63/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0243 - accuracy: 0.5279 - auc: 0.6916 - val_loss: 1.0254 - val_accuracy: 0.5246 - val_auc: 0.6908\n",
      "Epoch 64/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0240 - accuracy: 0.5280 - auc: 0.6918 - val_loss: 1.0250 - val_accuracy: 0.5230 - val_auc: 0.6908\n",
      "Epoch 65/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0238 - accuracy: 0.5279 - auc: 0.6919 - val_loss: 1.0251 - val_accuracy: 0.5248 - val_auc: 0.6906\n",
      "Epoch 66/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0237 - accuracy: 0.5279 - auc: 0.6917 - val_loss: 1.0246 - val_accuracy: 0.5247 - val_auc: 0.6914\n",
      "Epoch 67/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0235 - accuracy: 0.5286 - auc: 0.6919 - val_loss: 1.0244 - val_accuracy: 0.5259 - val_auc: 0.6911\n",
      "Epoch 68/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0231 - accuracy: 0.5284 - auc: 0.6921 - val_loss: 1.0245 - val_accuracy: 0.5252 - val_auc: 0.6913\n",
      "Epoch 69/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0231 - accuracy: 0.5288 - auc: 0.6920 - val_loss: 1.0237 - val_accuracy: 0.5256 - val_auc: 0.6918\n",
      "Epoch 70/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0227 - accuracy: 0.5286 - auc: 0.6924 - val_loss: 1.0235 - val_accuracy: 0.5256 - val_auc: 0.6918\n",
      "Epoch 71/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0226 - accuracy: 0.5291 - auc: 0.6924 - val_loss: 1.0234 - val_accuracy: 0.5254 - val_auc: 0.6921\n",
      "Epoch 72/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0224 - accuracy: 0.5288 - auc: 0.6925 - val_loss: 1.0234 - val_accuracy: 0.5257 - val_auc: 0.6919\n",
      "Epoch 73/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0221 - accuracy: 0.5290 - auc: 0.6927 - val_loss: 1.0232 - val_accuracy: 0.5275 - val_auc: 0.6921\n",
      "Epoch 74/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0221 - accuracy: 0.5293 - auc: 0.6925 - val_loss: 1.0228 - val_accuracy: 0.5257 - val_auc: 0.6923\n",
      "Epoch 75/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0219 - accuracy: 0.5291 - auc: 0.6928 - val_loss: 1.0224 - val_accuracy: 0.5259 - val_auc: 0.6926\n",
      "Epoch 76/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0217 - accuracy: 0.5293 - auc: 0.6928 - val_loss: 1.0226 - val_accuracy: 0.5251 - val_auc: 0.6923\n",
      "Epoch 77/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0215 - accuracy: 0.5295 - auc: 0.6930 - val_loss: 1.0227 - val_accuracy: 0.5266 - val_auc: 0.6920\n",
      "Epoch 78/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0214 - accuracy: 0.5292 - auc: 0.6931 - val_loss: 1.0221 - val_accuracy: 0.5258 - val_auc: 0.6923\n",
      "Epoch 79/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0212 - accuracy: 0.5296 - auc: 0.6931 - val_loss: 1.0220 - val_accuracy: 0.5258 - val_auc: 0.6930\n",
      "Epoch 80/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0210 - accuracy: 0.5296 - auc: 0.6932 - val_loss: 1.0216 - val_accuracy: 0.5261 - val_auc: 0.6925\n",
      "Epoch 81/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0208 - accuracy: 0.5301 - auc: 0.6933 - val_loss: 1.0221 - val_accuracy: 0.5264 - val_auc: 0.6926\n",
      "Epoch 82/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0207 - accuracy: 0.5304 - auc: 0.6934 - val_loss: 1.0213 - val_accuracy: 0.5271 - val_auc: 0.6929\n",
      "Epoch 83/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0205 - accuracy: 0.5301 - auc: 0.6934 - val_loss: 1.0210 - val_accuracy: 0.5286 - val_auc: 0.6932\n",
      "Epoch 84/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0203 - accuracy: 0.5300 - auc: 0.6935 - val_loss: 1.0215 - val_accuracy: 0.5278 - val_auc: 0.6929\n",
      "Epoch 85/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0203 - accuracy: 0.5307 - auc: 0.6936 - val_loss: 1.0214 - val_accuracy: 0.5268 - val_auc: 0.6931\n",
      "Epoch 86/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0202 - accuracy: 0.5306 - auc: 0.6936 - val_loss: 1.0210 - val_accuracy: 0.5271 - val_auc: 0.6926\n",
      "Epoch 87/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0199 - accuracy: 0.5306 - auc: 0.6937 - val_loss: 1.0211 - val_accuracy: 0.5280 - val_auc: 0.6934\n",
      "Epoch 88/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0198 - accuracy: 0.5307 - auc: 0.6937 - val_loss: 1.0207 - val_accuracy: 0.5275 - val_auc: 0.6933\n",
      "Epoch 89/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0197 - accuracy: 0.5307 - auc: 0.6936 - val_loss: 1.0205 - val_accuracy: 0.5278 - val_auc: 0.6929\n",
      "Epoch 90/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0196 - accuracy: 0.5309 - auc: 0.6937 - val_loss: 1.0207 - val_accuracy: 0.5269 - val_auc: 0.6932\n",
      "Epoch 91/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0195 - accuracy: 0.5312 - auc: 0.6938 - val_loss: 1.0201 - val_accuracy: 0.5280 - val_auc: 0.6938\n",
      "Epoch 92/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0192 - accuracy: 0.5314 - auc: 0.6940 - val_loss: 1.0203 - val_accuracy: 0.5284 - val_auc: 0.6934\n",
      "Epoch 93/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0191 - accuracy: 0.5312 - auc: 0.6940 - val_loss: 1.0201 - val_accuracy: 0.5290 - val_auc: 0.6940\n",
      "Epoch 94/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0191 - accuracy: 0.5315 - auc: 0.6940 - val_loss: 1.0199 - val_accuracy: 0.5284 - val_auc: 0.6938\n",
      "Epoch 95/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0188 - accuracy: 0.5314 - auc: 0.6942 - val_loss: 1.0197 - val_accuracy: 0.5273 - val_auc: 0.6934\n",
      "Epoch 96/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0189 - accuracy: 0.5314 - auc: 0.6941 - val_loss: 1.0195 - val_accuracy: 0.5281 - val_auc: 0.6938\n",
      "Epoch 97/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0187 - accuracy: 0.5316 - auc: 0.6943 - val_loss: 1.0194 - val_accuracy: 0.5293 - val_auc: 0.6939\n",
      "Epoch 98/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0185 - accuracy: 0.5317 - auc: 0.6942 - val_loss: 1.0194 - val_accuracy: 0.5286 - val_auc: 0.6938\n",
      "Epoch 99/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0184 - accuracy: 0.5313 - auc: 0.6945 - val_loss: 1.0191 - val_accuracy: 0.5293 - val_auc: 0.6940\n",
      "Epoch 100/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0183 - accuracy: 0.5313 - auc: 0.6943 - val_loss: 1.0187 - val_accuracy: 0.5287 - val_auc: 0.6942\n",
      "Epoch 101/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0182 - accuracy: 0.5314 - auc: 0.6942 - val_loss: 1.0189 - val_accuracy: 0.5293 - val_auc: 0.6943\n",
      "Epoch 102/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0180 - accuracy: 0.5322 - auc: 0.6944 - val_loss: 1.0189 - val_accuracy: 0.5283 - val_auc: 0.6941\n",
      "Epoch 103/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0179 - accuracy: 0.5315 - auc: 0.6946 - val_loss: 1.0194 - val_accuracy: 0.5296 - val_auc: 0.6942\n",
      "Epoch 104/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0178 - accuracy: 0.5312 - auc: 0.6946 - val_loss: 1.0185 - val_accuracy: 0.5289 - val_auc: 0.6944\n",
      "Epoch 105/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0176 - accuracy: 0.5317 - auc: 0.6947 - val_loss: 1.0187 - val_accuracy: 0.5284 - val_auc: 0.6942\n",
      "Epoch 106/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0176 - accuracy: 0.5311 - auc: 0.6948 - val_loss: 1.0184 - val_accuracy: 0.5284 - val_auc: 0.6942\n",
      "Epoch 107/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0174 - accuracy: 0.5313 - auc: 0.6948 - val_loss: 1.0185 - val_accuracy: 0.5283 - val_auc: 0.6943\n",
      "Epoch 108/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0173 - accuracy: 0.5319 - auc: 0.6949 - val_loss: 1.0182 - val_accuracy: 0.5290 - val_auc: 0.6942\n",
      "Epoch 109/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0174 - accuracy: 0.5318 - auc: 0.6947 - val_loss: 1.0181 - val_accuracy: 0.5304 - val_auc: 0.6944\n",
      "Epoch 110/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0171 - accuracy: 0.5320 - auc: 0.6949 - val_loss: 1.0178 - val_accuracy: 0.5297 - val_auc: 0.6946\n",
      "Epoch 111/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0170 - accuracy: 0.5318 - auc: 0.6949 - val_loss: 1.0175 - val_accuracy: 0.5286 - val_auc: 0.6948\n",
      "Epoch 112/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0169 - accuracy: 0.5319 - auc: 0.6951 - val_loss: 1.0178 - val_accuracy: 0.5296 - val_auc: 0.6946\n",
      "Epoch 113/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0168 - accuracy: 0.5323 - auc: 0.6951 - val_loss: 1.0178 - val_accuracy: 0.5294 - val_auc: 0.6949\n",
      "Epoch 114/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0167 - accuracy: 0.5326 - auc: 0.6951 - val_loss: 1.0172 - val_accuracy: 0.5290 - val_auc: 0.6951\n",
      "Epoch 115/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0166 - accuracy: 0.5320 - auc: 0.6952 - val_loss: 1.0173 - val_accuracy: 0.5297 - val_auc: 0.6950\n",
      "Epoch 116/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0164 - accuracy: 0.5321 - auc: 0.6953 - val_loss: 1.0173 - val_accuracy: 0.5287 - val_auc: 0.6949\n",
      "Epoch 117/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0164 - accuracy: 0.5320 - auc: 0.6953 - val_loss: 1.0174 - val_accuracy: 0.5302 - val_auc: 0.6948\n",
      "Epoch 118/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0164 - accuracy: 0.5321 - auc: 0.6955 - val_loss: 1.0171 - val_accuracy: 0.5288 - val_auc: 0.6948\n",
      "Epoch 119/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0163 - accuracy: 0.5321 - auc: 0.6952 - val_loss: 1.0169 - val_accuracy: 0.5293 - val_auc: 0.6948\n",
      "Epoch 120/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0161 - accuracy: 0.5319 - auc: 0.6954 - val_loss: 1.0168 - val_accuracy: 0.5297 - val_auc: 0.6956\n",
      "Epoch 121/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0160 - accuracy: 0.5323 - auc: 0.6955 - val_loss: 1.0168 - val_accuracy: 0.5289 - val_auc: 0.6950\n",
      "Epoch 122/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0159 - accuracy: 0.5326 - auc: 0.6955 - val_loss: 1.0167 - val_accuracy: 0.5284 - val_auc: 0.6950\n",
      "Epoch 123/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0158 - accuracy: 0.5324 - auc: 0.6956 - val_loss: 1.0168 - val_accuracy: 0.5291 - val_auc: 0.6946\n",
      "Epoch 124/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0158 - accuracy: 0.5322 - auc: 0.6954 - val_loss: 1.0164 - val_accuracy: 0.5290 - val_auc: 0.6953\n",
      "Epoch 125/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0156 - accuracy: 0.5325 - auc: 0.6956 - val_loss: 1.0165 - val_accuracy: 0.5309 - val_auc: 0.6956\n",
      "Epoch 126/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0157 - accuracy: 0.5328 - auc: 0.6956 - val_loss: 1.0160 - val_accuracy: 0.5287 - val_auc: 0.6956\n",
      "Epoch 127/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0154 - accuracy: 0.5319 - auc: 0.6957 - val_loss: 1.0162 - val_accuracy: 0.5293 - val_auc: 0.6953\n",
      "Epoch 128/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0153 - accuracy: 0.5327 - auc: 0.6957 - val_loss: 1.0162 - val_accuracy: 0.5286 - val_auc: 0.6952\n",
      "Epoch 129/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0152 - accuracy: 0.5321 - auc: 0.6958 - val_loss: 1.0160 - val_accuracy: 0.5293 - val_auc: 0.6952\n",
      "Epoch 130/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0152 - accuracy: 0.5321 - auc: 0.6958 - val_loss: 1.0162 - val_accuracy: 0.5306 - val_auc: 0.6956\n",
      "Epoch 131/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0152 - accuracy: 0.5323 - auc: 0.6957 - val_loss: 1.0156 - val_accuracy: 0.5306 - val_auc: 0.6959\n",
      "Epoch 132/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0150 - accuracy: 0.5322 - auc: 0.6958 - val_loss: 1.0156 - val_accuracy: 0.5302 - val_auc: 0.6957\n",
      "Epoch 133/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0150 - accuracy: 0.5320 - auc: 0.6959 - val_loss: 1.0153 - val_accuracy: 0.5301 - val_auc: 0.6959\n",
      "Epoch 134/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0148 - accuracy: 0.5324 - auc: 0.6959 - val_loss: 1.0153 - val_accuracy: 0.5296 - val_auc: 0.6958\n",
      "Epoch 135/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0148 - accuracy: 0.5318 - auc: 0.6960 - val_loss: 1.0158 - val_accuracy: 0.5290 - val_auc: 0.6954\n",
      "Epoch 136/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0147 - accuracy: 0.5320 - auc: 0.6960 - val_loss: 1.0154 - val_accuracy: 0.5302 - val_auc: 0.6959\n",
      "Epoch 137/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0146 - accuracy: 0.5323 - auc: 0.6962 - val_loss: 1.0152 - val_accuracy: 0.5299 - val_auc: 0.6960\n",
      "Epoch 138/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0145 - accuracy: 0.5331 - auc: 0.6962 - val_loss: 1.0155 - val_accuracy: 0.5299 - val_auc: 0.6956\n",
      "Epoch 139/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0144 - accuracy: 0.5328 - auc: 0.6962 - val_loss: 1.0148 - val_accuracy: 0.5299 - val_auc: 0.6961\n",
      "Epoch 140/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0144 - accuracy: 0.5331 - auc: 0.6960 - val_loss: 1.0149 - val_accuracy: 0.5295 - val_auc: 0.6962\n",
      "Epoch 141/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0142 - accuracy: 0.5324 - auc: 0.6964 - val_loss: 1.0153 - val_accuracy: 0.5298 - val_auc: 0.6955\n",
      "Epoch 142/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0142 - accuracy: 0.5326 - auc: 0.6963 - val_loss: 1.0150 - val_accuracy: 0.5303 - val_auc: 0.6962\n",
      "Epoch 143/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0140 - accuracy: 0.5328 - auc: 0.6963 - val_loss: 1.0147 - val_accuracy: 0.5294 - val_auc: 0.6960\n",
      "Epoch 144/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0141 - accuracy: 0.5323 - auc: 0.6963 - val_loss: 1.0146 - val_accuracy: 0.5303 - val_auc: 0.6963\n",
      "Epoch 145/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0140 - accuracy: 0.5323 - auc: 0.6964 - val_loss: 1.0142 - val_accuracy: 0.5301 - val_auc: 0.6965\n",
      "Epoch 146/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0138 - accuracy: 0.5323 - auc: 0.6965 - val_loss: 1.0149 - val_accuracy: 0.5309 - val_auc: 0.6959\n",
      "Epoch 147/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0138 - accuracy: 0.5326 - auc: 0.6965 - val_loss: 1.0144 - val_accuracy: 0.5306 - val_auc: 0.6965\n",
      "Epoch 148/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0136 - accuracy: 0.5328 - auc: 0.6966 - val_loss: 1.0143 - val_accuracy: 0.5297 - val_auc: 0.6957\n",
      "Epoch 149/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0137 - accuracy: 0.5327 - auc: 0.6964 - val_loss: 1.0141 - val_accuracy: 0.5296 - val_auc: 0.6966\n",
      "Epoch 150/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0135 - accuracy: 0.5331 - auc: 0.6965 - val_loss: 1.0148 - val_accuracy: 0.5317 - val_auc: 0.6963\n",
      "Epoch 151/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0136 - accuracy: 0.5328 - auc: 0.6965 - val_loss: 1.0140 - val_accuracy: 0.5308 - val_auc: 0.6964\n",
      "Epoch 152/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0134 - accuracy: 0.5328 - auc: 0.6966 - val_loss: 1.0144 - val_accuracy: 0.5295 - val_auc: 0.6959\n",
      "Epoch 153/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0134 - accuracy: 0.5331 - auc: 0.6965 - val_loss: 1.0142 - val_accuracy: 0.5298 - val_auc: 0.6961\n",
      "Epoch 154/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0133 - accuracy: 0.5330 - auc: 0.6966 - val_loss: 1.0144 - val_accuracy: 0.5302 - val_auc: 0.6964\n",
      "Epoch 155/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0132 - accuracy: 0.5330 - auc: 0.6968 - val_loss: 1.0143 - val_accuracy: 0.5311 - val_auc: 0.6963\n",
      "Epoch 156/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0132 - accuracy: 0.5330 - auc: 0.6965 - val_loss: 1.0139 - val_accuracy: 0.5303 - val_auc: 0.6962\n",
      "Epoch 157/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0131 - accuracy: 0.5326 - auc: 0.6966 - val_loss: 1.0138 - val_accuracy: 0.5297 - val_auc: 0.6967\n",
      "Epoch 158/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0130 - accuracy: 0.5332 - auc: 0.6968 - val_loss: 1.0142 - val_accuracy: 0.5302 - val_auc: 0.6962\n",
      "Epoch 159/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0130 - accuracy: 0.5328 - auc: 0.6968 - val_loss: 1.0131 - val_accuracy: 0.5296 - val_auc: 0.6968\n",
      "Epoch 160/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0129 - accuracy: 0.5330 - auc: 0.6967 - val_loss: 1.0133 - val_accuracy: 0.5304 - val_auc: 0.6967\n",
      "Epoch 161/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0128 - accuracy: 0.5326 - auc: 0.6968 - val_loss: 1.0136 - val_accuracy: 0.5304 - val_auc: 0.6970\n",
      "Epoch 162/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0128 - accuracy: 0.5330 - auc: 0.6969 - val_loss: 1.0136 - val_accuracy: 0.5308 - val_auc: 0.6966\n",
      "Epoch 163/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0127 - accuracy: 0.5331 - auc: 0.6968 - val_loss: 1.0130 - val_accuracy: 0.5311 - val_auc: 0.6968\n",
      "Epoch 164/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0126 - accuracy: 0.5328 - auc: 0.6969 - val_loss: 1.0134 - val_accuracy: 0.5305 - val_auc: 0.6968\n",
      "Epoch 165/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0126 - accuracy: 0.5336 - auc: 0.6969 - val_loss: 1.0130 - val_accuracy: 0.5310 - val_auc: 0.6968\n",
      "Epoch 166/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0125 - accuracy: 0.5337 - auc: 0.6970 - val_loss: 1.0132 - val_accuracy: 0.5319 - val_auc: 0.6971\n",
      "Epoch 167/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0125 - accuracy: 0.5330 - auc: 0.6969 - val_loss: 1.0127 - val_accuracy: 0.5307 - val_auc: 0.6970\n",
      "Epoch 168/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0124 - accuracy: 0.5330 - auc: 0.6969 - val_loss: 1.0128 - val_accuracy: 0.5317 - val_auc: 0.6969\n",
      "Epoch 169/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0123 - accuracy: 0.5330 - auc: 0.6970 - val_loss: 1.0132 - val_accuracy: 0.5299 - val_auc: 0.6968\n",
      "Epoch 170/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0123 - accuracy: 0.5329 - auc: 0.6970 - val_loss: 1.0129 - val_accuracy: 0.5309 - val_auc: 0.6970\n",
      "Epoch 171/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0122 - accuracy: 0.5330 - auc: 0.6969 - val_loss: 1.0131 - val_accuracy: 0.5316 - val_auc: 0.6968\n",
      "Epoch 172/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0121 - accuracy: 0.5331 - auc: 0.6971 - val_loss: 1.0123 - val_accuracy: 0.5306 - val_auc: 0.6972\n",
      "Epoch 173/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0120 - accuracy: 0.5334 - auc: 0.6972 - val_loss: 1.0125 - val_accuracy: 0.5313 - val_auc: 0.6972\n",
      "Epoch 174/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0120 - accuracy: 0.5333 - auc: 0.6971 - val_loss: 1.0125 - val_accuracy: 0.5300 - val_auc: 0.6971\n",
      "Epoch 175/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0119 - accuracy: 0.5339 - auc: 0.6972 - val_loss: 1.0123 - val_accuracy: 0.5310 - val_auc: 0.6971\n",
      "Epoch 176/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0119 - accuracy: 0.5329 - auc: 0.6973 - val_loss: 1.0128 - val_accuracy: 0.5313 - val_auc: 0.6969\n",
      "Epoch 177/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0119 - accuracy: 0.5333 - auc: 0.6971 - val_loss: 1.0121 - val_accuracy: 0.5306 - val_auc: 0.6970\n",
      "Epoch 178/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0118 - accuracy: 0.5327 - auc: 0.6972 - val_loss: 1.0123 - val_accuracy: 0.5325 - val_auc: 0.6971\n",
      "Epoch 179/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0117 - accuracy: 0.5330 - auc: 0.6973 - val_loss: 1.0122 - val_accuracy: 0.5316 - val_auc: 0.6973\n",
      "Epoch 180/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0116 - accuracy: 0.5336 - auc: 0.6973 - val_loss: 1.0124 - val_accuracy: 0.5305 - val_auc: 0.6970\n",
      "Epoch 181/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0116 - accuracy: 0.5333 - auc: 0.6972 - val_loss: 1.0125 - val_accuracy: 0.5309 - val_auc: 0.6974\n",
      "Epoch 182/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0116 - accuracy: 0.5335 - auc: 0.6973 - val_loss: 1.0121 - val_accuracy: 0.5301 - val_auc: 0.6972\n",
      "Epoch 183/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0115 - accuracy: 0.5332 - auc: 0.6974 - val_loss: 1.0121 - val_accuracy: 0.5318 - val_auc: 0.6975\n",
      "Epoch 184/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0115 - accuracy: 0.5336 - auc: 0.6974 - val_loss: 1.0117 - val_accuracy: 0.5313 - val_auc: 0.6974\n",
      "Epoch 185/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0114 - accuracy: 0.5336 - auc: 0.6973 - val_loss: 1.0122 - val_accuracy: 0.5306 - val_auc: 0.6969\n",
      "Epoch 186/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0113 - accuracy: 0.5334 - auc: 0.6974 - val_loss: 1.0118 - val_accuracy: 0.5308 - val_auc: 0.6974\n",
      "Epoch 187/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0113 - accuracy: 0.5331 - auc: 0.6973 - val_loss: 1.0120 - val_accuracy: 0.5307 - val_auc: 0.6973\n",
      "Epoch 188/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0113 - accuracy: 0.5330 - auc: 0.6974 - val_loss: 1.0117 - val_accuracy: 0.5317 - val_auc: 0.6976\n",
      "Epoch 189/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0111 - accuracy: 0.5334 - auc: 0.6976 - val_loss: 1.0115 - val_accuracy: 0.5311 - val_auc: 0.6975\n",
      "Epoch 190/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0112 - accuracy: 0.5338 - auc: 0.6975 - val_loss: 1.0119 - val_accuracy: 0.5319 - val_auc: 0.6975\n",
      "Epoch 191/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0112 - accuracy: 0.5331 - auc: 0.6974 - val_loss: 1.0117 - val_accuracy: 0.5317 - val_auc: 0.6973\n",
      "Epoch 192/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0111 - accuracy: 0.5334 - auc: 0.6975 - val_loss: 1.0113 - val_accuracy: 0.5302 - val_auc: 0.6977\n",
      "Epoch 193/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0109 - accuracy: 0.5334 - auc: 0.6976 - val_loss: 1.0116 - val_accuracy: 0.5320 - val_auc: 0.6975\n",
      "Epoch 194/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0110 - accuracy: 0.5334 - auc: 0.6974 - val_loss: 1.0114 - val_accuracy: 0.5307 - val_auc: 0.6975\n",
      "Epoch 195/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0109 - accuracy: 0.5333 - auc: 0.6977 - val_loss: 1.0112 - val_accuracy: 0.5307 - val_auc: 0.6974\n",
      "Epoch 196/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0108 - accuracy: 0.5338 - auc: 0.6976 - val_loss: 1.0116 - val_accuracy: 0.5306 - val_auc: 0.6973\n",
      "Epoch 197/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0107 - accuracy: 0.5336 - auc: 0.6977 - val_loss: 1.0113 - val_accuracy: 0.5310 - val_auc: 0.6977\n",
      "Epoch 198/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0108 - accuracy: 0.5339 - auc: 0.6977 - val_loss: 1.0109 - val_accuracy: 0.5311 - val_auc: 0.6978\n",
      "Epoch 199/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0107 - accuracy: 0.5335 - auc: 0.6976 - val_loss: 1.0113 - val_accuracy: 0.5312 - val_auc: 0.6975\n",
      "Epoch 200/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0107 - accuracy: 0.5336 - auc: 0.6977 - val_loss: 1.0111 - val_accuracy: 0.5310 - val_auc: 0.6980\n",
      "Epoch 201/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0106 - accuracy: 0.5331 - auc: 0.6977 - val_loss: 1.0109 - val_accuracy: 0.5308 - val_auc: 0.6975\n",
      "Epoch 202/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0105 - accuracy: 0.5337 - auc: 0.6977 - val_loss: 1.0108 - val_accuracy: 0.5325 - val_auc: 0.6979\n",
      "Epoch 203/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0105 - accuracy: 0.5335 - auc: 0.6978 - val_loss: 1.0108 - val_accuracy: 0.5315 - val_auc: 0.6977\n",
      "Epoch 204/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0104 - accuracy: 0.5335 - auc: 0.6978 - val_loss: 1.0108 - val_accuracy: 0.5315 - val_auc: 0.6981\n",
      "Epoch 205/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0105 - accuracy: 0.5335 - auc: 0.6978 - val_loss: 1.0108 - val_accuracy: 0.5320 - val_auc: 0.6979\n",
      "Epoch 206/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0104 - accuracy: 0.5336 - auc: 0.6977 - val_loss: 1.0107 - val_accuracy: 0.5309 - val_auc: 0.6976\n",
      "Epoch 207/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0103 - accuracy: 0.5335 - auc: 0.6979 - val_loss: 1.0108 - val_accuracy: 0.5303 - val_auc: 0.6978\n",
      "Epoch 208/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0103 - accuracy: 0.5341 - auc: 0.6978 - val_loss: 1.0104 - val_accuracy: 0.5309 - val_auc: 0.6977\n",
      "Epoch 209/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0102 - accuracy: 0.5334 - auc: 0.6979 - val_loss: 1.0105 - val_accuracy: 0.5312 - val_auc: 0.6980\n",
      "Epoch 210/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0102 - accuracy: 0.5334 - auc: 0.6978 - val_loss: 1.0105 - val_accuracy: 0.5316 - val_auc: 0.6980\n",
      "Epoch 211/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0101 - accuracy: 0.5337 - auc: 0.6977 - val_loss: 1.0105 - val_accuracy: 0.5310 - val_auc: 0.6980\n",
      "Epoch 212/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0101 - accuracy: 0.5335 - auc: 0.6979 - val_loss: 1.0106 - val_accuracy: 0.5322 - val_auc: 0.6981\n",
      "Epoch 213/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0101 - accuracy: 0.5336 - auc: 0.6979 - val_loss: 1.0109 - val_accuracy: 0.5322 - val_auc: 0.6981\n",
      "Epoch 214/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0101 - accuracy: 0.5336 - auc: 0.6979 - val_loss: 1.0106 - val_accuracy: 0.5313 - val_auc: 0.6979\n",
      "Epoch 215/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0100 - accuracy: 0.5338 - auc: 0.6979 - val_loss: 1.0103 - val_accuracy: 0.5331 - val_auc: 0.6980\n",
      "Epoch 216/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0099 - accuracy: 0.5336 - auc: 0.6979 - val_loss: 1.0106 - val_accuracy: 0.5317 - val_auc: 0.6978\n",
      "Epoch 217/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0099 - accuracy: 0.5340 - auc: 0.6980 - val_loss: 1.0106 - val_accuracy: 0.5301 - val_auc: 0.6978\n",
      "Epoch 218/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0098 - accuracy: 0.5339 - auc: 0.6981 - val_loss: 1.0100 - val_accuracy: 0.5313 - val_auc: 0.6980\n",
      "Epoch 219/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0099 - accuracy: 0.5335 - auc: 0.6980 - val_loss: 1.0100 - val_accuracy: 0.5308 - val_auc: 0.6981\n",
      "Epoch 220/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0097 - accuracy: 0.5335 - auc: 0.6981 - val_loss: 1.0108 - val_accuracy: 0.5313 - val_auc: 0.6979\n",
      "Epoch 221/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0097 - accuracy: 0.5334 - auc: 0.6981 - val_loss: 1.0101 - val_accuracy: 0.5313 - val_auc: 0.6982\n",
      "Epoch 222/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0097 - accuracy: 0.5330 - auc: 0.6981 - val_loss: 1.0099 - val_accuracy: 0.5327 - val_auc: 0.6986\n",
      "Epoch 223/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0097 - accuracy: 0.5340 - auc: 0.6981 - val_loss: 1.0103 - val_accuracy: 0.5319 - val_auc: 0.6984\n",
      "Epoch 224/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0096 - accuracy: 0.5336 - auc: 0.6982 - val_loss: 1.0099 - val_accuracy: 0.5333 - val_auc: 0.6983\n",
      "Epoch 225/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0096 - accuracy: 0.5337 - auc: 0.6981 - val_loss: 1.0099 - val_accuracy: 0.5316 - val_auc: 0.6982\n",
      "Epoch 226/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0095 - accuracy: 0.5335 - auc: 0.6982 - val_loss: 1.0097 - val_accuracy: 0.5326 - val_auc: 0.6983\n",
      "Epoch 227/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0095 - accuracy: 0.5334 - auc: 0.6981 - val_loss: 1.0101 - val_accuracy: 0.5320 - val_auc: 0.6985\n",
      "Epoch 228/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0095 - accuracy: 0.5337 - auc: 0.6982 - val_loss: 1.0098 - val_accuracy: 0.5315 - val_auc: 0.6984\n",
      "Epoch 229/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0094 - accuracy: 0.5340 - auc: 0.6982 - val_loss: 1.0097 - val_accuracy: 0.5328 - val_auc: 0.6985\n",
      "Epoch 230/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0092 - accuracy: 0.5340 - auc: 0.6984 - val_loss: 1.0103 - val_accuracy: 0.5316 - val_auc: 0.6976\n",
      "Epoch 231/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0094 - accuracy: 0.5333 - auc: 0.6982 - val_loss: 1.0094 - val_accuracy: 0.5326 - val_auc: 0.6983\n",
      "Epoch 232/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0093 - accuracy: 0.5336 - auc: 0.6984 - val_loss: 1.0094 - val_accuracy: 0.5316 - val_auc: 0.6984\n",
      "Epoch 233/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0093 - accuracy: 0.5335 - auc: 0.6981 - val_loss: 1.0099 - val_accuracy: 0.5318 - val_auc: 0.6982\n",
      "Epoch 234/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0092 - accuracy: 0.5340 - auc: 0.6983 - val_loss: 1.0095 - val_accuracy: 0.5316 - val_auc: 0.6986\n",
      "Epoch 235/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0091 - accuracy: 0.5339 - auc: 0.6984 - val_loss: 1.0094 - val_accuracy: 0.5317 - val_auc: 0.6983\n",
      "Epoch 236/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0092 - accuracy: 0.5339 - auc: 0.6983 - val_loss: 1.0095 - val_accuracy: 0.5315 - val_auc: 0.6986\n",
      "Epoch 237/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0091 - accuracy: 0.5338 - auc: 0.6984 - val_loss: 1.0096 - val_accuracy: 0.5302 - val_auc: 0.6985\n",
      "Epoch 238/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0092 - accuracy: 0.5340 - auc: 0.6981 - val_loss: 1.0091 - val_accuracy: 0.5317 - val_auc: 0.6985\n",
      "Epoch 239/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0090 - accuracy: 0.5336 - auc: 0.6985 - val_loss: 1.0094 - val_accuracy: 0.5323 - val_auc: 0.6983\n",
      "Epoch 240/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0090 - accuracy: 0.5340 - auc: 0.6985 - val_loss: 1.0091 - val_accuracy: 0.5313 - val_auc: 0.6983\n",
      "Epoch 241/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0089 - accuracy: 0.5342 - auc: 0.6983 - val_loss: 1.0091 - val_accuracy: 0.5317 - val_auc: 0.6986\n",
      "Epoch 242/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0089 - accuracy: 0.5337 - auc: 0.6984 - val_loss: 1.0092 - val_accuracy: 0.5325 - val_auc: 0.6987\n",
      "Epoch 243/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0088 - accuracy: 0.5335 - auc: 0.6985 - val_loss: 1.0087 - val_accuracy: 0.5316 - val_auc: 0.6989\n",
      "Epoch 244/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0087 - accuracy: 0.5338 - auc: 0.6986 - val_loss: 1.0096 - val_accuracy: 0.5321 - val_auc: 0.6983\n",
      "Epoch 245/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0088 - accuracy: 0.5343 - auc: 0.6985 - val_loss: 1.0090 - val_accuracy: 0.5316 - val_auc: 0.6986\n",
      "Epoch 246/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0087 - accuracy: 0.5339 - auc: 0.6984 - val_loss: 1.0091 - val_accuracy: 0.5323 - val_auc: 0.6985\n",
      "Epoch 247/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0087 - accuracy: 0.5337 - auc: 0.6986 - val_loss: 1.0096 - val_accuracy: 0.5316 - val_auc: 0.6984\n",
      "Epoch 248/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0087 - accuracy: 0.5342 - auc: 0.6986 - val_loss: 1.0088 - val_accuracy: 0.5320 - val_auc: 0.6984\n",
      "Epoch 249/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0087 - accuracy: 0.5341 - auc: 0.6986 - val_loss: 1.0088 - val_accuracy: 0.5320 - val_auc: 0.6989\n",
      "Epoch 250/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0085 - accuracy: 0.5343 - auc: 0.6986 - val_loss: 1.0087 - val_accuracy: 0.5315 - val_auc: 0.6988\n",
      "Epoch 251/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0085 - accuracy: 0.5336 - auc: 0.6988 - val_loss: 1.0094 - val_accuracy: 0.5313 - val_auc: 0.6983\n",
      "Epoch 252/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0085 - accuracy: 0.5338 - auc: 0.6987 - val_loss: 1.0091 - val_accuracy: 0.5323 - val_auc: 0.6987\n",
      "Epoch 253/500\n",
      "676/676 [==============================] - 2s 3ms/step - loss: 1.0085 - accuracy: 0.5337 - auc: 0.6987 - val_loss: 1.0090 - val_accuracy: 0.5309 - val_auc: 0.6984\n"
     ]
    }
   ],
   "source": [
    "history = mlpclassifier.fit(\n",
    "    train_X, train_y_multi,\n",
    "    validation_split=0.2, epochs=epochs, batch_size=batch_size,\n",
    "    callbacks=[callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541/541 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((17289, 3), (17289,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = mlpclassifier.predict(test_X)\n",
    "test_results = np.argmax(test_pred, axis=-1)\n",
    "test_pred.shape, test_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2638/2638 [==============================] - 3s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((84406, 3), (84406,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred = mlpclassifier.predict(train_X)\n",
    "train_results = np.argmax(train_pred, axis=-1)\n",
    "train_pred.shape, train_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4998115378547827"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# evaluate\n",
    "train_f1 = f1_score(train_y, train_results, average='macro')\n",
    "train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3개를 하나로!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84406, 39), (17289, 38), (17289, 2))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission_df = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "train_df.shape, test_df.shape, sample_submission_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mname = 'mlpclassifier_keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def make_report(template, test_pred, mname):\n",
    "    template['TARGET'] = test_pred\n",
    "    now = dt.strftime(dt.now(), '%y-%m-%d')\n",
    "    template.to_csv(f'results/{mname}-{now}-2.csv', index=False)\n",
    "    \n",
    "make_report(sample_submission_df, test_results, mname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimizer\n",
    "## early stopping\n",
    "## compile\n",
    "## train\n",
    "## evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
