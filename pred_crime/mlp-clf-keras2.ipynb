{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84406, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>월</th>\n",
       "      <th>요일</th>\n",
       "      <th>시간</th>\n",
       "      <th>소관경찰서</th>\n",
       "      <th>소관지역</th>\n",
       "      <th>사건발생거리</th>\n",
       "      <th>강수량(mm)</th>\n",
       "      <th>강설량(mm)</th>\n",
       "      <th>적설량(cm)</th>\n",
       "      <th>풍향</th>\n",
       "      <th>안개</th>\n",
       "      <th>짙은안개</th>\n",
       "      <th>번개</th>\n",
       "      <th>진눈깨비</th>\n",
       "      <th>서리</th>\n",
       "      <th>연기/연무</th>\n",
       "      <th>눈날림</th>\n",
       "      <th>범죄발생지</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>9</td>\n",
       "      <td>화요일</td>\n",
       "      <td>10</td>\n",
       "      <td>137</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.611124</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>차도</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>11</td>\n",
       "      <td>화요일</td>\n",
       "      <td>6</td>\n",
       "      <td>438</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.209093</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>차도</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>8</td>\n",
       "      <td>일요일</td>\n",
       "      <td>6</td>\n",
       "      <td>1729</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.619597</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>인도</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>5</td>\n",
       "      <td>월요일</td>\n",
       "      <td>6</td>\n",
       "      <td>2337</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.921615</td>\n",
       "      <td>11.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>주거지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>9</td>\n",
       "      <td>일요일</td>\n",
       "      <td>11</td>\n",
       "      <td>1439</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.789721</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>주유소</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID   월   요일  시간  소관경찰서  소관지역    사건발생거리  강수량(mm)  강설량(mm)  적설량(cm)   \n",
       "0  TRAIN_00000   9  화요일  10    137   8.0  2.611124    0.000      0.0      0.0  \\\n",
       "1  TRAIN_00001  11  화요일   6    438  13.0  3.209093    0.000      0.0      0.0   \n",
       "2  TRAIN_00002   8  일요일   6   1729  47.0  1.619597    0.000      0.0      0.0   \n",
       "3  TRAIN_00003   5  월요일   6   2337  53.0  1.921615   11.375      0.0      0.0   \n",
       "4  TRAIN_00004   9  일요일  11   1439  41.0  1.789721    0.000      0.0      0.0   \n",
       "\n",
       "      풍향   안개  짙은안개   번개  진눈깨비   서리  연기/연무  눈날림 범죄발생지  TARGET  \n",
       "0  245.0  0.0   0.0  0.0   0.0  0.0    0.0  0.0    차도       2  \n",
       "1  200.0  0.0   0.0  0.0   0.0  0.0    0.0  0.0    차도       0  \n",
       "2   40.0  1.0   0.0  0.0   0.0  0.0    1.0  0.0    인도       1  \n",
       "3  225.0  1.0   1.0  0.0   0.0  0.0    0.0  0.0   주거지       1  \n",
       "4  255.0  0.0   0.0  0.0   0.0  0.0    0.0  0.0   주유소       2  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17289, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>월</th>\n",
       "      <th>요일</th>\n",
       "      <th>시간</th>\n",
       "      <th>소관경찰서</th>\n",
       "      <th>소관지역</th>\n",
       "      <th>사건발생거리</th>\n",
       "      <th>강수량(mm)</th>\n",
       "      <th>강설량(mm)</th>\n",
       "      <th>적설량(cm)</th>\n",
       "      <th>풍향</th>\n",
       "      <th>안개</th>\n",
       "      <th>짙은안개</th>\n",
       "      <th>번개</th>\n",
       "      <th>진눈깨비</th>\n",
       "      <th>서리</th>\n",
       "      <th>연기/연무</th>\n",
       "      <th>눈날림</th>\n",
       "      <th>범죄발생지</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>9</td>\n",
       "      <td>금요일</td>\n",
       "      <td>5</td>\n",
       "      <td>927</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.570654</td>\n",
       "      <td>19.625000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>차도</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>5</td>\n",
       "      <td>수요일</td>\n",
       "      <td>3</td>\n",
       "      <td>926</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.712457</td>\n",
       "      <td>21.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>식당</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>5</td>\n",
       "      <td>월요일</td>\n",
       "      <td>6</td>\n",
       "      <td>1437</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>25.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>주거지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>11</td>\n",
       "      <td>화요일</td>\n",
       "      <td>1</td>\n",
       "      <td>1739</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.878585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>주거지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>10</td>\n",
       "      <td>목요일</td>\n",
       "      <td>10</td>\n",
       "      <td>830</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.496423</td>\n",
       "      <td>26.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>주거지</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID   월   요일  시간  소관경찰서  소관지역    사건발생거리    강수량(mm)  강설량(mm)   \n",
       "0  TEST_00000   9  금요일   5    927  28.0  1.570654  19.625000      0.0  \\\n",
       "1  TEST_00001   5  수요일   3    926  28.0  1.712457  21.444444      0.0   \n",
       "2  TEST_00002   5  월요일   6   1437  33.0  0.447496  25.200000      0.0   \n",
       "3  TEST_00003  11  화요일   1   1739  31.0  0.878585   0.000000      0.0   \n",
       "4  TEST_00004  10  목요일  10    830  15.0  0.496423  26.142857      0.0   \n",
       "\n",
       "   적설량(cm)     풍향   안개  짙은안개   번개  진눈깨비   서리  연기/연무  눈날림 범죄발생지  \n",
       "0      0.0  165.0  1.0   0.0  1.0   0.0  0.0    0.0  0.0    차도  \n",
       "1      0.0  175.0  1.0   0.0  0.0   0.0  0.0    1.0  0.0    식당  \n",
       "2      0.0  290.0  1.0   0.0  0.0   0.0  0.0    0.0  0.0   주거지  \n",
       "3      0.0  285.0  0.0   0.0  0.0   0.0  0.0    0.0  0.0   주거지  \n",
       "4      0.0   95.0  1.0   0.0  0.0   0.0  0.0    0.0  0.0   주거지  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical columns:  ['요일', '범죄발생지']\n",
      "numeric columns:  ['월', '시간', '소관경찰서', '소관지역', '사건발생거리', '강수량(mm)', '강설량(mm)', '적설량(cm)', '풍향', '안개', '짙은안개', '번개', '진눈깨비', '서리', '연기/연무', '눈날림']\n"
     ]
    }
   ],
   "source": [
    "cat_cols = []\n",
    "num_cols = []\n",
    "\n",
    "for col in test_df.columns[1:]:\n",
    "    if train_df[col].dtype == 'object':\n",
    "        cat_cols.append(col)\n",
    "    else:\n",
    "        num_cols.append(col)\n",
    "\n",
    "print('categorical columns: ', cat_cols)\n",
    "print('numeric columns: ', num_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess cat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['금요일', '목요일', '수요일', '월요일', '일요일', '토요일', '화요일', '공원', '백화점', '병원',\n",
       "       '식당', '약국', '은행', '인도', '주거지', '주유소', '주차장', '차도', '편의점', '학교',\n",
       "       '호텔/모텔'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat_array = encoder.fit_transform(train_df[cat_cols])\n",
    "test_cat_array = encoder.transform(test_df[cat_cols])\n",
    "\n",
    "encoded_cols = np.concatenate(encoder.categories_)\n",
    "encoded_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84406, 39), (17289, 38))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([\n",
    "    train_df.drop(columns=cat_cols),\n",
    "    pd.DataFrame(train_cat_array, columns=encoded_cols)], axis=1)\n",
    "    \n",
    "test_df = pd.concat([\n",
    "    test_df.drop(columns=cat_cols),\n",
    "    pd.DataFrame(test_cat_array, columns=encoded_cols)], axis=1)\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>월</th>\n",
       "      <th>시간</th>\n",
       "      <th>소관경찰서</th>\n",
       "      <th>소관지역</th>\n",
       "      <th>사건발생거리</th>\n",
       "      <th>강수량(mm)</th>\n",
       "      <th>강설량(mm)</th>\n",
       "      <th>적설량(cm)</th>\n",
       "      <th>풍향</th>\n",
       "      <th>안개</th>\n",
       "      <th>짙은안개</th>\n",
       "      <th>번개</th>\n",
       "      <th>진눈깨비</th>\n",
       "      <th>서리</th>\n",
       "      <th>연기/연무</th>\n",
       "      <th>눈날림</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "      <td>84406.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.430195</td>\n",
       "      <td>6.769507</td>\n",
       "      <td>1060.027581</td>\n",
       "      <td>26.881726</td>\n",
       "      <td>1.912424</td>\n",
       "      <td>24.608776</td>\n",
       "      <td>2.284407</td>\n",
       "      <td>23.430503</td>\n",
       "      <td>186.926107</td>\n",
       "      <td>0.385423</td>\n",
       "      <td>0.017842</td>\n",
       "      <td>0.144042</td>\n",
       "      <td>0.020330</td>\n",
       "      <td>0.010260</td>\n",
       "      <td>0.210755</td>\n",
       "      <td>0.008921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.108302</td>\n",
       "      <td>3.566390</td>\n",
       "      <td>698.380485</td>\n",
       "      <td>13.870968</td>\n",
       "      <td>0.958556</td>\n",
       "      <td>62.711211</td>\n",
       "      <td>15.852881</td>\n",
       "      <td>85.199896</td>\n",
       "      <td>98.299485</td>\n",
       "      <td>0.486698</td>\n",
       "      <td>0.132379</td>\n",
       "      <td>0.351134</td>\n",
       "      <td>0.141128</td>\n",
       "      <td>0.100771</td>\n",
       "      <td>0.407847</td>\n",
       "      <td>0.094030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.012269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>526.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.209985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>937.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>1.822279</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1638.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>2.476528</td>\n",
       "      <td>18.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2450.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>4.998936</td>\n",
       "      <td>614.875000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>649.800000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  월            시간         소관경찰서          소관지역        사건발생거리   \n",
       "count  84406.000000  84406.000000  84406.000000  84406.000000  84406.000000  \\\n",
       "mean       6.430195      6.769507   1060.027581     26.881726      1.912424   \n",
       "std        3.108302      3.566390    698.380485     13.870968      0.958556   \n",
       "min        1.000000      1.000000     26.000000      5.000000      0.012269   \n",
       "25%        4.000000      4.000000    526.000000     13.000000      1.209985   \n",
       "50%        7.000000      7.000000    937.000000     27.000000      1.822279   \n",
       "75%        9.000000     10.000000   1638.000000     38.000000      2.476528   \n",
       "max       12.000000     12.000000   2450.000000     54.000000      4.998936   \n",
       "\n",
       "            강수량(mm)       강설량(mm)       적설량(cm)            풍향            안개   \n",
       "count  84406.000000  84406.000000  84406.000000  84406.000000  84406.000000  \\\n",
       "mean      24.608776      2.284407     23.430503    186.926107      0.385423   \n",
       "std       62.711211     15.852881     85.199896     98.299485      0.486698   \n",
       "min        0.000000      0.000000      0.000000     10.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000     95.000000      0.000000   \n",
       "50%        0.625000      0.000000      0.000000    205.000000      0.000000   \n",
       "75%       18.571429      0.000000      0.000000    260.000000      1.000000   \n",
       "max      614.875000    295.000000    649.800000    360.000000      1.000000   \n",
       "\n",
       "               짙은안개            번개          진눈깨비            서리         연기/연무   \n",
       "count  84406.000000  84406.000000  84406.000000  84406.000000  84406.000000  \\\n",
       "mean       0.017842      0.144042      0.020330      0.010260      0.210755   \n",
       "std        0.132379      0.351134      0.141128      0.100771      0.407847   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                눈날림  \n",
       "count  84406.000000  \n",
       "mean       0.008921  \n",
       "std        0.094030  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[num_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>월</th>\n",
       "      <th>시간</th>\n",
       "      <th>소관경찰서</th>\n",
       "      <th>소관지역</th>\n",
       "      <th>사건발생거리</th>\n",
       "      <th>강수량(mm)</th>\n",
       "      <th>강설량(mm)</th>\n",
       "      <th>적설량(cm)</th>\n",
       "      <th>풍향</th>\n",
       "      <th>안개</th>\n",
       "      <th>짙은안개</th>\n",
       "      <th>번개</th>\n",
       "      <th>진눈깨비</th>\n",
       "      <th>서리</th>\n",
       "      <th>연기/연무</th>\n",
       "      <th>눈날림</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "      <td>8.440600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.522002e-16</td>\n",
       "      <td>1.146132e-16</td>\n",
       "      <td>3.703988e-17</td>\n",
       "      <td>7.176477e-17</td>\n",
       "      <td>2.200506e-16</td>\n",
       "      <td>7.660520e-18</td>\n",
       "      <td>-1.043851e-17</td>\n",
       "      <td>4.322722e-17</td>\n",
       "      <td>5.219256e-17</td>\n",
       "      <td>-3.493534e-18</td>\n",
       "      <td>-4.444785e-17</td>\n",
       "      <td>1.056794e-16</td>\n",
       "      <td>-4.284841e-17</td>\n",
       "      <td>7.500576e-17</td>\n",
       "      <td>-1.052269e-17</td>\n",
       "      <td>-6.759778e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.747008e+00</td>\n",
       "      <td>-1.617754e+00</td>\n",
       "      <td>-1.480617e+00</td>\n",
       "      <td>-1.577529e+00</td>\n",
       "      <td>-1.982320e+00</td>\n",
       "      <td>-3.924166e-01</td>\n",
       "      <td>-1.441013e-01</td>\n",
       "      <td>-2.750079e-01</td>\n",
       "      <td>-1.799879e+00</td>\n",
       "      <td>-7.919185e-01</td>\n",
       "      <td>-1.347830e-01</td>\n",
       "      <td>-4.102213e-01</td>\n",
       "      <td>-1.440563e-01</td>\n",
       "      <td>-1.018150e-01</td>\n",
       "      <td>-5.167533e-01</td>\n",
       "      <td>-9.487608e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.818446e-01</td>\n",
       "      <td>-7.765621e-01</td>\n",
       "      <td>-7.646702e-01</td>\n",
       "      <td>-1.000782e+00</td>\n",
       "      <td>-7.328137e-01</td>\n",
       "      <td>-3.924166e-01</td>\n",
       "      <td>-1.441013e-01</td>\n",
       "      <td>-2.750079e-01</td>\n",
       "      <td>-9.351692e-01</td>\n",
       "      <td>-7.919185e-01</td>\n",
       "      <td>-1.347830e-01</td>\n",
       "      <td>-4.102213e-01</td>\n",
       "      <td>-1.440563e-01</td>\n",
       "      <td>-1.018150e-01</td>\n",
       "      <td>-5.167533e-01</td>\n",
       "      <td>-9.487608e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.833184e-01</td>\n",
       "      <td>6.462963e-02</td>\n",
       "      <td>-1.761623e-01</td>\n",
       "      <td>8.526751e-03</td>\n",
       "      <td>-9.404287e-02</td>\n",
       "      <td>-3.824502e-01</td>\n",
       "      <td>-1.441013e-01</td>\n",
       "      <td>-2.750079e-01</td>\n",
       "      <td>1.838667e-01</td>\n",
       "      <td>-7.919185e-01</td>\n",
       "      <td>-1.347830e-01</td>\n",
       "      <td>-4.102213e-01</td>\n",
       "      <td>-1.440563e-01</td>\n",
       "      <td>-1.018150e-01</td>\n",
       "      <td>-5.167533e-01</td>\n",
       "      <td>-9.487608e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.267604e-01</td>\n",
       "      <td>9.058214e-01</td>\n",
       "      <td>8.275945e-01</td>\n",
       "      <td>8.015547e-01</td>\n",
       "      <td>5.884964e-01</td>\n",
       "      <td>-9.627278e-02</td>\n",
       "      <td>-1.441013e-01</td>\n",
       "      <td>-2.750079e-01</td>\n",
       "      <td>7.433846e-01</td>\n",
       "      <td>1.262756e+00</td>\n",
       "      <td>-1.347830e-01</td>\n",
       "      <td>-4.102213e-01</td>\n",
       "      <td>-1.440563e-01</td>\n",
       "      <td>-1.018150e-01</td>\n",
       "      <td>-5.167533e-01</td>\n",
       "      <td>-9.487608e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.791923e+00</td>\n",
       "      <td>1.466616e+00</td>\n",
       "      <td>1.990291e+00</td>\n",
       "      <td>1.955050e+00</td>\n",
       "      <td>3.219977e+00</td>\n",
       "      <td>9.412507e+00</td>\n",
       "      <td>1.846461e+01</td>\n",
       "      <td>7.351807e+00</td>\n",
       "      <td>1.760690e+00</td>\n",
       "      <td>1.262756e+00</td>\n",
       "      <td>7.419332e+00</td>\n",
       "      <td>2.437709e+00</td>\n",
       "      <td>6.941732e+00</td>\n",
       "      <td>9.821737e+00</td>\n",
       "      <td>1.935160e+00</td>\n",
       "      <td>1.054006e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  월            시간         소관경찰서          소관지역        사건발생거리   \n",
       "count  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  \\\n",
       "mean   1.522002e-16  1.146132e-16  3.703988e-17  7.176477e-17  2.200506e-16   \n",
       "std    1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00   \n",
       "min   -1.747008e+00 -1.617754e+00 -1.480617e+00 -1.577529e+00 -1.982320e+00   \n",
       "25%   -7.818446e-01 -7.765621e-01 -7.646702e-01 -1.000782e+00 -7.328137e-01   \n",
       "50%    1.833184e-01  6.462963e-02 -1.761623e-01  8.526751e-03 -9.404287e-02   \n",
       "75%    8.267604e-01  9.058214e-01  8.275945e-01  8.015547e-01  5.884964e-01   \n",
       "max    1.791923e+00  1.466616e+00  1.990291e+00  1.955050e+00  3.219977e+00   \n",
       "\n",
       "            강수량(mm)       강설량(mm)       적설량(cm)            풍향            안개   \n",
       "count  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  \\\n",
       "mean   7.660520e-18 -1.043851e-17  4.322722e-17  5.219256e-17 -3.493534e-18   \n",
       "std    1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00   \n",
       "min   -3.924166e-01 -1.441013e-01 -2.750079e-01 -1.799879e+00 -7.919185e-01   \n",
       "25%   -3.924166e-01 -1.441013e-01 -2.750079e-01 -9.351692e-01 -7.919185e-01   \n",
       "50%   -3.824502e-01 -1.441013e-01 -2.750079e-01  1.838667e-01 -7.919185e-01   \n",
       "75%   -9.627278e-02 -1.441013e-01 -2.750079e-01  7.433846e-01  1.262756e+00   \n",
       "max    9.412507e+00  1.846461e+01  7.351807e+00  1.760690e+00  1.262756e+00   \n",
       "\n",
       "               짙은안개            번개          진눈깨비            서리         연기/연무   \n",
       "count  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  8.440600e+04  \\\n",
       "mean  -4.444785e-17  1.056794e-16 -4.284841e-17  7.500576e-17 -1.052269e-17   \n",
       "std    1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00   \n",
       "min   -1.347830e-01 -4.102213e-01 -1.440563e-01 -1.018150e-01 -5.167533e-01   \n",
       "25%   -1.347830e-01 -4.102213e-01 -1.440563e-01 -1.018150e-01 -5.167533e-01   \n",
       "50%   -1.347830e-01 -4.102213e-01 -1.440563e-01 -1.018150e-01 -5.167533e-01   \n",
       "75%   -1.347830e-01 -4.102213e-01 -1.440563e-01 -1.018150e-01 -5.167533e-01   \n",
       "max    7.419332e+00  2.437709e+00  6.941732e+00  9.821737e+00  1.935160e+00   \n",
       "\n",
       "                눈날림  \n",
       "count  8.440600e+04  \n",
       "mean  -6.759778e-17  \n",
       "std    1.000006e+00  \n",
       "min   -9.487608e-02  \n",
       "25%   -9.487608e-02  \n",
       "50%   -9.487608e-02  \n",
       "75%   -9.487608e-02  \n",
       "max    1.054006e+01  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n",
    "test_df[num_cols] = scaler.transform(test_df[num_cols])\n",
    "\n",
    "train_df[num_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84406, 37), (17289, 37))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = train_df[np.concatenate([encoded_cols, num_cols])]\n",
    "test_X = test_df[np.concatenate([encoded_cols, num_cols])]\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84406,), (84406, 3))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = train_df['TARGET']\n",
    "train_y_multi = pd.get_dummies(train_df['TARGET'])\n",
    "train_y.shape, train_y_multi.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train data into train/val with StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[    0     1     2 ... 84403 84404 84405]\n",
      "  Test:  index=[    6    10    20 ... 84387 84393 84394]\n",
      "Fold 1:\n",
      "  Train: index=[    0     1     2 ... 84403 84404 84405]\n",
      "  Test:  index=[    7    11    12 ... 84390 84392 84402]\n",
      "Fold 2:\n",
      "  Train: index=[    0     1     2 ... 84402 84403 84405]\n",
      "  Test:  index=[   13    17    18 ... 84399 84400 84404]\n",
      "Fold 3:\n",
      "  Train: index=[    4     5     6 ... 84401 84402 84404]\n",
      "  Test:  index=[    0     1     2 ... 84397 84403 84405]\n",
      "Fold 4:\n",
      "  Train: index=[    0     1     2 ... 84403 84404 84405]\n",
      "  Test:  index=[    4     5     9 ... 84396 84398 84401]\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, val_index) in enumerate(skf.split(train_X, train_y)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={val_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 나중에 꼭 잘라서 교차검증하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_dim):\n",
    "    inputs = tf.keras.Input(shape=input_dim, dtype='float32')\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=1024, activation='relu', #kernel_initializer='he_normal', \n",
    "        kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.001, l2=0.001)\n",
    "        )(inputs)\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=128, activation='relu', #kernel_initializer='he_normal', \n",
    "        kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.001, l2=0.001)\n",
    "        )(x)\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=64, activation='relu', #kernel_initializer='he_normal', \n",
    "        kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.001, l2=0.001)\n",
    "        )(x)\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=3, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 37)]              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1024)              38912     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 128)               131200    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 178,563\n",
      "Trainable params: 178,563\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlpclassifier = build_mlp(train_X.shape[1])\n",
    "mlpclassifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "epochs = 500\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "precision = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "# f1_score = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(patience=30)\n",
    "mlpclassifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(#RMSprop( #\n",
    "    learning_rate=learning_rate), \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "676/676 [==============================] - 4s 4ms/step - loss: 4.4379 - accuracy: 0.4628 - auc: 0.6417 - val_loss: 1.9034 - val_accuracy: 0.4487 - val_auc: 0.6419\n",
      "Epoch 2/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.4162 - accuracy: 0.4563 - auc: 0.6411 - val_loss: 1.2005 - val_accuracy: 0.4550 - val_auc: 0.6397\n",
      "Epoch 3/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.1255 - accuracy: 0.4667 - auc: 0.6484 - val_loss: 1.0861 - val_accuracy: 0.4796 - val_auc: 0.6540\n",
      "Epoch 4/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0724 - accuracy: 0.4827 - auc: 0.6574 - val_loss: 1.0674 - val_accuracy: 0.4855 - val_auc: 0.6565\n",
      "Epoch 5/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0618 - accuracy: 0.4867 - auc: 0.6586 - val_loss: 1.0613 - val_accuracy: 0.4889 - val_auc: 0.6580\n",
      "Epoch 6/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0574 - accuracy: 0.4890 - auc: 0.6599 - val_loss: 1.0581 - val_accuracy: 0.4905 - val_auc: 0.6584\n",
      "Epoch 7/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0549 - accuracy: 0.4899 - auc: 0.6602 - val_loss: 1.0559 - val_accuracy: 0.4911 - val_auc: 0.6588\n",
      "Epoch 8/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0529 - accuracy: 0.4909 - auc: 0.6608 - val_loss: 1.0539 - val_accuracy: 0.4936 - val_auc: 0.6591\n",
      "Epoch 9/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0512 - accuracy: 0.4925 - auc: 0.6608 - val_loss: 1.0522 - val_accuracy: 0.4929 - val_auc: 0.6596\n",
      "Epoch 10/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0496 - accuracy: 0.4927 - auc: 0.6617 - val_loss: 1.0506 - val_accuracy: 0.4931 - val_auc: 0.6608\n",
      "Epoch 11/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0482 - accuracy: 0.4936 - auc: 0.6628 - val_loss: 1.0493 - val_accuracy: 0.4934 - val_auc: 0.6622\n",
      "Epoch 12/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0469 - accuracy: 0.4941 - auc: 0.6643 - val_loss: 1.0477 - val_accuracy: 0.4957 - val_auc: 0.6640\n",
      "Epoch 13/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0455 - accuracy: 0.4944 - auc: 0.6663 - val_loss: 1.0461 - val_accuracy: 0.4970 - val_auc: 0.6660\n",
      "Epoch 14/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0441 - accuracy: 0.4959 - auc: 0.6682 - val_loss: 1.0446 - val_accuracy: 0.4964 - val_auc: 0.6679\n",
      "Epoch 15/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0427 - accuracy: 0.4964 - auc: 0.6699 - val_loss: 1.0429 - val_accuracy: 0.4998 - val_auc: 0.6695\n",
      "Epoch 16/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0411 - accuracy: 0.4979 - auc: 0.6719 - val_loss: 1.0414 - val_accuracy: 0.5008 - val_auc: 0.6714\n",
      "Epoch 17/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0397 - accuracy: 0.4990 - auc: 0.6734 - val_loss: 1.0395 - val_accuracy: 0.4993 - val_auc: 0.6736\n",
      "Epoch 18/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0381 - accuracy: 0.4997 - auc: 0.6752 - val_loss: 1.0383 - val_accuracy: 0.4966 - val_auc: 0.6755\n",
      "Epoch 19/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0367 - accuracy: 0.4996 - auc: 0.6768 - val_loss: 1.0366 - val_accuracy: 0.4995 - val_auc: 0.6765\n",
      "Epoch 20/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0352 - accuracy: 0.4985 - auc: 0.6781 - val_loss: 1.0351 - val_accuracy: 0.5017 - val_auc: 0.6791\n",
      "Epoch 21/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0338 - accuracy: 0.5002 - auc: 0.6802 - val_loss: 1.0338 - val_accuracy: 0.5081 - val_auc: 0.6811\n",
      "Epoch 22/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0322 - accuracy: 0.5100 - auc: 0.6828 - val_loss: 1.0321 - val_accuracy: 0.5148 - val_auc: 0.6828\n",
      "Epoch 23/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0306 - accuracy: 0.5178 - auc: 0.6846 - val_loss: 1.0304 - val_accuracy: 0.5234 - val_auc: 0.6853\n",
      "Epoch 24/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0290 - accuracy: 0.5243 - auc: 0.6869 - val_loss: 1.0289 - val_accuracy: 0.5253 - val_auc: 0.6870\n",
      "Epoch 25/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0274 - accuracy: 0.5277 - auc: 0.6888 - val_loss: 1.0274 - val_accuracy: 0.5261 - val_auc: 0.6888\n",
      "Epoch 26/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0257 - accuracy: 0.5294 - auc: 0.6902 - val_loss: 1.0259 - val_accuracy: 0.5274 - val_auc: 0.6901\n",
      "Epoch 27/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0241 - accuracy: 0.5314 - auc: 0.6915 - val_loss: 1.0245 - val_accuracy: 0.5286 - val_auc: 0.6908\n",
      "Epoch 28/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 1.0228 - accuracy: 0.5319 - auc: 0.6925 - val_loss: 1.0230 - val_accuracy: 0.5296 - val_auc: 0.6922\n",
      "Epoch 29/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0214 - accuracy: 0.5325 - auc: 0.6935 - val_loss: 1.0219 - val_accuracy: 0.5299 - val_auc: 0.6925\n",
      "Epoch 30/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0200 - accuracy: 0.5328 - auc: 0.6944 - val_loss: 1.0203 - val_accuracy: 0.5304 - val_auc: 0.6937\n",
      "Epoch 31/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0187 - accuracy: 0.5333 - auc: 0.6952 - val_loss: 1.0189 - val_accuracy: 0.5307 - val_auc: 0.6949\n",
      "Epoch 32/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0175 - accuracy: 0.5336 - auc: 0.6961 - val_loss: 1.0178 - val_accuracy: 0.5317 - val_auc: 0.6955\n",
      "Epoch 33/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0165 - accuracy: 0.5335 - auc: 0.6967 - val_loss: 1.0170 - val_accuracy: 0.5319 - val_auc: 0.6958\n",
      "Epoch 34/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0155 - accuracy: 0.5333 - auc: 0.6974 - val_loss: 1.0159 - val_accuracy: 0.5331 - val_auc: 0.6965\n",
      "Epoch 35/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0145 - accuracy: 0.5337 - auc: 0.6977 - val_loss: 1.0147 - val_accuracy: 0.5322 - val_auc: 0.6972\n",
      "Epoch 36/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0134 - accuracy: 0.5345 - auc: 0.6984 - val_loss: 1.0140 - val_accuracy: 0.5326 - val_auc: 0.6974\n",
      "Epoch 37/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0125 - accuracy: 0.5342 - auc: 0.6988 - val_loss: 1.0127 - val_accuracy: 0.5331 - val_auc: 0.6983\n",
      "Epoch 38/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0118 - accuracy: 0.5347 - auc: 0.6991 - val_loss: 1.0122 - val_accuracy: 0.5328 - val_auc: 0.6985\n",
      "Epoch 39/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0112 - accuracy: 0.5346 - auc: 0.6993 - val_loss: 1.0115 - val_accuracy: 0.5330 - val_auc: 0.6986\n",
      "Epoch 40/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0106 - accuracy: 0.5346 - auc: 0.6996 - val_loss: 1.0112 - val_accuracy: 0.5322 - val_auc: 0.6990\n",
      "Epoch 41/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0102 - accuracy: 0.5347 - auc: 0.6998 - val_loss: 1.0105 - val_accuracy: 0.5323 - val_auc: 0.6992\n",
      "Epoch 42/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0098 - accuracy: 0.5348 - auc: 0.6999 - val_loss: 1.0100 - val_accuracy: 0.5325 - val_auc: 0.6994\n",
      "Epoch 43/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0094 - accuracy: 0.5352 - auc: 0.7001 - val_loss: 1.0099 - val_accuracy: 0.5316 - val_auc: 0.6997\n",
      "Epoch 44/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0089 - accuracy: 0.5342 - auc: 0.7004 - val_loss: 1.0094 - val_accuracy: 0.5320 - val_auc: 0.7000\n",
      "Epoch 45/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0086 - accuracy: 0.5346 - auc: 0.7006 - val_loss: 1.0089 - val_accuracy: 0.5319 - val_auc: 0.7003\n",
      "Epoch 46/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0082 - accuracy: 0.5345 - auc: 0.7008 - val_loss: 1.0084 - val_accuracy: 0.5329 - val_auc: 0.7004\n",
      "Epoch 47/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0078 - accuracy: 0.5341 - auc: 0.7009 - val_loss: 1.0080 - val_accuracy: 0.5329 - val_auc: 0.7006\n",
      "Epoch 48/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0075 - accuracy: 0.5344 - auc: 0.7010 - val_loss: 1.0075 - val_accuracy: 0.5322 - val_auc: 0.7009\n",
      "Epoch 49/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0072 - accuracy: 0.5339 - auc: 0.7012 - val_loss: 1.0072 - val_accuracy: 0.5326 - val_auc: 0.7011\n",
      "Epoch 50/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0069 - accuracy: 0.5343 - auc: 0.7014 - val_loss: 1.0070 - val_accuracy: 0.5324 - val_auc: 0.7011\n",
      "Epoch 51/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0065 - accuracy: 0.5346 - auc: 0.7015 - val_loss: 1.0070 - val_accuracy: 0.5318 - val_auc: 0.7011\n",
      "Epoch 52/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0064 - accuracy: 0.5345 - auc: 0.7016 - val_loss: 1.0065 - val_accuracy: 0.5321 - val_auc: 0.7011\n",
      "Epoch 53/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0060 - accuracy: 0.5347 - auc: 0.7017 - val_loss: 1.0060 - val_accuracy: 0.5315 - val_auc: 0.7015\n",
      "Epoch 54/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0056 - accuracy: 0.5346 - auc: 0.7019 - val_loss: 1.0058 - val_accuracy: 0.5322 - val_auc: 0.7015\n",
      "Epoch 55/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0054 - accuracy: 0.5344 - auc: 0.7020 - val_loss: 1.0056 - val_accuracy: 0.5322 - val_auc: 0.7015\n",
      "Epoch 56/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0052 - accuracy: 0.5343 - auc: 0.7020 - val_loss: 1.0054 - val_accuracy: 0.5323 - val_auc: 0.7016\n",
      "Epoch 57/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0049 - accuracy: 0.5344 - auc: 0.7022 - val_loss: 1.0051 - val_accuracy: 0.5326 - val_auc: 0.7019\n",
      "Epoch 58/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0048 - accuracy: 0.5346 - auc: 0.7023 - val_loss: 1.0049 - val_accuracy: 0.5319 - val_auc: 0.7022\n",
      "Epoch 59/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0045 - accuracy: 0.5341 - auc: 0.7024 - val_loss: 1.0045 - val_accuracy: 0.5318 - val_auc: 0.7022\n",
      "Epoch 60/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0044 - accuracy: 0.5343 - auc: 0.7023 - val_loss: 1.0046 - val_accuracy: 0.5328 - val_auc: 0.7019\n",
      "Epoch 61/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0042 - accuracy: 0.5346 - auc: 0.7025 - val_loss: 1.0044 - val_accuracy: 0.5323 - val_auc: 0.7019\n",
      "Epoch 62/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0041 - accuracy: 0.5343 - auc: 0.7024 - val_loss: 1.0040 - val_accuracy: 0.5326 - val_auc: 0.7025\n",
      "Epoch 63/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0039 - accuracy: 0.5344 - auc: 0.7025 - val_loss: 1.0041 - val_accuracy: 0.5316 - val_auc: 0.7021\n",
      "Epoch 64/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0037 - accuracy: 0.5346 - auc: 0.7027 - val_loss: 1.0040 - val_accuracy: 0.5324 - val_auc: 0.7022\n",
      "Epoch 65/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0036 - accuracy: 0.5344 - auc: 0.7026 - val_loss: 1.0035 - val_accuracy: 0.5334 - val_auc: 0.7025\n",
      "Epoch 66/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0034 - accuracy: 0.5344 - auc: 0.7027 - val_loss: 1.0038 - val_accuracy: 0.5316 - val_auc: 0.7022\n",
      "Epoch 67/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0033 - accuracy: 0.5349 - auc: 0.7028 - val_loss: 1.0032 - val_accuracy: 0.5320 - val_auc: 0.7027\n",
      "Epoch 68/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0031 - accuracy: 0.5346 - auc: 0.7029 - val_loss: 1.0030 - val_accuracy: 0.5325 - val_auc: 0.7027\n",
      "Epoch 69/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0030 - accuracy: 0.5352 - auc: 0.7029 - val_loss: 1.0031 - val_accuracy: 0.5322 - val_auc: 0.7027\n",
      "Epoch 70/500\n",
      "676/676 [==============================] - 3s 5ms/step - loss: 1.0028 - accuracy: 0.5346 - auc: 0.7030 - val_loss: 1.0032 - val_accuracy: 0.5333 - val_auc: 0.7022\n",
      "Epoch 71/500\n",
      "676/676 [==============================] - 3s 5ms/step - loss: 1.0027 - accuracy: 0.5348 - auc: 0.7029 - val_loss: 1.0026 - val_accuracy: 0.5331 - val_auc: 0.7029\n",
      "Epoch 72/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0023 - accuracy: 0.5346 - auc: 0.7031 - val_loss: 1.0026 - val_accuracy: 0.5335 - val_auc: 0.7025\n",
      "Epoch 73/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0023 - accuracy: 0.5347 - auc: 0.7029 - val_loss: 1.0021 - val_accuracy: 0.5335 - val_auc: 0.7029\n",
      "Epoch 74/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0022 - accuracy: 0.5347 - auc: 0.7031 - val_loss: 1.0020 - val_accuracy: 0.5336 - val_auc: 0.7031\n",
      "Epoch 75/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0019 - accuracy: 0.5348 - auc: 0.7031 - val_loss: 1.0020 - val_accuracy: 0.5334 - val_auc: 0.7027\n",
      "Epoch 76/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0018 - accuracy: 0.5347 - auc: 0.7033 - val_loss: 1.0016 - val_accuracy: 0.5332 - val_auc: 0.7033\n",
      "Epoch 77/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0017 - accuracy: 0.5348 - auc: 0.7033 - val_loss: 1.0020 - val_accuracy: 0.5337 - val_auc: 0.7028\n",
      "Epoch 78/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0016 - accuracy: 0.5345 - auc: 0.7033 - val_loss: 1.0015 - val_accuracy: 0.5338 - val_auc: 0.7032\n",
      "Epoch 79/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0015 - accuracy: 0.5347 - auc: 0.7033 - val_loss: 1.0012 - val_accuracy: 0.5336 - val_auc: 0.7035\n",
      "Epoch 80/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0014 - accuracy: 0.5346 - auc: 0.7035 - val_loss: 1.0013 - val_accuracy: 0.5342 - val_auc: 0.7034\n",
      "Epoch 81/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0014 - accuracy: 0.5346 - auc: 0.7033 - val_loss: 1.0012 - val_accuracy: 0.5339 - val_auc: 0.7035\n",
      "Epoch 82/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0012 - accuracy: 0.5349 - auc: 0.7034 - val_loss: 1.0011 - val_accuracy: 0.5344 - val_auc: 0.7035\n",
      "Epoch 83/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0012 - accuracy: 0.5351 - auc: 0.7034 - val_loss: 1.0008 - val_accuracy: 0.5344 - val_auc: 0.7037\n",
      "Epoch 84/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0010 - accuracy: 0.5353 - auc: 0.7036 - val_loss: 1.0007 - val_accuracy: 0.5348 - val_auc: 0.7037\n",
      "Epoch 85/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0010 - accuracy: 0.5357 - auc: 0.7035 - val_loss: 1.0006 - val_accuracy: 0.5351 - val_auc: 0.7038\n",
      "Epoch 86/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0008 - accuracy: 0.5358 - auc: 0.7037 - val_loss: 1.0010 - val_accuracy: 0.5353 - val_auc: 0.7033\n",
      "Epoch 87/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0007 - accuracy: 0.5355 - auc: 0.7037 - val_loss: 1.0004 - val_accuracy: 0.5352 - val_auc: 0.7039\n",
      "Epoch 88/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0007 - accuracy: 0.5358 - auc: 0.7037 - val_loss: 1.0003 - val_accuracy: 0.5348 - val_auc: 0.7039\n",
      "Epoch 89/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0005 - accuracy: 0.5359 - auc: 0.7037 - val_loss: 1.0001 - val_accuracy: 0.5361 - val_auc: 0.7040\n",
      "Epoch 90/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0004 - accuracy: 0.5360 - auc: 0.7038 - val_loss: 1.0003 - val_accuracy: 0.5352 - val_auc: 0.7038\n",
      "Epoch 91/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0004 - accuracy: 0.5360 - auc: 0.7039 - val_loss: 1.0002 - val_accuracy: 0.5364 - val_auc: 0.7040\n",
      "Epoch 92/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0004 - accuracy: 0.5364 - auc: 0.7038 - val_loss: 1.0003 - val_accuracy: 0.5356 - val_auc: 0.7037\n",
      "Epoch 93/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0003 - accuracy: 0.5360 - auc: 0.7038 - val_loss: 1.0002 - val_accuracy: 0.5355 - val_auc: 0.7040\n",
      "Epoch 94/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0003 - accuracy: 0.5360 - auc: 0.7038 - val_loss: 0.9999 - val_accuracy: 0.5366 - val_auc: 0.7040\n",
      "Epoch 95/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0002 - accuracy: 0.5359 - auc: 0.7038 - val_loss: 0.9999 - val_accuracy: 0.5363 - val_auc: 0.7038\n",
      "Epoch 96/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0001 - accuracy: 0.5359 - auc: 0.7039 - val_loss: 0.9999 - val_accuracy: 0.5363 - val_auc: 0.7041\n",
      "Epoch 97/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0001 - accuracy: 0.5364 - auc: 0.7040 - val_loss: 0.9997 - val_accuracy: 0.5356 - val_auc: 0.7041\n",
      "Epoch 98/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 1.0000 - accuracy: 0.5367 - auc: 0.7039 - val_loss: 0.9996 - val_accuracy: 0.5365 - val_auc: 0.7043\n",
      "Epoch 99/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9999 - accuracy: 0.5360 - auc: 0.7041 - val_loss: 0.9993 - val_accuracy: 0.5366 - val_auc: 0.7043\n",
      "Epoch 100/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9998 - accuracy: 0.5366 - auc: 0.7041 - val_loss: 0.9996 - val_accuracy: 0.5357 - val_auc: 0.7041\n",
      "Epoch 101/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9998 - accuracy: 0.5365 - auc: 0.7039 - val_loss: 0.9994 - val_accuracy: 0.5364 - val_auc: 0.7043\n",
      "Epoch 102/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9997 - accuracy: 0.5362 - auc: 0.7040 - val_loss: 0.9998 - val_accuracy: 0.5366 - val_auc: 0.7037\n",
      "Epoch 103/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9997 - accuracy: 0.5368 - auc: 0.7040 - val_loss: 0.9993 - val_accuracy: 0.5370 - val_auc: 0.7042\n",
      "Epoch 104/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9996 - accuracy: 0.5365 - auc: 0.7041 - val_loss: 0.9991 - val_accuracy: 0.5367 - val_auc: 0.7043\n",
      "Epoch 105/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9996 - accuracy: 0.5363 - auc: 0.7041 - val_loss: 0.9992 - val_accuracy: 0.5373 - val_auc: 0.7043\n",
      "Epoch 106/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9995 - accuracy: 0.5365 - auc: 0.7041 - val_loss: 0.9992 - val_accuracy: 0.5361 - val_auc: 0.7043\n",
      "Epoch 107/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9995 - accuracy: 0.5363 - auc: 0.7042 - val_loss: 0.9990 - val_accuracy: 0.5373 - val_auc: 0.7043\n",
      "Epoch 108/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9994 - accuracy: 0.5367 - auc: 0.7042 - val_loss: 0.9988 - val_accuracy: 0.5367 - val_auc: 0.7045\n",
      "Epoch 109/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9994 - accuracy: 0.5371 - auc: 0.7041 - val_loss: 0.9989 - val_accuracy: 0.5372 - val_auc: 0.7043\n",
      "Epoch 110/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9993 - accuracy: 0.5360 - auc: 0.7041 - val_loss: 0.9989 - val_accuracy: 0.5367 - val_auc: 0.7044\n",
      "Epoch 111/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9993 - accuracy: 0.5369 - auc: 0.7041 - val_loss: 0.9988 - val_accuracy: 0.5373 - val_auc: 0.7043\n",
      "Epoch 112/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9992 - accuracy: 0.5364 - auc: 0.7043 - val_loss: 0.9986 - val_accuracy: 0.5374 - val_auc: 0.7045\n",
      "Epoch 113/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9992 - accuracy: 0.5369 - auc: 0.7042 - val_loss: 0.9987 - val_accuracy: 0.5367 - val_auc: 0.7044\n",
      "Epoch 114/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9991 - accuracy: 0.5371 - auc: 0.7041 - val_loss: 0.9986 - val_accuracy: 0.5376 - val_auc: 0.7046\n",
      "Epoch 115/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9990 - accuracy: 0.5367 - auc: 0.7042 - val_loss: 0.9987 - val_accuracy: 0.5376 - val_auc: 0.7044\n",
      "Epoch 116/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9989 - accuracy: 0.5371 - auc: 0.7043 - val_loss: 0.9986 - val_accuracy: 0.5377 - val_auc: 0.7045\n",
      "Epoch 117/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9990 - accuracy: 0.5362 - auc: 0.7042 - val_loss: 0.9985 - val_accuracy: 0.5378 - val_auc: 0.7045\n",
      "Epoch 118/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9989 - accuracy: 0.5364 - auc: 0.7042 - val_loss: 0.9984 - val_accuracy: 0.5376 - val_auc: 0.7044\n",
      "Epoch 119/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9988 - accuracy: 0.5366 - auc: 0.7042 - val_loss: 0.9985 - val_accuracy: 0.5373 - val_auc: 0.7045\n",
      "Epoch 120/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9988 - accuracy: 0.5373 - auc: 0.7042 - val_loss: 0.9983 - val_accuracy: 0.5374 - val_auc: 0.7045\n",
      "Epoch 121/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9987 - accuracy: 0.5367 - auc: 0.7043 - val_loss: 0.9983 - val_accuracy: 0.5368 - val_auc: 0.7046\n",
      "Epoch 122/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9987 - accuracy: 0.5372 - auc: 0.7042 - val_loss: 0.9982 - val_accuracy: 0.5369 - val_auc: 0.7046\n",
      "Epoch 123/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9986 - accuracy: 0.5372 - auc: 0.7043 - val_loss: 0.9984 - val_accuracy: 0.5360 - val_auc: 0.7044\n",
      "Epoch 124/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9986 - accuracy: 0.5371 - auc: 0.7043 - val_loss: 0.9983 - val_accuracy: 0.5369 - val_auc: 0.7043\n",
      "Epoch 125/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9985 - accuracy: 0.5370 - auc: 0.7043 - val_loss: 0.9982 - val_accuracy: 0.5371 - val_auc: 0.7043\n",
      "Epoch 126/500\n",
      "676/676 [==============================] - 3s 5ms/step - loss: 0.9985 - accuracy: 0.5367 - auc: 0.7044 - val_loss: 0.9980 - val_accuracy: 0.5377 - val_auc: 0.7046\n",
      "Epoch 127/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9985 - accuracy: 0.5370 - auc: 0.7043 - val_loss: 0.9981 - val_accuracy: 0.5370 - val_auc: 0.7045\n",
      "Epoch 128/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9985 - accuracy: 0.5372 - auc: 0.7043 - val_loss: 0.9983 - val_accuracy: 0.5367 - val_auc: 0.7042\n",
      "Epoch 129/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9984 - accuracy: 0.5372 - auc: 0.7044 - val_loss: 0.9979 - val_accuracy: 0.5367 - val_auc: 0.7046\n",
      "Epoch 130/500\n",
      "676/676 [==============================] - 3s 5ms/step - loss: 0.9983 - accuracy: 0.5371 - auc: 0.7044 - val_loss: 0.9982 - val_accuracy: 0.5377 - val_auc: 0.7042\n",
      "Epoch 131/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9983 - accuracy: 0.5370 - auc: 0.7043 - val_loss: 0.9978 - val_accuracy: 0.5376 - val_auc: 0.7046\n",
      "Epoch 132/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9983 - accuracy: 0.5369 - auc: 0.7043 - val_loss: 0.9977 - val_accuracy: 0.5376 - val_auc: 0.7048\n",
      "Epoch 133/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9983 - accuracy: 0.5368 - auc: 0.7043 - val_loss: 0.9977 - val_accuracy: 0.5374 - val_auc: 0.7050\n",
      "Epoch 134/500\n",
      "676/676 [==============================] - 3s 5ms/step - loss: 0.9982 - accuracy: 0.5373 - auc: 0.7045 - val_loss: 0.9978 - val_accuracy: 0.5367 - val_auc: 0.7046\n",
      "Epoch 135/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9983 - accuracy: 0.5369 - auc: 0.7043 - val_loss: 0.9976 - val_accuracy: 0.5374 - val_auc: 0.7049\n",
      "Epoch 136/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9982 - accuracy: 0.5368 - auc: 0.7043 - val_loss: 0.9978 - val_accuracy: 0.5374 - val_auc: 0.7045\n",
      "Epoch 137/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9982 - accuracy: 0.5370 - auc: 0.7045 - val_loss: 0.9978 - val_accuracy: 0.5370 - val_auc: 0.7046\n",
      "Epoch 138/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9981 - accuracy: 0.5366 - auc: 0.7045 - val_loss: 0.9977 - val_accuracy: 0.5380 - val_auc: 0.7044\n",
      "Epoch 139/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9980 - accuracy: 0.5368 - auc: 0.7045 - val_loss: 0.9977 - val_accuracy: 0.5373 - val_auc: 0.7047\n",
      "Epoch 140/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9980 - accuracy: 0.5372 - auc: 0.7045 - val_loss: 0.9976 - val_accuracy: 0.5374 - val_auc: 0.7047\n",
      "Epoch 141/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9979 - accuracy: 0.5374 - auc: 0.7046 - val_loss: 0.9973 - val_accuracy: 0.5377 - val_auc: 0.7049\n",
      "Epoch 142/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9980 - accuracy: 0.5370 - auc: 0.7044 - val_loss: 0.9975 - val_accuracy: 0.5376 - val_auc: 0.7047\n",
      "Epoch 143/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9978 - accuracy: 0.5372 - auc: 0.7046 - val_loss: 0.9975 - val_accuracy: 0.5368 - val_auc: 0.7047\n",
      "Epoch 144/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9978 - accuracy: 0.5372 - auc: 0.7045 - val_loss: 0.9974 - val_accuracy: 0.5376 - val_auc: 0.7048\n",
      "Epoch 145/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9978 - accuracy: 0.5369 - auc: 0.7045 - val_loss: 0.9973 - val_accuracy: 0.5368 - val_auc: 0.7049\n",
      "Epoch 146/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9978 - accuracy: 0.5370 - auc: 0.7045 - val_loss: 0.9972 - val_accuracy: 0.5373 - val_auc: 0.7048\n",
      "Epoch 147/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9977 - accuracy: 0.5372 - auc: 0.7045 - val_loss: 0.9971 - val_accuracy: 0.5372 - val_auc: 0.7050\n",
      "Epoch 148/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9978 - accuracy: 0.5373 - auc: 0.7046 - val_loss: 0.9976 - val_accuracy: 0.5370 - val_auc: 0.7046\n",
      "Epoch 149/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9978 - accuracy: 0.5372 - auc: 0.7044 - val_loss: 0.9971 - val_accuracy: 0.5370 - val_auc: 0.7049\n",
      "Epoch 150/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9976 - accuracy: 0.5370 - auc: 0.7045 - val_loss: 0.9972 - val_accuracy: 0.5372 - val_auc: 0.7046\n",
      "Epoch 151/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9976 - accuracy: 0.5373 - auc: 0.7046 - val_loss: 0.9971 - val_accuracy: 0.5373 - val_auc: 0.7048\n",
      "Epoch 152/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9977 - accuracy: 0.5372 - auc: 0.7044 - val_loss: 0.9973 - val_accuracy: 0.5371 - val_auc: 0.7049\n",
      "Epoch 153/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9975 - accuracy: 0.5371 - auc: 0.7046 - val_loss: 0.9970 - val_accuracy: 0.5373 - val_auc: 0.7050\n",
      "Epoch 154/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9975 - accuracy: 0.5372 - auc: 0.7045 - val_loss: 0.9970 - val_accuracy: 0.5374 - val_auc: 0.7048\n",
      "Epoch 155/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9975 - accuracy: 0.5370 - auc: 0.7046 - val_loss: 0.9969 - val_accuracy: 0.5370 - val_auc: 0.7049\n",
      "Epoch 156/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9974 - accuracy: 0.5369 - auc: 0.7046 - val_loss: 0.9970 - val_accuracy: 0.5370 - val_auc: 0.7048\n",
      "Epoch 157/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9974 - accuracy: 0.5372 - auc: 0.7045 - val_loss: 0.9970 - val_accuracy: 0.5370 - val_auc: 0.7048\n",
      "Epoch 158/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9975 - accuracy: 0.5367 - auc: 0.7044 - val_loss: 0.9975 - val_accuracy: 0.5365 - val_auc: 0.7042\n",
      "Epoch 159/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9973 - accuracy: 0.5372 - auc: 0.7046 - val_loss: 0.9975 - val_accuracy: 0.5366 - val_auc: 0.7044\n",
      "Epoch 160/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9972 - accuracy: 0.5369 - auc: 0.7047 - val_loss: 0.9968 - val_accuracy: 0.5369 - val_auc: 0.7049\n",
      "Epoch 161/500\n",
      "676/676 [==============================] - 3s 5ms/step - loss: 0.9972 - accuracy: 0.5373 - auc: 0.7046 - val_loss: 0.9968 - val_accuracy: 0.5368 - val_auc: 0.7051\n",
      "Epoch 162/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9972 - accuracy: 0.5369 - auc: 0.7046 - val_loss: 0.9968 - val_accuracy: 0.5371 - val_auc: 0.7049\n",
      "Epoch 163/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9973 - accuracy: 0.5371 - auc: 0.7046 - val_loss: 0.9968 - val_accuracy: 0.5374 - val_auc: 0.7049\n",
      "Epoch 164/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9972 - accuracy: 0.5369 - auc: 0.7046 - val_loss: 0.9967 - val_accuracy: 0.5375 - val_auc: 0.7050\n",
      "Epoch 165/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9972 - accuracy: 0.5372 - auc: 0.7047 - val_loss: 0.9968 - val_accuracy: 0.5371 - val_auc: 0.7047\n",
      "Epoch 166/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9971 - accuracy: 0.5374 - auc: 0.7046 - val_loss: 0.9967 - val_accuracy: 0.5376 - val_auc: 0.7050\n",
      "Epoch 167/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9972 - accuracy: 0.5372 - auc: 0.7046 - val_loss: 0.9967 - val_accuracy: 0.5373 - val_auc: 0.7049\n",
      "Epoch 168/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9971 - accuracy: 0.5373 - auc: 0.7046 - val_loss: 0.9968 - val_accuracy: 0.5369 - val_auc: 0.7047\n",
      "Epoch 169/500\n",
      "676/676 [==============================] - 3s 5ms/step - loss: 0.9970 - accuracy: 0.5369 - auc: 0.7047 - val_loss: 0.9966 - val_accuracy: 0.5380 - val_auc: 0.7048\n",
      "Epoch 170/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9971 - accuracy: 0.5369 - auc: 0.7046 - val_loss: 0.9967 - val_accuracy: 0.5357 - val_auc: 0.7047\n",
      "Epoch 171/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9969 - accuracy: 0.5372 - auc: 0.7047 - val_loss: 0.9966 - val_accuracy: 0.5368 - val_auc: 0.7047\n",
      "Epoch 172/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9969 - accuracy: 0.5379 - auc: 0.7048 - val_loss: 0.9965 - val_accuracy: 0.5363 - val_auc: 0.7050\n",
      "Epoch 173/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9969 - accuracy: 0.5372 - auc: 0.7046 - val_loss: 0.9965 - val_accuracy: 0.5367 - val_auc: 0.7049\n",
      "Epoch 174/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9968 - accuracy: 0.5373 - auc: 0.7048 - val_loss: 0.9966 - val_accuracy: 0.5364 - val_auc: 0.7051\n",
      "Epoch 175/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9969 - accuracy: 0.5370 - auc: 0.7047 - val_loss: 0.9965 - val_accuracy: 0.5374 - val_auc: 0.7048\n",
      "Epoch 176/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9968 - accuracy: 0.5370 - auc: 0.7047 - val_loss: 0.9966 - val_accuracy: 0.5374 - val_auc: 0.7049\n",
      "Epoch 177/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9968 - accuracy: 0.5369 - auc: 0.7047 - val_loss: 0.9964 - val_accuracy: 0.5383 - val_auc: 0.7049\n",
      "Epoch 178/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9967 - accuracy: 0.5372 - auc: 0.7048 - val_loss: 0.9964 - val_accuracy: 0.5370 - val_auc: 0.7050\n",
      "Epoch 179/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9968 - accuracy: 0.5369 - auc: 0.7047 - val_loss: 0.9960 - val_accuracy: 0.5377 - val_auc: 0.7052\n",
      "Epoch 180/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9967 - accuracy: 0.5369 - auc: 0.7047 - val_loss: 0.9961 - val_accuracy: 0.5380 - val_auc: 0.7051\n",
      "Epoch 181/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9966 - accuracy: 0.5371 - auc: 0.7048 - val_loss: 0.9968 - val_accuracy: 0.5359 - val_auc: 0.7044\n",
      "Epoch 182/500\n",
      "676/676 [==============================] - 3s 5ms/step - loss: 0.9967 - accuracy: 0.5372 - auc: 0.7047 - val_loss: 0.9964 - val_accuracy: 0.5361 - val_auc: 0.7048\n",
      "Epoch 183/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9967 - accuracy: 0.5373 - auc: 0.7046 - val_loss: 0.9961 - val_accuracy: 0.5374 - val_auc: 0.7050\n",
      "Epoch 184/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9966 - accuracy: 0.5369 - auc: 0.7048 - val_loss: 0.9961 - val_accuracy: 0.5379 - val_auc: 0.7050\n",
      "Epoch 185/500\n",
      "676/676 [==============================] - 3s 5ms/step - loss: 0.9966 - accuracy: 0.5369 - auc: 0.7046 - val_loss: 0.9961 - val_accuracy: 0.5370 - val_auc: 0.7050\n",
      "Epoch 186/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9966 - accuracy: 0.5372 - auc: 0.7046 - val_loss: 0.9964 - val_accuracy: 0.5358 - val_auc: 0.7047\n",
      "Epoch 187/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9965 - accuracy: 0.5371 - auc: 0.7047 - val_loss: 0.9959 - val_accuracy: 0.5371 - val_auc: 0.7051\n",
      "Epoch 188/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9966 - accuracy: 0.5370 - auc: 0.7046 - val_loss: 0.9960 - val_accuracy: 0.5374 - val_auc: 0.7051\n",
      "Epoch 189/500\n",
      "676/676 [==============================] - 3s 5ms/step - loss: 0.9965 - accuracy: 0.5374 - auc: 0.7047 - val_loss: 0.9963 - val_accuracy: 0.5368 - val_auc: 0.7047\n",
      "Epoch 190/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9965 - accuracy: 0.5373 - auc: 0.7047 - val_loss: 0.9958 - val_accuracy: 0.5381 - val_auc: 0.7053\n",
      "Epoch 191/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9965 - accuracy: 0.5371 - auc: 0.7048 - val_loss: 0.9960 - val_accuracy: 0.5381 - val_auc: 0.7051\n",
      "Epoch 192/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9964 - accuracy: 0.5373 - auc: 0.7047 - val_loss: 0.9959 - val_accuracy: 0.5371 - val_auc: 0.7050\n",
      "Epoch 193/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9963 - accuracy: 0.5372 - auc: 0.7048 - val_loss: 0.9957 - val_accuracy: 0.5375 - val_auc: 0.7051\n",
      "Epoch 194/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9964 - accuracy: 0.5372 - auc: 0.7048 - val_loss: 0.9958 - val_accuracy: 0.5376 - val_auc: 0.7051\n",
      "Epoch 195/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9964 - accuracy: 0.5371 - auc: 0.7046 - val_loss: 0.9957 - val_accuracy: 0.5374 - val_auc: 0.7052\n",
      "Epoch 196/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9963 - accuracy: 0.5372 - auc: 0.7048 - val_loss: 0.9959 - val_accuracy: 0.5379 - val_auc: 0.7048\n",
      "Epoch 197/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9963 - accuracy: 0.5371 - auc: 0.7048 - val_loss: 0.9957 - val_accuracy: 0.5360 - val_auc: 0.7051\n",
      "Epoch 198/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9963 - accuracy: 0.5373 - auc: 0.7047 - val_loss: 0.9955 - val_accuracy: 0.5367 - val_auc: 0.7054\n",
      "Epoch 199/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9962 - accuracy: 0.5371 - auc: 0.7048 - val_loss: 0.9960 - val_accuracy: 0.5376 - val_auc: 0.7050\n",
      "Epoch 200/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9963 - accuracy: 0.5373 - auc: 0.7047 - val_loss: 0.9957 - val_accuracy: 0.5376 - val_auc: 0.7049\n",
      "Epoch 201/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9962 - accuracy: 0.5375 - auc: 0.7047 - val_loss: 0.9956 - val_accuracy: 0.5376 - val_auc: 0.7049\n",
      "Epoch 202/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9962 - accuracy: 0.5370 - auc: 0.7047 - val_loss: 0.9957 - val_accuracy: 0.5371 - val_auc: 0.7049\n",
      "Epoch 203/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9962 - accuracy: 0.5368 - auc: 0.7048 - val_loss: 0.9956 - val_accuracy: 0.5367 - val_auc: 0.7049\n",
      "Epoch 204/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9962 - accuracy: 0.5371 - auc: 0.7047 - val_loss: 0.9954 - val_accuracy: 0.5372 - val_auc: 0.7053\n",
      "Epoch 205/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9962 - accuracy: 0.5369 - auc: 0.7048 - val_loss: 0.9957 - val_accuracy: 0.5378 - val_auc: 0.7049\n",
      "Epoch 206/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9961 - accuracy: 0.5372 - auc: 0.7047 - val_loss: 0.9954 - val_accuracy: 0.5374 - val_auc: 0.7051\n",
      "Epoch 207/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9960 - accuracy: 0.5371 - auc: 0.7048 - val_loss: 0.9954 - val_accuracy: 0.5374 - val_auc: 0.7051\n",
      "Epoch 208/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9959 - accuracy: 0.5372 - auc: 0.7049 - val_loss: 0.9954 - val_accuracy: 0.5377 - val_auc: 0.7053\n",
      "Epoch 209/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9959 - accuracy: 0.5372 - auc: 0.7048 - val_loss: 0.9954 - val_accuracy: 0.5379 - val_auc: 0.7053\n",
      "Epoch 210/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9960 - accuracy: 0.5373 - auc: 0.7047 - val_loss: 0.9956 - val_accuracy: 0.5365 - val_auc: 0.7048\n",
      "Epoch 211/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9960 - accuracy: 0.5369 - auc: 0.7048 - val_loss: 0.9954 - val_accuracy: 0.5376 - val_auc: 0.7053\n",
      "Epoch 212/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9960 - accuracy: 0.5369 - auc: 0.7048 - val_loss: 0.9953 - val_accuracy: 0.5364 - val_auc: 0.7052\n",
      "Epoch 213/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9959 - accuracy: 0.5376 - auc: 0.7048 - val_loss: 0.9958 - val_accuracy: 0.5371 - val_auc: 0.7048\n",
      "Epoch 214/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9960 - accuracy: 0.5370 - auc: 0.7047 - val_loss: 0.9952 - val_accuracy: 0.5373 - val_auc: 0.7054\n",
      "Epoch 215/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9959 - accuracy: 0.5371 - auc: 0.7048 - val_loss: 0.9952 - val_accuracy: 0.5373 - val_auc: 0.7051\n",
      "Epoch 216/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9958 - accuracy: 0.5371 - auc: 0.7048 - val_loss: 0.9955 - val_accuracy: 0.5364 - val_auc: 0.7046\n",
      "Epoch 217/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9959 - accuracy: 0.5370 - auc: 0.7047 - val_loss: 0.9952 - val_accuracy: 0.5372 - val_auc: 0.7052\n",
      "Epoch 218/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9958 - accuracy: 0.5373 - auc: 0.7049 - val_loss: 0.9952 - val_accuracy: 0.5371 - val_auc: 0.7052\n",
      "Epoch 219/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9958 - accuracy: 0.5373 - auc: 0.7048 - val_loss: 0.9952 - val_accuracy: 0.5374 - val_auc: 0.7051\n",
      "Epoch 220/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9958 - accuracy: 0.5373 - auc: 0.7047 - val_loss: 0.9952 - val_accuracy: 0.5380 - val_auc: 0.7053\n",
      "Epoch 221/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9957 - accuracy: 0.5373 - auc: 0.7048 - val_loss: 0.9952 - val_accuracy: 0.5367 - val_auc: 0.7051\n",
      "Epoch 222/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9957 - accuracy: 0.5370 - auc: 0.7048 - val_loss: 0.9951 - val_accuracy: 0.5377 - val_auc: 0.7053\n",
      "Epoch 223/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9957 - accuracy: 0.5374 - auc: 0.7048 - val_loss: 0.9950 - val_accuracy: 0.5375 - val_auc: 0.7053\n",
      "Epoch 224/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9957 - accuracy: 0.5370 - auc: 0.7049 - val_loss: 0.9950 - val_accuracy: 0.5375 - val_auc: 0.7053\n",
      "Epoch 225/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9957 - accuracy: 0.5373 - auc: 0.7049 - val_loss: 0.9951 - val_accuracy: 0.5371 - val_auc: 0.7052\n",
      "Epoch 226/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9957 - accuracy: 0.5376 - auc: 0.7047 - val_loss: 0.9949 - val_accuracy: 0.5378 - val_auc: 0.7053\n",
      "Epoch 227/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9957 - accuracy: 0.5374 - auc: 0.7048 - val_loss: 0.9952 - val_accuracy: 0.5372 - val_auc: 0.7050\n",
      "Epoch 228/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9956 - accuracy: 0.5374 - auc: 0.7049 - val_loss: 0.9953 - val_accuracy: 0.5364 - val_auc: 0.7047\n",
      "Epoch 229/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9956 - accuracy: 0.5377 - auc: 0.7048 - val_loss: 0.9950 - val_accuracy: 0.5370 - val_auc: 0.7052\n",
      "Epoch 230/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9955 - accuracy: 0.5377 - auc: 0.7049 - val_loss: 0.9950 - val_accuracy: 0.5372 - val_auc: 0.7052\n",
      "Epoch 231/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9956 - accuracy: 0.5376 - auc: 0.7049 - val_loss: 0.9951 - val_accuracy: 0.5379 - val_auc: 0.7051\n",
      "Epoch 232/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9955 - accuracy: 0.5374 - auc: 0.7049 - val_loss: 0.9951 - val_accuracy: 0.5373 - val_auc: 0.7050\n",
      "Epoch 233/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9954 - accuracy: 0.5379 - auc: 0.7050 - val_loss: 0.9948 - val_accuracy: 0.5377 - val_auc: 0.7054\n",
      "Epoch 234/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9955 - accuracy: 0.5381 - auc: 0.7048 - val_loss: 0.9948 - val_accuracy: 0.5370 - val_auc: 0.7054\n",
      "Epoch 235/500\n",
      "676/676 [==============================] - 3s 5ms/step - loss: 0.9955 - accuracy: 0.5372 - auc: 0.7049 - val_loss: 0.9955 - val_accuracy: 0.5368 - val_auc: 0.7046\n",
      "Epoch 236/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9955 - accuracy: 0.5374 - auc: 0.7049 - val_loss: 0.9948 - val_accuracy: 0.5369 - val_auc: 0.7053\n",
      "Epoch 237/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9956 - accuracy: 0.5374 - auc: 0.7048 - val_loss: 0.9950 - val_accuracy: 0.5382 - val_auc: 0.7054\n",
      "Epoch 238/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9954 - accuracy: 0.5373 - auc: 0.7050 - val_loss: 0.9948 - val_accuracy: 0.5371 - val_auc: 0.7053\n",
      "Epoch 239/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9955 - accuracy: 0.5377 - auc: 0.7048 - val_loss: 0.9948 - val_accuracy: 0.5370 - val_auc: 0.7051\n",
      "Epoch 240/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9954 - accuracy: 0.5377 - auc: 0.7049 - val_loss: 0.9950 - val_accuracy: 0.5369 - val_auc: 0.7049\n",
      "Epoch 241/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9954 - accuracy: 0.5375 - auc: 0.7049 - val_loss: 0.9946 - val_accuracy: 0.5373 - val_auc: 0.7053\n",
      "Epoch 242/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9954 - accuracy: 0.5375 - auc: 0.7048 - val_loss: 0.9947 - val_accuracy: 0.5371 - val_auc: 0.7054\n",
      "Epoch 243/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9952 - accuracy: 0.5377 - auc: 0.7050 - val_loss: 0.9947 - val_accuracy: 0.5374 - val_auc: 0.7052\n",
      "Epoch 244/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9954 - accuracy: 0.5374 - auc: 0.7049 - val_loss: 0.9947 - val_accuracy: 0.5374 - val_auc: 0.7052\n",
      "Epoch 245/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9953 - accuracy: 0.5377 - auc: 0.7049 - val_loss: 0.9945 - val_accuracy: 0.5376 - val_auc: 0.7055\n",
      "Epoch 246/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9952 - accuracy: 0.5374 - auc: 0.7050 - val_loss: 0.9946 - val_accuracy: 0.5370 - val_auc: 0.7055\n",
      "Epoch 247/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9952 - accuracy: 0.5373 - auc: 0.7049 - val_loss: 0.9948 - val_accuracy: 0.5376 - val_auc: 0.7052\n",
      "Epoch 248/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9952 - accuracy: 0.5376 - auc: 0.7049 - val_loss: 0.9945 - val_accuracy: 0.5376 - val_auc: 0.7054\n",
      "Epoch 249/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9952 - accuracy: 0.5379 - auc: 0.7048 - val_loss: 0.9945 - val_accuracy: 0.5378 - val_auc: 0.7054\n",
      "Epoch 250/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9952 - accuracy: 0.5378 - auc: 0.7048 - val_loss: 0.9947 - val_accuracy: 0.5369 - val_auc: 0.7053\n",
      "Epoch 251/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9952 - accuracy: 0.5377 - auc: 0.7049 - val_loss: 0.9946 - val_accuracy: 0.5374 - val_auc: 0.7053\n",
      "Epoch 252/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9952 - accuracy: 0.5373 - auc: 0.7049 - val_loss: 0.9945 - val_accuracy: 0.5381 - val_auc: 0.7055\n",
      "Epoch 253/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9953 - accuracy: 0.5377 - auc: 0.7048 - val_loss: 0.9945 - val_accuracy: 0.5366 - val_auc: 0.7055\n",
      "Epoch 254/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9953 - accuracy: 0.5376 - auc: 0.7049 - val_loss: 0.9945 - val_accuracy: 0.5377 - val_auc: 0.7054\n",
      "Epoch 255/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9952 - accuracy: 0.5375 - auc: 0.7050 - val_loss: 0.9944 - val_accuracy: 0.5384 - val_auc: 0.7056\n",
      "Epoch 256/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9952 - accuracy: 0.5375 - auc: 0.7049 - val_loss: 0.9943 - val_accuracy: 0.5380 - val_auc: 0.7056\n",
      "Epoch 257/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9952 - accuracy: 0.5376 - auc: 0.7049 - val_loss: 0.9946 - val_accuracy: 0.5373 - val_auc: 0.7051\n",
      "Epoch 258/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9951 - accuracy: 0.5373 - auc: 0.7050 - val_loss: 0.9944 - val_accuracy: 0.5377 - val_auc: 0.7055\n",
      "Epoch 259/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9951 - accuracy: 0.5375 - auc: 0.7050 - val_loss: 0.9946 - val_accuracy: 0.5367 - val_auc: 0.7052\n",
      "Epoch 260/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9951 - accuracy: 0.5372 - auc: 0.7051 - val_loss: 0.9943 - val_accuracy: 0.5370 - val_auc: 0.7055\n",
      "Epoch 261/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9951 - accuracy: 0.5378 - auc: 0.7050 - val_loss: 0.9942 - val_accuracy: 0.5378 - val_auc: 0.7057\n",
      "Epoch 262/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9951 - accuracy: 0.5374 - auc: 0.7050 - val_loss: 0.9943 - val_accuracy: 0.5374 - val_auc: 0.7054\n",
      "Epoch 263/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9950 - accuracy: 0.5380 - auc: 0.7051 - val_loss: 0.9943 - val_accuracy: 0.5383 - val_auc: 0.7055\n",
      "Epoch 264/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9951 - accuracy: 0.5378 - auc: 0.7049 - val_loss: 0.9944 - val_accuracy: 0.5376 - val_auc: 0.7053\n",
      "Epoch 265/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9950 - accuracy: 0.5380 - auc: 0.7049 - val_loss: 0.9943 - val_accuracy: 0.5379 - val_auc: 0.7054\n",
      "Epoch 266/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9951 - accuracy: 0.5372 - auc: 0.7050 - val_loss: 0.9941 - val_accuracy: 0.5378 - val_auc: 0.7057\n",
      "Epoch 267/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9950 - accuracy: 0.5374 - auc: 0.7051 - val_loss: 0.9943 - val_accuracy: 0.5375 - val_auc: 0.7054\n",
      "Epoch 268/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9950 - accuracy: 0.5375 - auc: 0.7050 - val_loss: 0.9944 - val_accuracy: 0.5374 - val_auc: 0.7055\n",
      "Epoch 269/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9950 - accuracy: 0.5374 - auc: 0.7050 - val_loss: 0.9943 - val_accuracy: 0.5377 - val_auc: 0.7054\n",
      "Epoch 270/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9950 - accuracy: 0.5377 - auc: 0.7049 - val_loss: 0.9940 - val_accuracy: 0.5384 - val_auc: 0.7056\n",
      "Epoch 271/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9948 - accuracy: 0.5377 - auc: 0.7052 - val_loss: 0.9945 - val_accuracy: 0.5374 - val_auc: 0.7054\n",
      "Epoch 272/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9949 - accuracy: 0.5374 - auc: 0.7050 - val_loss: 0.9945 - val_accuracy: 0.5371 - val_auc: 0.7052\n",
      "Epoch 273/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9949 - accuracy: 0.5376 - auc: 0.7051 - val_loss: 0.9943 - val_accuracy: 0.5374 - val_auc: 0.7057\n",
      "Epoch 274/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9949 - accuracy: 0.5378 - auc: 0.7051 - val_loss: 0.9943 - val_accuracy: 0.5365 - val_auc: 0.7053\n",
      "Epoch 275/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9949 - accuracy: 0.5379 - auc: 0.7050 - val_loss: 0.9940 - val_accuracy: 0.5374 - val_auc: 0.7056\n",
      "Epoch 276/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9948 - accuracy: 0.5373 - auc: 0.7051 - val_loss: 0.9943 - val_accuracy: 0.5381 - val_auc: 0.7055\n",
      "Epoch 277/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9947 - accuracy: 0.5376 - auc: 0.7052 - val_loss: 0.9941 - val_accuracy: 0.5374 - val_auc: 0.7055\n",
      "Epoch 278/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9948 - accuracy: 0.5380 - auc: 0.7051 - val_loss: 0.9941 - val_accuracy: 0.5371 - val_auc: 0.7056\n",
      "Epoch 279/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9948 - accuracy: 0.5374 - auc: 0.7050 - val_loss: 0.9940 - val_accuracy: 0.5366 - val_auc: 0.7057\n",
      "Epoch 280/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9947 - accuracy: 0.5379 - auc: 0.7051 - val_loss: 0.9939 - val_accuracy: 0.5367 - val_auc: 0.7056\n",
      "Epoch 281/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9948 - accuracy: 0.5380 - auc: 0.7051 - val_loss: 0.9938 - val_accuracy: 0.5382 - val_auc: 0.7058\n",
      "Epoch 282/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9947 - accuracy: 0.5378 - auc: 0.7052 - val_loss: 0.9940 - val_accuracy: 0.5374 - val_auc: 0.7054\n",
      "Epoch 283/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9947 - accuracy: 0.5380 - auc: 0.7051 - val_loss: 0.9946 - val_accuracy: 0.5378 - val_auc: 0.7050\n",
      "Epoch 284/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9948 - accuracy: 0.5377 - auc: 0.7050 - val_loss: 0.9944 - val_accuracy: 0.5375 - val_auc: 0.7054\n",
      "Epoch 285/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9948 - accuracy: 0.5376 - auc: 0.7051 - val_loss: 0.9940 - val_accuracy: 0.5373 - val_auc: 0.7055\n",
      "Epoch 286/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9947 - accuracy: 0.5376 - auc: 0.7052 - val_loss: 0.9940 - val_accuracy: 0.5380 - val_auc: 0.7056\n",
      "Epoch 287/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9948 - accuracy: 0.5377 - auc: 0.7050 - val_loss: 0.9939 - val_accuracy: 0.5371 - val_auc: 0.7055\n",
      "Epoch 288/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9946 - accuracy: 0.5377 - auc: 0.7053 - val_loss: 0.9946 - val_accuracy: 0.5371 - val_auc: 0.7052\n",
      "Epoch 289/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9947 - accuracy: 0.5376 - auc: 0.7051 - val_loss: 0.9938 - val_accuracy: 0.5386 - val_auc: 0.7059\n",
      "Epoch 290/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9946 - accuracy: 0.5378 - auc: 0.7051 - val_loss: 0.9937 - val_accuracy: 0.5369 - val_auc: 0.7057\n",
      "Epoch 291/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9947 - accuracy: 0.5376 - auc: 0.7051 - val_loss: 0.9937 - val_accuracy: 0.5376 - val_auc: 0.7058\n",
      "Epoch 292/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 0.9945 - accuracy: 0.5379 - auc: 0.7052 - val_loss: 0.9936 - val_accuracy: 0.5383 - val_auc: 0.7059\n",
      "Epoch 293/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9946 - accuracy: 0.5385 - auc: 0.7052 - val_loss: 0.9939 - val_accuracy: 0.5378 - val_auc: 0.7056\n",
      "Epoch 294/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 0.9947 - accuracy: 0.5379 - auc: 0.7050 - val_loss: 0.9938 - val_accuracy: 0.5376 - val_auc: 0.7057\n",
      "Epoch 295/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 0.9946 - accuracy: 0.5376 - auc: 0.7051 - val_loss: 0.9939 - val_accuracy: 0.5379 - val_auc: 0.7058\n",
      "Epoch 296/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9946 - accuracy: 0.5377 - auc: 0.7050 - val_loss: 0.9937 - val_accuracy: 0.5373 - val_auc: 0.7057\n",
      "Epoch 297/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 0.9945 - accuracy: 0.5379 - auc: 0.7052 - val_loss: 0.9936 - val_accuracy: 0.5383 - val_auc: 0.7059\n",
      "Epoch 298/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9945 - accuracy: 0.5377 - auc: 0.7052 - val_loss: 0.9935 - val_accuracy: 0.5377 - val_auc: 0.7059\n",
      "Epoch 299/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9945 - accuracy: 0.5375 - auc: 0.7051 - val_loss: 0.9940 - val_accuracy: 0.5369 - val_auc: 0.7053\n",
      "Epoch 300/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9945 - accuracy: 0.5376 - auc: 0.7051 - val_loss: 0.9940 - val_accuracy: 0.5375 - val_auc: 0.7053\n",
      "Epoch 301/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9945 - accuracy: 0.5377 - auc: 0.7053 - val_loss: 0.9937 - val_accuracy: 0.5372 - val_auc: 0.7056\n",
      "Epoch 302/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9945 - accuracy: 0.5378 - auc: 0.7051 - val_loss: 0.9939 - val_accuracy: 0.5378 - val_auc: 0.7053\n",
      "Epoch 303/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9945 - accuracy: 0.5378 - auc: 0.7051 - val_loss: 0.9939 - val_accuracy: 0.5371 - val_auc: 0.7055\n",
      "Epoch 304/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9944 - accuracy: 0.5378 - auc: 0.7052 - val_loss: 0.9936 - val_accuracy: 0.5370 - val_auc: 0.7058\n",
      "Epoch 305/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9944 - accuracy: 0.5378 - auc: 0.7052 - val_loss: 0.9935 - val_accuracy: 0.5377 - val_auc: 0.7058\n",
      "Epoch 306/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9945 - accuracy: 0.5376 - auc: 0.7052 - val_loss: 0.9934 - val_accuracy: 0.5382 - val_auc: 0.7059\n",
      "Epoch 307/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9945 - accuracy: 0.5380 - auc: 0.7052 - val_loss: 0.9945 - val_accuracy: 0.5376 - val_auc: 0.7054\n",
      "Epoch 308/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9945 - accuracy: 0.5380 - auc: 0.7051 - val_loss: 0.9936 - val_accuracy: 0.5372 - val_auc: 0.7057\n",
      "Epoch 309/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9944 - accuracy: 0.5377 - auc: 0.7052 - val_loss: 0.9935 - val_accuracy: 0.5371 - val_auc: 0.7057\n",
      "Epoch 310/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9944 - accuracy: 0.5375 - auc: 0.7052 - val_loss: 0.9933 - val_accuracy: 0.5379 - val_auc: 0.7062\n",
      "Epoch 311/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9944 - accuracy: 0.5381 - auc: 0.7053 - val_loss: 0.9941 - val_accuracy: 0.5373 - val_auc: 0.7055\n",
      "Epoch 312/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9943 - accuracy: 0.5379 - auc: 0.7052 - val_loss: 0.9938 - val_accuracy: 0.5376 - val_auc: 0.7057\n",
      "Epoch 313/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9943 - accuracy: 0.5379 - auc: 0.7053 - val_loss: 0.9933 - val_accuracy: 0.5380 - val_auc: 0.7060\n",
      "Epoch 314/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9944 - accuracy: 0.5380 - auc: 0.7053 - val_loss: 0.9934 - val_accuracy: 0.5376 - val_auc: 0.7059\n",
      "Epoch 315/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9943 - accuracy: 0.5383 - auc: 0.7052 - val_loss: 0.9935 - val_accuracy: 0.5382 - val_auc: 0.7059\n",
      "Epoch 316/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9943 - accuracy: 0.5380 - auc: 0.7051 - val_loss: 0.9937 - val_accuracy: 0.5370 - val_auc: 0.7056\n",
      "Epoch 317/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9943 - accuracy: 0.5381 - auc: 0.7052 - val_loss: 0.9935 - val_accuracy: 0.5380 - val_auc: 0.7059\n",
      "Epoch 318/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9942 - accuracy: 0.5376 - auc: 0.7053 - val_loss: 0.9936 - val_accuracy: 0.5377 - val_auc: 0.7056\n",
      "Epoch 319/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9942 - accuracy: 0.5381 - auc: 0.7051 - val_loss: 0.9933 - val_accuracy: 0.5384 - val_auc: 0.7059\n",
      "Epoch 320/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9941 - accuracy: 0.5382 - auc: 0.7053 - val_loss: 0.9934 - val_accuracy: 0.5376 - val_auc: 0.7056\n",
      "Epoch 321/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9942 - accuracy: 0.5379 - auc: 0.7052 - val_loss: 0.9936 - val_accuracy: 0.5378 - val_auc: 0.7058\n",
      "Epoch 322/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9942 - accuracy: 0.5377 - auc: 0.7054 - val_loss: 0.9932 - val_accuracy: 0.5377 - val_auc: 0.7059\n",
      "Epoch 323/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9942 - accuracy: 0.5380 - auc: 0.7052 - val_loss: 0.9933 - val_accuracy: 0.5382 - val_auc: 0.7059\n",
      "Epoch 324/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9942 - accuracy: 0.5376 - auc: 0.7052 - val_loss: 0.9933 - val_accuracy: 0.5376 - val_auc: 0.7058\n",
      "Epoch 325/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9942 - accuracy: 0.5381 - auc: 0.7052 - val_loss: 0.9932 - val_accuracy: 0.5372 - val_auc: 0.7060\n",
      "Epoch 326/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9942 - accuracy: 0.5381 - auc: 0.7052 - val_loss: 0.9933 - val_accuracy: 0.5383 - val_auc: 0.7059\n",
      "Epoch 327/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9942 - accuracy: 0.5378 - auc: 0.7053 - val_loss: 0.9932 - val_accuracy: 0.5383 - val_auc: 0.7059\n",
      "Epoch 328/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9942 - accuracy: 0.5377 - auc: 0.7054 - val_loss: 0.9935 - val_accuracy: 0.5383 - val_auc: 0.7058\n",
      "Epoch 329/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9942 - accuracy: 0.5379 - auc: 0.7052 - val_loss: 0.9931 - val_accuracy: 0.5387 - val_auc: 0.7060\n",
      "Epoch 330/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9941 - accuracy: 0.5377 - auc: 0.7052 - val_loss: 0.9932 - val_accuracy: 0.5377 - val_auc: 0.7060\n",
      "Epoch 331/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9941 - accuracy: 0.5377 - auc: 0.7054 - val_loss: 0.9935 - val_accuracy: 0.5374 - val_auc: 0.7056\n",
      "Epoch 332/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9941 - accuracy: 0.5383 - auc: 0.7053 - val_loss: 0.9932 - val_accuracy: 0.5377 - val_auc: 0.7060\n",
      "Epoch 333/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9940 - accuracy: 0.5379 - auc: 0.7054 - val_loss: 0.9930 - val_accuracy: 0.5389 - val_auc: 0.7062\n",
      "Epoch 334/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9941 - accuracy: 0.5381 - auc: 0.7052 - val_loss: 0.9930 - val_accuracy: 0.5381 - val_auc: 0.7061\n",
      "Epoch 335/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9941 - accuracy: 0.5376 - auc: 0.7053 - val_loss: 0.9935 - val_accuracy: 0.5379 - val_auc: 0.7059\n",
      "Epoch 336/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9940 - accuracy: 0.5381 - auc: 0.7054 - val_loss: 0.9931 - val_accuracy: 0.5379 - val_auc: 0.7058\n",
      "Epoch 337/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9941 - accuracy: 0.5380 - auc: 0.7053 - val_loss: 0.9931 - val_accuracy: 0.5380 - val_auc: 0.7061\n",
      "Epoch 338/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9939 - accuracy: 0.5379 - auc: 0.7053 - val_loss: 0.9936 - val_accuracy: 0.5378 - val_auc: 0.7057\n",
      "Epoch 339/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9940 - accuracy: 0.5384 - auc: 0.7054 - val_loss: 0.9931 - val_accuracy: 0.5380 - val_auc: 0.7059\n",
      "Epoch 340/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9941 - accuracy: 0.5382 - auc: 0.7052 - val_loss: 0.9929 - val_accuracy: 0.5386 - val_auc: 0.7063\n",
      "Epoch 341/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9940 - accuracy: 0.5382 - auc: 0.7053 - val_loss: 0.9929 - val_accuracy: 0.5384 - val_auc: 0.7060\n",
      "Epoch 342/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9939 - accuracy: 0.5383 - auc: 0.7055 - val_loss: 0.9933 - val_accuracy: 0.5379 - val_auc: 0.7058\n",
      "Epoch 343/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9939 - accuracy: 0.5379 - auc: 0.7054 - val_loss: 0.9930 - val_accuracy: 0.5379 - val_auc: 0.7062\n",
      "Epoch 344/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9939 - accuracy: 0.5381 - auc: 0.7054 - val_loss: 0.9929 - val_accuracy: 0.5380 - val_auc: 0.7060\n",
      "Epoch 345/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9940 - accuracy: 0.5377 - auc: 0.7054 - val_loss: 0.9931 - val_accuracy: 0.5380 - val_auc: 0.7060\n",
      "Epoch 346/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9939 - accuracy: 0.5380 - auc: 0.7053 - val_loss: 0.9929 - val_accuracy: 0.5384 - val_auc: 0.7061\n",
      "Epoch 347/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9939 - accuracy: 0.5379 - auc: 0.7054 - val_loss: 0.9928 - val_accuracy: 0.5384 - val_auc: 0.7062\n",
      "Epoch 348/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9938 - accuracy: 0.5382 - auc: 0.7054 - val_loss: 0.9930 - val_accuracy: 0.5381 - val_auc: 0.7059\n",
      "Epoch 349/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9937 - accuracy: 0.5381 - auc: 0.7055 - val_loss: 0.9931 - val_accuracy: 0.5379 - val_auc: 0.7058\n",
      "Epoch 350/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9939 - accuracy: 0.5379 - auc: 0.7054 - val_loss: 0.9930 - val_accuracy: 0.5381 - val_auc: 0.7059\n",
      "Epoch 351/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9938 - accuracy: 0.5382 - auc: 0.7055 - val_loss: 0.9928 - val_accuracy: 0.5380 - val_auc: 0.7062\n",
      "Epoch 352/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9938 - accuracy: 0.5382 - auc: 0.7054 - val_loss: 0.9930 - val_accuracy: 0.5379 - val_auc: 0.7060\n",
      "Epoch 353/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9938 - accuracy: 0.5383 - auc: 0.7054 - val_loss: 0.9929 - val_accuracy: 0.5378 - val_auc: 0.7060\n",
      "Epoch 354/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9938 - accuracy: 0.5381 - auc: 0.7054 - val_loss: 0.9929 - val_accuracy: 0.5377 - val_auc: 0.7061\n",
      "Epoch 355/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9939 - accuracy: 0.5380 - auc: 0.7053 - val_loss: 0.9928 - val_accuracy: 0.5386 - val_auc: 0.7062\n",
      "Epoch 356/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9938 - accuracy: 0.5382 - auc: 0.7054 - val_loss: 0.9931 - val_accuracy: 0.5385 - val_auc: 0.7058\n",
      "Epoch 357/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9938 - accuracy: 0.5381 - auc: 0.7054 - val_loss: 0.9927 - val_accuracy: 0.5383 - val_auc: 0.7063\n",
      "Epoch 358/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9938 - accuracy: 0.5377 - auc: 0.7055 - val_loss: 0.9931 - val_accuracy: 0.5387 - val_auc: 0.7060\n",
      "Epoch 359/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9938 - accuracy: 0.5379 - auc: 0.7054 - val_loss: 0.9929 - val_accuracy: 0.5384 - val_auc: 0.7060\n",
      "Epoch 360/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9938 - accuracy: 0.5379 - auc: 0.7054 - val_loss: 0.9929 - val_accuracy: 0.5386 - val_auc: 0.7059\n",
      "Epoch 361/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9938 - accuracy: 0.5380 - auc: 0.7054 - val_loss: 0.9926 - val_accuracy: 0.5389 - val_auc: 0.7062\n",
      "Epoch 362/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9937 - accuracy: 0.5381 - auc: 0.7055 - val_loss: 0.9929 - val_accuracy: 0.5390 - val_auc: 0.7059\n",
      "Epoch 363/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9937 - accuracy: 0.5381 - auc: 0.7054 - val_loss: 0.9928 - val_accuracy: 0.5385 - val_auc: 0.7061\n",
      "Epoch 364/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9937 - accuracy: 0.5382 - auc: 0.7054 - val_loss: 0.9928 - val_accuracy: 0.5383 - val_auc: 0.7060\n",
      "Epoch 365/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9936 - accuracy: 0.5383 - auc: 0.7055 - val_loss: 0.9928 - val_accuracy: 0.5393 - val_auc: 0.7060\n",
      "Epoch 366/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9937 - accuracy: 0.5379 - auc: 0.7055 - val_loss: 0.9925 - val_accuracy: 0.5386 - val_auc: 0.7064\n",
      "Epoch 367/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9937 - accuracy: 0.5381 - auc: 0.7054 - val_loss: 0.9928 - val_accuracy: 0.5381 - val_auc: 0.7060\n",
      "Epoch 368/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9936 - accuracy: 0.5382 - auc: 0.7055 - val_loss: 0.9926 - val_accuracy: 0.5392 - val_auc: 0.7062\n",
      "Epoch 369/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9937 - accuracy: 0.5384 - auc: 0.7055 - val_loss: 0.9925 - val_accuracy: 0.5388 - val_auc: 0.7063\n",
      "Epoch 370/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9936 - accuracy: 0.5378 - auc: 0.7055 - val_loss: 0.9925 - val_accuracy: 0.5392 - val_auc: 0.7062\n",
      "Epoch 371/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9936 - accuracy: 0.5381 - auc: 0.7055 - val_loss: 0.9925 - val_accuracy: 0.5383 - val_auc: 0.7062\n",
      "Epoch 372/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9936 - accuracy: 0.5383 - auc: 0.7054 - val_loss: 0.9928 - val_accuracy: 0.5381 - val_auc: 0.7062\n",
      "Epoch 373/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9936 - accuracy: 0.5380 - auc: 0.7055 - val_loss: 0.9927 - val_accuracy: 0.5381 - val_auc: 0.7059\n",
      "Epoch 374/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9935 - accuracy: 0.5382 - auc: 0.7054 - val_loss: 0.9925 - val_accuracy: 0.5389 - val_auc: 0.7064\n",
      "Epoch 375/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9935 - accuracy: 0.5380 - auc: 0.7055 - val_loss: 0.9927 - val_accuracy: 0.5387 - val_auc: 0.7061\n",
      "Epoch 376/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9935 - accuracy: 0.5377 - auc: 0.7055 - val_loss: 0.9927 - val_accuracy: 0.5386 - val_auc: 0.7059\n",
      "Epoch 377/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9935 - accuracy: 0.5383 - auc: 0.7054 - val_loss: 0.9926 - val_accuracy: 0.5386 - val_auc: 0.7061\n",
      "Epoch 378/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9935 - accuracy: 0.5381 - auc: 0.7055 - val_loss: 0.9927 - val_accuracy: 0.5383 - val_auc: 0.7061\n",
      "Epoch 379/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9936 - accuracy: 0.5382 - auc: 0.7055 - val_loss: 0.9926 - val_accuracy: 0.5392 - val_auc: 0.7061\n",
      "Epoch 380/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9935 - accuracy: 0.5383 - auc: 0.7054 - val_loss: 0.9925 - val_accuracy: 0.5388 - val_auc: 0.7064\n",
      "Epoch 381/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9935 - accuracy: 0.5382 - auc: 0.7055 - val_loss: 0.9925 - val_accuracy: 0.5390 - val_auc: 0.7063\n",
      "Epoch 382/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9935 - accuracy: 0.5385 - auc: 0.7056 - val_loss: 0.9922 - val_accuracy: 0.5390 - val_auc: 0.7067\n",
      "Epoch 383/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9936 - accuracy: 0.5387 - auc: 0.7054 - val_loss: 0.9922 - val_accuracy: 0.5393 - val_auc: 0.7064\n",
      "Epoch 384/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9934 - accuracy: 0.5380 - auc: 0.7056 - val_loss: 0.9923 - val_accuracy: 0.5387 - val_auc: 0.7065\n",
      "Epoch 385/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9934 - accuracy: 0.5372 - auc: 0.7056 - val_loss: 0.9924 - val_accuracy: 0.5381 - val_auc: 0.7063\n",
      "Epoch 386/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9934 - accuracy: 0.5379 - auc: 0.7056 - val_loss: 0.9924 - val_accuracy: 0.5390 - val_auc: 0.7065\n",
      "Epoch 387/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9934 - accuracy: 0.5381 - auc: 0.7055 - val_loss: 0.9926 - val_accuracy: 0.5382 - val_auc: 0.7062\n",
      "Epoch 388/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9934 - accuracy: 0.5380 - auc: 0.7056 - val_loss: 0.9923 - val_accuracy: 0.5387 - val_auc: 0.7064\n",
      "Epoch 389/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9934 - accuracy: 0.5379 - auc: 0.7056 - val_loss: 0.9925 - val_accuracy: 0.5392 - val_auc: 0.7061\n",
      "Epoch 390/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9934 - accuracy: 0.5381 - auc: 0.7055 - val_loss: 0.9925 - val_accuracy: 0.5383 - val_auc: 0.7064\n",
      "Epoch 391/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9934 - accuracy: 0.5385 - auc: 0.7055 - val_loss: 0.9924 - val_accuracy: 0.5381 - val_auc: 0.7065\n",
      "Epoch 392/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9935 - accuracy: 0.5383 - auc: 0.7056 - val_loss: 0.9925 - val_accuracy: 0.5383 - val_auc: 0.7061\n",
      "Epoch 393/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9933 - accuracy: 0.5381 - auc: 0.7056 - val_loss: 0.9923 - val_accuracy: 0.5377 - val_auc: 0.7064\n",
      "Epoch 394/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9934 - accuracy: 0.5379 - auc: 0.7056 - val_loss: 0.9922 - val_accuracy: 0.5392 - val_auc: 0.7065\n",
      "Epoch 395/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9934 - accuracy: 0.5377 - auc: 0.7056 - val_loss: 0.9922 - val_accuracy: 0.5380 - val_auc: 0.7065\n",
      "Epoch 396/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9933 - accuracy: 0.5383 - auc: 0.7056 - val_loss: 0.9923 - val_accuracy: 0.5392 - val_auc: 0.7062\n",
      "Epoch 397/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9933 - accuracy: 0.5383 - auc: 0.7056 - val_loss: 0.9926 - val_accuracy: 0.5387 - val_auc: 0.7063\n",
      "Epoch 398/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9934 - accuracy: 0.5381 - auc: 0.7055 - val_loss: 0.9923 - val_accuracy: 0.5387 - val_auc: 0.7063\n",
      "Epoch 399/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9933 - accuracy: 0.5376 - auc: 0.7055 - val_loss: 0.9921 - val_accuracy: 0.5387 - val_auc: 0.7065\n",
      "Epoch 400/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9933 - accuracy: 0.5385 - auc: 0.7056 - val_loss: 0.9925 - val_accuracy: 0.5389 - val_auc: 0.7062\n",
      "Epoch 401/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9933 - accuracy: 0.5382 - auc: 0.7056 - val_loss: 0.9924 - val_accuracy: 0.5395 - val_auc: 0.7061\n",
      "Epoch 402/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9932 - accuracy: 0.5381 - auc: 0.7056 - val_loss: 0.9925 - val_accuracy: 0.5383 - val_auc: 0.7062\n",
      "Epoch 403/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9933 - accuracy: 0.5380 - auc: 0.7056 - val_loss: 0.9924 - val_accuracy: 0.5374 - val_auc: 0.7061\n",
      "Epoch 404/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9932 - accuracy: 0.5381 - auc: 0.7056 - val_loss: 0.9922 - val_accuracy: 0.5385 - val_auc: 0.7064\n",
      "Epoch 405/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9933 - accuracy: 0.5379 - auc: 0.7056 - val_loss: 0.9921 - val_accuracy: 0.5387 - val_auc: 0.7065\n",
      "Epoch 406/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9932 - accuracy: 0.5379 - auc: 0.7056 - val_loss: 0.9925 - val_accuracy: 0.5383 - val_auc: 0.7062\n",
      "Epoch 407/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9933 - accuracy: 0.5382 - auc: 0.7056 - val_loss: 0.9922 - val_accuracy: 0.5388 - val_auc: 0.7063\n",
      "Epoch 408/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9931 - accuracy: 0.5378 - auc: 0.7057 - val_loss: 0.9923 - val_accuracy: 0.5386 - val_auc: 0.7063\n",
      "Epoch 409/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9932 - accuracy: 0.5377 - auc: 0.7056 - val_loss: 0.9923 - val_accuracy: 0.5381 - val_auc: 0.7064\n",
      "Epoch 410/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9932 - accuracy: 0.5380 - auc: 0.7056 - val_loss: 0.9921 - val_accuracy: 0.5386 - val_auc: 0.7063\n",
      "Epoch 411/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9932 - accuracy: 0.5381 - auc: 0.7056 - val_loss: 0.9920 - val_accuracy: 0.5386 - val_auc: 0.7066\n",
      "Epoch 412/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9932 - accuracy: 0.5390 - auc: 0.7056 - val_loss: 0.9922 - val_accuracy: 0.5392 - val_auc: 0.7065\n",
      "Epoch 413/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9931 - accuracy: 0.5379 - auc: 0.7057 - val_loss: 0.9922 - val_accuracy: 0.5384 - val_auc: 0.7063\n",
      "Epoch 414/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9932 - accuracy: 0.5382 - auc: 0.7056 - val_loss: 0.9923 - val_accuracy: 0.5390 - val_auc: 0.7063\n",
      "Epoch 415/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9932 - accuracy: 0.5381 - auc: 0.7056 - val_loss: 0.9918 - val_accuracy: 0.5395 - val_auc: 0.7066\n",
      "Epoch 416/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9931 - accuracy: 0.5382 - auc: 0.7057 - val_loss: 0.9921 - val_accuracy: 0.5386 - val_auc: 0.7064\n",
      "Epoch 417/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9932 - accuracy: 0.5385 - auc: 0.7056 - val_loss: 0.9918 - val_accuracy: 0.5383 - val_auc: 0.7068\n",
      "Epoch 418/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9932 - accuracy: 0.5384 - auc: 0.7056 - val_loss: 0.9920 - val_accuracy: 0.5383 - val_auc: 0.7065\n",
      "Epoch 419/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9931 - accuracy: 0.5382 - auc: 0.7057 - val_loss: 0.9921 - val_accuracy: 0.5387 - val_auc: 0.7066\n",
      "Epoch 420/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9931 - accuracy: 0.5380 - auc: 0.7057 - val_loss: 0.9923 - val_accuracy: 0.5385 - val_auc: 0.7062\n",
      "Epoch 421/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9931 - accuracy: 0.5381 - auc: 0.7057 - val_loss: 0.9920 - val_accuracy: 0.5381 - val_auc: 0.7065\n",
      "Epoch 422/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9931 - accuracy: 0.5377 - auc: 0.7056 - val_loss: 0.9919 - val_accuracy: 0.5383 - val_auc: 0.7066\n",
      "Epoch 423/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9931 - accuracy: 0.5381 - auc: 0.7058 - val_loss: 0.9920 - val_accuracy: 0.5384 - val_auc: 0.7064\n",
      "Epoch 424/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9931 - accuracy: 0.5381 - auc: 0.7055 - val_loss: 0.9919 - val_accuracy: 0.5377 - val_auc: 0.7066\n",
      "Epoch 425/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9931 - accuracy: 0.5379 - auc: 0.7056 - val_loss: 0.9919 - val_accuracy: 0.5389 - val_auc: 0.7066\n",
      "Epoch 426/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9931 - accuracy: 0.5385 - auc: 0.7057 - val_loss: 0.9918 - val_accuracy: 0.5384 - val_auc: 0.7067\n",
      "Epoch 427/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 0.9931 - accuracy: 0.5384 - auc: 0.7057 - val_loss: 0.9919 - val_accuracy: 0.5384 - val_auc: 0.7067\n",
      "Epoch 428/500\n",
      "676/676 [==============================] - 2s 4ms/step - loss: 0.9930 - accuracy: 0.5382 - auc: 0.7057 - val_loss: 0.9919 - val_accuracy: 0.5388 - val_auc: 0.7065\n",
      "Epoch 429/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9931 - accuracy: 0.5380 - auc: 0.7057 - val_loss: 0.9918 - val_accuracy: 0.5377 - val_auc: 0.7067\n",
      "Epoch 430/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9929 - accuracy: 0.5384 - auc: 0.7058 - val_loss: 0.9917 - val_accuracy: 0.5383 - val_auc: 0.7068\n",
      "Epoch 431/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9930 - accuracy: 0.5381 - auc: 0.7056 - val_loss: 0.9917 - val_accuracy: 0.5383 - val_auc: 0.7068\n",
      "Epoch 432/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9930 - accuracy: 0.5384 - auc: 0.7057 - val_loss: 0.9917 - val_accuracy: 0.5378 - val_auc: 0.7067\n",
      "Epoch 433/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9930 - accuracy: 0.5382 - auc: 0.7057 - val_loss: 0.9922 - val_accuracy: 0.5386 - val_auc: 0.7061\n",
      "Epoch 434/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9930 - accuracy: 0.5380 - auc: 0.7056 - val_loss: 0.9918 - val_accuracy: 0.5386 - val_auc: 0.7068\n",
      "Epoch 435/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9930 - accuracy: 0.5383 - auc: 0.7057 - val_loss: 0.9920 - val_accuracy: 0.5396 - val_auc: 0.7063\n",
      "Epoch 436/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9930 - accuracy: 0.5382 - auc: 0.7057 - val_loss: 0.9920 - val_accuracy: 0.5387 - val_auc: 0.7065\n",
      "Epoch 437/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9929 - accuracy: 0.5379 - auc: 0.7057 - val_loss: 0.9920 - val_accuracy: 0.5378 - val_auc: 0.7065\n",
      "Epoch 438/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9929 - accuracy: 0.5379 - auc: 0.7057 - val_loss: 0.9924 - val_accuracy: 0.5373 - val_auc: 0.7062\n",
      "Epoch 439/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9929 - accuracy: 0.5385 - auc: 0.7058 - val_loss: 0.9921 - val_accuracy: 0.5392 - val_auc: 0.7062\n",
      "Epoch 440/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9929 - accuracy: 0.5382 - auc: 0.7057 - val_loss: 0.9919 - val_accuracy: 0.5383 - val_auc: 0.7064\n",
      "Epoch 441/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9930 - accuracy: 0.5384 - auc: 0.7057 - val_loss: 0.9919 - val_accuracy: 0.5385 - val_auc: 0.7064\n",
      "Epoch 442/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9929 - accuracy: 0.5380 - auc: 0.7057 - val_loss: 0.9917 - val_accuracy: 0.5395 - val_auc: 0.7067\n",
      "Epoch 443/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9929 - accuracy: 0.5385 - auc: 0.7057 - val_loss: 0.9921 - val_accuracy: 0.5390 - val_auc: 0.7064\n",
      "Epoch 444/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9929 - accuracy: 0.5377 - auc: 0.7057 - val_loss: 0.9920 - val_accuracy: 0.5386 - val_auc: 0.7066\n",
      "Epoch 445/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9929 - accuracy: 0.5380 - auc: 0.7057 - val_loss: 0.9916 - val_accuracy: 0.5384 - val_auc: 0.7068\n",
      "Epoch 446/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5382 - auc: 0.7058 - val_loss: 0.9916 - val_accuracy: 0.5386 - val_auc: 0.7066\n",
      "Epoch 447/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5377 - auc: 0.7057 - val_loss: 0.9919 - val_accuracy: 0.5384 - val_auc: 0.7066\n",
      "Epoch 448/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5389 - auc: 0.7059 - val_loss: 0.9922 - val_accuracy: 0.5373 - val_auc: 0.7062\n",
      "Epoch 449/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9929 - accuracy: 0.5380 - auc: 0.7057 - val_loss: 0.9918 - val_accuracy: 0.5394 - val_auc: 0.7065\n",
      "Epoch 450/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9929 - accuracy: 0.5380 - auc: 0.7057 - val_loss: 0.9917 - val_accuracy: 0.5385 - val_auc: 0.7066\n",
      "Epoch 451/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9929 - accuracy: 0.5383 - auc: 0.7057 - val_loss: 0.9919 - val_accuracy: 0.5386 - val_auc: 0.7063\n",
      "Epoch 452/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5377 - auc: 0.7057 - val_loss: 0.9919 - val_accuracy: 0.5384 - val_auc: 0.7064\n",
      "Epoch 453/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5381 - auc: 0.7057 - val_loss: 0.9918 - val_accuracy: 0.5383 - val_auc: 0.7068\n",
      "Epoch 454/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5376 - auc: 0.7059 - val_loss: 0.9917 - val_accuracy: 0.5381 - val_auc: 0.7067\n",
      "Epoch 455/500\n",
      "676/676 [==============================] - 3s 5ms/step - loss: 0.9928 - accuracy: 0.5382 - auc: 0.7057 - val_loss: 0.9918 - val_accuracy: 0.5386 - val_auc: 0.7064\n",
      "Epoch 456/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5383 - auc: 0.7057 - val_loss: 0.9916 - val_accuracy: 0.5392 - val_auc: 0.7067\n",
      "Epoch 457/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5382 - auc: 0.7058 - val_loss: 0.9916 - val_accuracy: 0.5390 - val_auc: 0.7067\n",
      "Epoch 458/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5379 - auc: 0.7058 - val_loss: 0.9915 - val_accuracy: 0.5387 - val_auc: 0.7069\n",
      "Epoch 459/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9927 - accuracy: 0.5378 - auc: 0.7058 - val_loss: 0.9916 - val_accuracy: 0.5385 - val_auc: 0.7067\n",
      "Epoch 460/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5379 - auc: 0.7057 - val_loss: 0.9917 - val_accuracy: 0.5393 - val_auc: 0.7064\n",
      "Epoch 461/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9929 - accuracy: 0.5382 - auc: 0.7057 - val_loss: 0.9915 - val_accuracy: 0.5391 - val_auc: 0.7068\n",
      "Epoch 462/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5380 - auc: 0.7057 - val_loss: 0.9916 - val_accuracy: 0.5383 - val_auc: 0.7067\n",
      "Epoch 463/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5378 - auc: 0.7058 - val_loss: 0.9917 - val_accuracy: 0.5386 - val_auc: 0.7068\n",
      "Epoch 464/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5384 - auc: 0.7058 - val_loss: 0.9916 - val_accuracy: 0.5391 - val_auc: 0.7068\n",
      "Epoch 465/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5384 - auc: 0.7057 - val_loss: 0.9915 - val_accuracy: 0.5396 - val_auc: 0.7067\n",
      "Epoch 466/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9927 - accuracy: 0.5382 - auc: 0.7057 - val_loss: 0.9914 - val_accuracy: 0.5394 - val_auc: 0.7069\n",
      "Epoch 467/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9927 - accuracy: 0.5380 - auc: 0.7057 - val_loss: 0.9914 - val_accuracy: 0.5389 - val_auc: 0.7069\n",
      "Epoch 468/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5385 - auc: 0.7058 - val_loss: 0.9917 - val_accuracy: 0.5389 - val_auc: 0.7065\n",
      "Epoch 469/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9927 - accuracy: 0.5381 - auc: 0.7058 - val_loss: 0.9917 - val_accuracy: 0.5393 - val_auc: 0.7064\n",
      "Epoch 470/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9928 - accuracy: 0.5382 - auc: 0.7057 - val_loss: 0.9915 - val_accuracy: 0.5390 - val_auc: 0.7068\n",
      "Epoch 471/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9926 - accuracy: 0.5382 - auc: 0.7058 - val_loss: 0.9917 - val_accuracy: 0.5383 - val_auc: 0.7066\n",
      "Epoch 472/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9927 - accuracy: 0.5380 - auc: 0.7058 - val_loss: 0.9914 - val_accuracy: 0.5392 - val_auc: 0.7069\n",
      "Epoch 473/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9927 - accuracy: 0.5383 - auc: 0.7058 - val_loss: 0.9913 - val_accuracy: 0.5392 - val_auc: 0.7069\n",
      "Epoch 474/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9926 - accuracy: 0.5379 - auc: 0.7058 - val_loss: 0.9922 - val_accuracy: 0.5388 - val_auc: 0.7060\n",
      "Epoch 475/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9927 - accuracy: 0.5382 - auc: 0.7057 - val_loss: 0.9913 - val_accuracy: 0.5400 - val_auc: 0.7071\n",
      "Epoch 476/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9926 - accuracy: 0.5379 - auc: 0.7058 - val_loss: 0.9921 - val_accuracy: 0.5382 - val_auc: 0.7062\n",
      "Epoch 477/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9926 - accuracy: 0.5380 - auc: 0.7059 - val_loss: 0.9919 - val_accuracy: 0.5385 - val_auc: 0.7066\n",
      "Epoch 478/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9926 - accuracy: 0.5382 - auc: 0.7058 - val_loss: 0.9915 - val_accuracy: 0.5385 - val_auc: 0.7068\n",
      "Epoch 479/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9927 - accuracy: 0.5382 - auc: 0.7059 - val_loss: 0.9918 - val_accuracy: 0.5393 - val_auc: 0.7062\n",
      "Epoch 480/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9927 - accuracy: 0.5384 - auc: 0.7057 - val_loss: 0.9918 - val_accuracy: 0.5394 - val_auc: 0.7063\n",
      "Epoch 481/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9927 - accuracy: 0.5379 - auc: 0.7058 - val_loss: 0.9911 - val_accuracy: 0.5387 - val_auc: 0.7072\n",
      "Epoch 482/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9927 - accuracy: 0.5380 - auc: 0.7058 - val_loss: 0.9915 - val_accuracy: 0.5385 - val_auc: 0.7069\n",
      "Epoch 483/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9926 - accuracy: 0.5382 - auc: 0.7058 - val_loss: 0.9914 - val_accuracy: 0.5379 - val_auc: 0.7069\n",
      "Epoch 484/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9927 - accuracy: 0.5381 - auc: 0.7057 - val_loss: 0.9915 - val_accuracy: 0.5387 - val_auc: 0.7067\n",
      "Epoch 485/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9925 - accuracy: 0.5382 - auc: 0.7059 - val_loss: 0.9915 - val_accuracy: 0.5390 - val_auc: 0.7066\n",
      "Epoch 486/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9926 - accuracy: 0.5380 - auc: 0.7059 - val_loss: 0.9918 - val_accuracy: 0.5387 - val_auc: 0.7064\n",
      "Epoch 487/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9926 - accuracy: 0.5381 - auc: 0.7058 - val_loss: 0.9922 - val_accuracy: 0.5382 - val_auc: 0.7058\n",
      "Epoch 488/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9925 - accuracy: 0.5379 - auc: 0.7058 - val_loss: 0.9917 - val_accuracy: 0.5386 - val_auc: 0.7062\n",
      "Epoch 489/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9927 - accuracy: 0.5380 - auc: 0.7058 - val_loss: 0.9916 - val_accuracy: 0.5391 - val_auc: 0.7065\n",
      "Epoch 490/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9927 - accuracy: 0.5385 - auc: 0.7057 - val_loss: 0.9921 - val_accuracy: 0.5395 - val_auc: 0.7062\n",
      "Epoch 491/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9925 - accuracy: 0.5376 - auc: 0.7059 - val_loss: 0.9919 - val_accuracy: 0.5385 - val_auc: 0.7062\n",
      "Epoch 492/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9926 - accuracy: 0.5383 - auc: 0.7059 - val_loss: 0.9914 - val_accuracy: 0.5388 - val_auc: 0.7066\n",
      "Epoch 493/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9926 - accuracy: 0.5383 - auc: 0.7057 - val_loss: 0.9919 - val_accuracy: 0.5381 - val_auc: 0.7064\n",
      "Epoch 494/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9926 - accuracy: 0.5384 - auc: 0.7058 - val_loss: 0.9914 - val_accuracy: 0.5398 - val_auc: 0.7068\n",
      "Epoch 495/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9926 - accuracy: 0.5381 - auc: 0.7057 - val_loss: 0.9914 - val_accuracy: 0.5389 - val_auc: 0.7069\n",
      "Epoch 496/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9925 - accuracy: 0.5379 - auc: 0.7059 - val_loss: 0.9917 - val_accuracy: 0.5389 - val_auc: 0.7065\n",
      "Epoch 497/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9925 - accuracy: 0.5384 - auc: 0.7058 - val_loss: 0.9911 - val_accuracy: 0.5395 - val_auc: 0.7069\n",
      "Epoch 498/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9926 - accuracy: 0.5380 - auc: 0.7057 - val_loss: 0.9920 - val_accuracy: 0.5390 - val_auc: 0.7063\n",
      "Epoch 499/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9925 - accuracy: 0.5384 - auc: 0.7059 - val_loss: 0.9912 - val_accuracy: 0.5393 - val_auc: 0.7068\n",
      "Epoch 500/500\n",
      "676/676 [==============================] - 3s 4ms/step - loss: 0.9925 - accuracy: 0.5380 - auc: 0.7058 - val_loss: 0.9913 - val_accuracy: 0.5383 - val_auc: 0.7070\n"
     ]
    }
   ],
   "source": [
    "history = mlpclassifier.fit(\n",
    "    train_X, train_y_multi,\n",
    "    validation_split=0.2, epochs=epochs, batch_size=batch_size,\n",
    "    callbacks=[callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'][10:])\n",
    "plt.plot(history.history['val_loss'][10:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541/541 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((17289, 3), (17289,))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = mlpclassifier.predict(test_X)\n",
    "test_results = np.argmax(test_pred, axis=-1)\n",
    "test_pred.shape, test_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2638/2638 [==============================] - 3s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((84406, 3), (84406,))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred = mlpclassifier.predict(train_X)\n",
    "train_results = np.argmax(train_pred, axis=-1)\n",
    "train_pred.shape, train_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5057333880685334"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# evaluate\n",
    "train_f1 = f1_score(train_y, train_results, average='macro')\n",
    "train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3개를 하나로!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84406, 39), (17289, 38), (17289, 2))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission_df = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "train_df.shape, test_df.shape, sample_submission_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mname = 'mlpclassifier_keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def make_report(template, test_pred, mname):\n",
    "    template['TARGET'] = test_pred\n",
    "    now = dt.strftime(dt.now(), '%y-%m-%d')\n",
    "    template.to_csv(f'results/{mname}-{now}-3.csv', index=False)\n",
    "    \n",
    "make_report(sample_submission_df, test_results, mname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimizer\n",
    "## early stopping\n",
    "## compile\n",
    "## train\n",
    "## evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.512232061"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
